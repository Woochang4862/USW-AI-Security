{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트 기반 스팸 탐지 모델 (BERT + SVM) with W&B 모니터링 🔍\n",
    "\n",
    "이 노트북은 BERT와 SVM을 결합하여 텍스트 기반의 스팸 탐지 모델을 구현하고, Weights & Biases를 통해 실험을 모니터링합니다.\n",
    "\n",
    "## 실험 설계\n",
    "1. BERT를 특징 추출기로 사용\n",
    "2. BERT의 [CLS] 토큰 임베딩을 SVM의 입력으로 사용\n",
    "3. SVM으로 최종 분류 수행\n",
    "4. W&B로 실험 결과 추적 및 시각화\n",
    "\n",
    "## 데이터셋\n",
    "- SpamAssassin Public Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 환경 설정 및 라이브러리 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch transformers scikit-learn pandas numpy tqdm kagglehub wandb email\n",
    "!nvidia-smi  # GPU 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import email\n",
    "from tqdm import tqdm\n",
    "import kagglehub\n",
    "import wandb\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. W&B 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb 로그인\n",
    "!wandb login\n",
    "\n",
    "# 실험 설정\n",
    "config = {\n",
    "    \"architecture\": \"BERT-SVM\",\n",
    "    \"dataset\": \"SpamAssassin\",\n",
    "    \"bert_model\": \"bert-base-uncased\",\n",
    "    \"max_length\": 512,\n",
    "    \"svm_kernel\": \"rbf\",\n",
    "    \"svm_C\": 1.0,\n",
    "    \"batch_size\": 32,\n",
    "    \"random_state\": 42,\n",
    "    \"test_size\": 0.2,\n",
    "    \"experiment_timestamp\": datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "}\n",
    "\n",
    "# wandb 실험 초기화\n",
    "run = wandb.init(\n",
    "    project=\"text-spam-detection\",\n",
    "    name=f\"BERT-SVM_{config['experiment_timestamp']}\",\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 데이터셋 다운로드 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_email(file_path):\n",
    "    \"\"\"이메일 파일을 파싱하여 본문 텍스트를 추출\"\"\"\n",
    "    with open(file_path, 'r', encoding='latin-1') as f:\n",
    "        msg = email.message_from_file(f)\n",
    "\n",
    "    body = \"\"\n",
    "    if msg.is_multipart():\n",
    "        for part in msg.walk():\n",
    "            if part.get_content_type() == \"text/plain\":\n",
    "                body += part.get_payload(decode=True).decode('latin-1', errors='ignore')\n",
    "    else:\n",
    "        body = msg.get_payload(decode=True).decode('latin-1', errors='ignore')\n",
    "\n",
    "    return body.strip()\n",
    "\n",
    "def load_dataset(dataset_path):\n",
    "    \"\"\"데이터셋 로드 및 전처리\"\"\"\n",
    "    texts = []\n",
    "    labels = []\n",
    "    stats = {\"spam\": 0, \"easy_ham\": 0, \"hard_ham\": 0}\n",
    "\n",
    "    # 스팸 이메일 로드\n",
    "    spam_dir = os.path.join(dataset_path, 'spam_2/spam_2')\n",
    "    for file_name in tqdm(os.listdir(spam_dir), desc='Loading spam emails'):\n",
    "        file_path = os.path.join(spam_dir, file_name)\n",
    "        try:\n",
    "            text = parse_email(file_path)\n",
    "            texts.append(text)\n",
    "            labels.append(1)\n",
    "            stats[\"spam\"] += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing {file_path}: {str(e)}\")\n",
    "\n",
    "    # 정상 이메일 로드 (easy_ham)\n",
    "    ham_dir = os.path.join(dataset_path, 'easy_ham/easy_ham')\n",
    "    for file_name in tqdm(os.listdir(ham_dir), desc='Loading easy ham emails'):\n",
    "        file_path = os.path.join(ham_dir, file_name)\n",
    "        try:\n",
    "            text = parse_email(file_path)\n",
    "            texts.append(text)\n",
    "            labels.append(0)\n",
    "            stats[\"easy_ham\"] += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing {file_path}: {str(e)}\")\n",
    "\n",
    "    # 정상 이메일 로드 (hard_ham)\n",
    "    ham_dir = os.path.join(dataset_path, 'hard_ham/hard_ham')\n",
    "    for file_name in tqdm(os.listdir(ham_dir), desc='Loading hard ham emails'):\n",
    "        file_path = os.path.join(ham_dir, file_name)\n",
    "        try:\n",
    "            text = parse_email(file_path)\n",
    "            texts.append(text)\n",
    "            labels.append(0)\n",
    "            stats[\"hard_ham\"] += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing {file_path}: {str(e)}\")\n",
    "\n",
    "    # 데이터셋 통계 로깅\n",
    "    wandb.log({\n",
    "        \"dataset_stats/spam_count\": stats[\"spam\"],\n",
    "        \"dataset_stats/easy_ham_count\": stats[\"easy_ham\"],\n",
    "        \"dataset_stats/hard_ham_count\": stats[\"hard_ham\"],\n",
    "        \"dataset_stats/total_samples\": len(texts),\n",
    "        \"dataset_stats/spam_ratio\": stats[\"spam\"] / len(texts)\n",
    "    })\n",
    "\n",
    "    # 데이터셋 분포 시각화\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(['Spam', 'Easy Ham', 'Hard Ham'], \n",
    "            [stats['spam'], stats['easy_ham'], stats['hard_ham']])\n",
    "    plt.title('Dataset Distribution')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    wandb.log({\"dataset_stats/distribution\": wandb.Image(plt)})\n",
    "    plt.close()\n",
    "\n",
    "    return texts, labels\n",
    "\n",
    "# SpamAssassin 데이터셋 다운로드\n",
    "dataset_path = kagglehub.dataset_download(\"beatoa/spamassassin-public-corpus\")\n",
    "print(\"Dataset path:\", dataset_path)\n",
    "\n",
    "# 데이터 로드\n",
    "texts, labels = load_dataset(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. BERT 특징 추출기 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertFeatureExtractor:\n",
    "    def __init__(self, model_name='bert-base-uncased', max_length=512):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        self.model = BertModel.from_pretrained(model_name).to(self.device)\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # 모델 정보 로깅\n",
    "        wandb.log({\n",
    "            \"model_info/device\": str(self.device),\n",
    "            \"model_info/bert_model\": model_name,\n",
    "            \"model_info/max_length\": max_length\n",
    "        })\n",
    "\n",
    "    def extract_features(self, texts, batch_size=32):\n",
    "        self.model.eval()\n",
    "        all_features = []\n",
    "        total_tokens = 0\n",
    "        truncated_count = 0\n",
    "\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc='Extracting BERT features'):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            \n",
    "            # 텍스트 토큰화\n",
    "            inputs = self.tokenizer(\n",
    "                batch_texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                return_tensors='pt'\n",
    "            ).to(self.device)\n",
    "\n",
    "            # 토큰화 통계 수집\n",
    "            total_tokens += inputs.input_ids.numel()\n",
    "            truncated_count += sum(len(self.tokenizer.encode(text)) > self.max_length for text in batch_texts)\n",
    "\n",
    "            # BERT 특징 추출\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "                features = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "                all_features.append(features)\n",
    "\n",
    "        # 토큰화 통계 로깅\n",
    "        wandb.log({\n",
    "            \"tokenization/avg_tokens_per_sample\": total_tokens / len(texts),\n",
    "            \"tokenization/truncated_samples\": truncated_count,\n",
    "            \"tokenization/truncation_ratio\": truncated_count / len(texts)\n",
    "        })\n",
    "\n",
    "        return np.vstack(all_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 실험 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    texts, labels, \n",
    "    test_size=config['test_size'], \n",
    "    random_state=config['random_state'], \n",
    "    stratify=labels\n",
    ")\n",
    "\n",
    "# 분할 정보 로깅\n",
    "wandb.log({\n",
    "    \"split/train_size\": len(X_train),\n",
    "    \"split/test_size\": len(X_test),\n",
    "    \"split/train_spam_ratio\": sum(y_train) / len(y_train),\n",
    "    \"split/test_spam_ratio\": sum(y_test) / len(y_test)\n",
    "})\n",
    "\n",
    "# BERT 특징 추출\n",
    "print(\"\\nBERT 특징 추출 중...\")\n",
    "feature_extractor = BertFeatureExtractor(\n",
    "    model_name=config['bert_model'],\n",
    "    max_length=config['max_length']\n",
    ")\n",
    "\n",
    "X_train_features = feature_extractor.extract_features(X_train, batch_size=config['batch_size'])\n",
    "X_test_features = feature_extractor.extract_features(X_test, batch_size=config['batch_size'])\n",
    "\n",
    "# 특징 추출 결과 로깅\n",
    "wandb.log({\n",
    "    \"features/train_shape\": list(X_train_features.shape),\n",
    "    \"features/test_shape\": list(X_test_features.shape),\n",
    "    \"features/mean_value\": float(X_train_features.mean()),\n",
    "    \"features/std_value\": float(X_train_features.std())\n",
    "})\n",
    "\n",
    "# SVM 학습\n",
    "print(\"\\nSVM 학습 중...\")\n",
    "svm = SVC(kernel=config['svm_kernel'], C=config['svm_C'], probability=True)\n",
    "svm.fit(X_train_features, y_train)\n",
    "\n",
    "# 예측 및 평가\n",
    "y_pred = svm.predict(X_test_features)\n",
    "y_prob = svm.predict_proba(X_test_features)[:, 1]\n",
    "\n",
    "# 메트릭 계산\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# ROC 곡선 계산\n",
    "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# 결과 출력\n",
    "print(\"\\n실험 결과:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# 혼동 행렬 시각화\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "wandb.log({\"evaluation/confusion_matrix\": wandb.Image(plt)})\n",
    "plt.close()\n",
    "\n",
    "# ROC 곡선 시각화\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "wandb.log({\"evaluation/roc_curve\": wandb.Image(plt)})\n",
    "plt.close()\n",
    "\n",
    "# 예측 확률 분포 시각화\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(y_prob[y_test == 0], bins=50, alpha=0.5, label='Normal', density=True)\n",
    "plt.hist(y_prob[y_test == 1], bins=50, alpha=0.5, label='Spam', density=True)\n",
    "plt.xlabel('Predicted Spam Probability')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Distribution of Predicted Probabilities')\n",
    "plt.legend()\n",
    "wandb.log({\"evaluation/probability_distribution\": wandb.Image(plt)})\n",
    "plt.close()\n",
    "\n",
    "# 메트릭 로깅\n",
    "wandb.log({\n",
    "    \"metrics/accuracy\": accuracy,\n",
    "    \"metrics/precision\": precision,\n",
    "    \"metrics/recall\": recall,\n",
    "    \"metrics/f1\": f1,\n",
    "    \"metrics/roc_auc\": roc_auc,\n",
    "    \"metrics/true_positives\": int(conf_matrix[1][1]),\n",
    "    \"metrics/false_positives\": int(conf_matrix[0][1]),\n",
    "    \"metrics/true_negatives\": int(conf_matrix[0][0]),\n",
    "    \"metrics/false_negatives\": int(conf_matrix[1][0])\n",
    "})\n",
    "\n",
    "# 실험 종료\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 새로운 이메일에 대한 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_email(text, feature_extractor, classifier):\n",
    "    # BERT 특징 추출\n",
    "    features = feature_extractor.extract_features([text])\n",
    "    \n",
    "    # SVM 예측\n",
    "    prediction = classifier.predict(features)[0]\n",
    "    probability = classifier.predict_proba(features)[0]\n",
    "    \n",
    "    return {\n",
    "        'prediction': '스팸' if prediction == 1 else '정상',\n",
    "        'spam_prob': probability[1],\n",
    "        'normal_prob': probability[0]\n",
    "    }\n",
    "\n",
    "# wandb 실험 초기화\n",
    "run = wandb.init(\n",
    "    project=\"text-spam-detection\",\n",
    "    name=f\"BERT-SVM_{config['experiment_timestamp']}\",\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# 테스트 이메일\n",
    "test_email = \"\"\"\n",
    "From: john.doe@company.com\n",
    "Return-Path: <john.doe@company.com>\n",
    "Delivered-To: recipient@localhost.com\n",
    "Received: from localhost (localhost [127.0.0.1])\n",
    "\tby server.company.com (Postfix) with ESMTP id ABC123XYZ\n",
    "\tfor <recipient@localhost.com>; Tue, 15 Feb 2024 10:30:15 +0900 (KST)\n",
    "Received: from mail.company.com (mail.company.com [192.168.1.100])\n",
    "\tby server.company.com (Postfix) with ESMTP id DEF456UVW\n",
    "\tfor <recipient@localhost.com>; Tue, 15 Feb 2024 10:30:14 +0900 (KST)\n",
    "Message-Id: <20240215103014.DEF456UVW@mail.company.com>\n",
    "Date: Tue, 15 Feb 2024 10:30:14 +0900 (KST)\n",
    "To: team@company.com\n",
    "From: \"John Doe\" <john.doe@company.com>\n",
    "MIME-Version: 1.0\n",
    "Content-Type: text/plain; charset=\"UTF-8\"\n",
    "Subject: Project Progress Report\n",
    "\n",
    "Dear Team Members,\n",
    "\n",
    "I would like to share this week's project progress update.\n",
    "\n",
    "1. AI Model Development\n",
    "- Completed implementation of BERT-based text classification model\n",
    "- Achieved 95% test accuracy\n",
    "- Hyperparameter optimization in progress\n",
    "\n",
    "2. Data Preprocessing\n",
    "- Data cleaning completed\n",
    "- Labeling work 80% complete\n",
    "- Additional data collection planned\n",
    "\n",
    "3. Next Week's Plan\n",
    "- Model performance improvement\n",
    "- Start API development\n",
    "- Documentation work\n",
    "\n",
    "Please reply if you have any questions or comments.\n",
    "\n",
    "Best regards,\n",
    "\n",
    "John Doe\n",
    "AI Development Team\n",
    "Company Inc.\n",
    "Tel: 02-123-4567\n",
    "\"\"\"\n",
    "\n",
    "result = predict_email(test_email, feature_extractor, svm)\n",
    "\n",
    "print(\"\\n예측 결과:\")\n",
    "print(f\"판정: {result['prediction']}\")\n",
    "print(f\"스팸 확률: {result['spam_prob']:.4f}\")\n",
    "print(f\"정상 확률: {result['normal_prob']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
