{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# í…ìŠ¤íŠ¸ ê¸°ë°˜ ìŠ¤íŒ¸ íƒì§€ ëª¨ë¸ (BERT + SVM) with W&B ëª¨ë‹ˆí„°ë§ ğŸ”\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ BERTì™€ SVMì„ ê²°í•©í•˜ì—¬ í…ìŠ¤íŠ¸ ê¸°ë°˜ì˜ ìŠ¤íŒ¸ íƒì§€ ëª¨ë¸ì„ êµ¬í˜„í•˜ê³ , Weights & Biasesë¥¼ í†µí•´ ì‹¤í—˜ì„ ëª¨ë‹ˆí„°ë§í•©ë‹ˆë‹¤.\n",
    "\n",
    "## ì‹¤í—˜ ì„¤ê³„\n",
    "1. BERTë¥¼ íŠ¹ì§• ì¶”ì¶œê¸°ë¡œ ì‚¬ìš©\n",
    "2. BERTì˜ [CLS] í† í° ì„ë² ë”©ì„ SVMì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©\n",
    "3. SVMìœ¼ë¡œ ìµœì¢… ë¶„ë¥˜ ìˆ˜í–‰\n",
    "4. W&Bë¡œ ì‹¤í—˜ ê²°ê³¼ ì¶”ì  ë° ì‹œê°í™”\n",
    "\n",
    "## ë°ì´í„°ì…‹\n",
    "- SpamAssassin Public Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. í™˜ê²½ ì„¤ì • ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch transformers scikit-learn pandas numpy tqdm kagglehub wandb email\n",
    "!nvidia-smi  # GPU í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import email\n",
    "from tqdm import tqdm\n",
    "import kagglehub\n",
    "import wandb\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. W&B ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb ë¡œê·¸ì¸\n",
    "!wandb login\n",
    "\n",
    "# ì‹¤í—˜ ì„¤ì •\n",
    "config = {\n",
    "    \"architecture\": \"BERT-SVM\",\n",
    "    \"dataset\": \"SpamAssassin\",\n",
    "    \"bert_model\": \"bert-base-uncased\",\n",
    "    \"max_length\": 512,\n",
    "    \"svm_kernel\": \"rbf\",\n",
    "    \"svm_C\": 1.0,\n",
    "    \"batch_size\": 32,\n",
    "    \"random_state\": 42,\n",
    "    \"test_size\": 0.2,\n",
    "    \"experiment_timestamp\": datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "}\n",
    "\n",
    "# wandb ì‹¤í—˜ ì´ˆê¸°í™”\n",
    "run = wandb.init(\n",
    "    project=\"text-spam-detection\",\n",
    "    name=f\"BERT-SVM_{config['experiment_timestamp']}\",\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ë° ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_email(file_path):\n",
    "    \"\"\"ì´ë©”ì¼ íŒŒì¼ì„ íŒŒì‹±í•˜ì—¬ ë³¸ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œ\"\"\"\n",
    "    with open(file_path, 'r', encoding='latin-1') as f:\n",
    "        msg = email.message_from_file(f)\n",
    "\n",
    "    body = \"\"\n",
    "    if msg.is_multipart():\n",
    "        for part in msg.walk():\n",
    "            if part.get_content_type() == \"text/plain\":\n",
    "                body += part.get_payload(decode=True).decode('latin-1', errors='ignore')\n",
    "    else:\n",
    "        body = msg.get_payload(decode=True).decode('latin-1', errors='ignore')\n",
    "\n",
    "    return body.strip()\n",
    "\n",
    "def load_dataset(dataset_path):\n",
    "    \"\"\"ë°ì´í„°ì…‹ ë¡œë“œ ë° ì „ì²˜ë¦¬\"\"\"\n",
    "    texts = []\n",
    "    labels = []\n",
    "    stats = {\"spam\": 0, \"easy_ham\": 0, \"hard_ham\": 0}\n",
    "\n",
    "    # ìŠ¤íŒ¸ ì´ë©”ì¼ ë¡œë“œ\n",
    "    spam_dir = os.path.join(dataset_path, 'spam_2/spam_2')\n",
    "    for file_name in tqdm(os.listdir(spam_dir), desc='Loading spam emails'):\n",
    "        file_path = os.path.join(spam_dir, file_name)\n",
    "        try:\n",
    "            text = parse_email(file_path)\n",
    "            texts.append(text)\n",
    "            labels.append(1)\n",
    "            stats[\"spam\"] += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing {file_path}: {str(e)}\")\n",
    "\n",
    "    # ì •ìƒ ì´ë©”ì¼ ë¡œë“œ (easy_ham)\n",
    "    ham_dir = os.path.join(dataset_path, 'easy_ham/easy_ham')\n",
    "    for file_name in tqdm(os.listdir(ham_dir), desc='Loading easy ham emails'):\n",
    "        file_path = os.path.join(ham_dir, file_name)\n",
    "        try:\n",
    "            text = parse_email(file_path)\n",
    "            texts.append(text)\n",
    "            labels.append(0)\n",
    "            stats[\"easy_ham\"] += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing {file_path}: {str(e)}\")\n",
    "\n",
    "    # ì •ìƒ ì´ë©”ì¼ ë¡œë“œ (hard_ham)\n",
    "    ham_dir = os.path.join(dataset_path, 'hard_ham/hard_ham')\n",
    "    for file_name in tqdm(os.listdir(ham_dir), desc='Loading hard ham emails'):\n",
    "        file_path = os.path.join(ham_dir, file_name)\n",
    "        try:\n",
    "            text = parse_email(file_path)\n",
    "            texts.append(text)\n",
    "            labels.append(0)\n",
    "            stats[\"hard_ham\"] += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing {file_path}: {str(e)}\")\n",
    "\n",
    "    # ë°ì´í„°ì…‹ í†µê³„ ë¡œê¹…\n",
    "    wandb.log({\n",
    "        \"dataset_stats/spam_count\": stats[\"spam\"],\n",
    "        \"dataset_stats/easy_ham_count\": stats[\"easy_ham\"],\n",
    "        \"dataset_stats/hard_ham_count\": stats[\"hard_ham\"],\n",
    "        \"dataset_stats/total_samples\": len(texts),\n",
    "        \"dataset_stats/spam_ratio\": stats[\"spam\"] / len(texts)\n",
    "    })\n",
    "\n",
    "    # ë°ì´í„°ì…‹ ë¶„í¬ ì‹œê°í™”\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(['Spam', 'Easy Ham', 'Hard Ham'], \n",
    "            [stats['spam'], stats['easy_ham'], stats['hard_ham']])\n",
    "    plt.title('Dataset Distribution')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    wandb.log({\"dataset_stats/distribution\": wandb.Image(plt)})\n",
    "    plt.close()\n",
    "\n",
    "    return texts, labels\n",
    "\n",
    "# SpamAssassin ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ\n",
    "dataset_path = kagglehub.dataset_download(\"beatoa/spamassassin-public-corpus\")\n",
    "print(\"Dataset path:\", dataset_path)\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ\n",
    "texts, labels = load_dataset(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. BERT íŠ¹ì§• ì¶”ì¶œê¸° ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertFeatureExtractor:\n",
    "    def __init__(self, model_name='bert-base-uncased', max_length=512):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        self.model = BertModel.from_pretrained(model_name).to(self.device)\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # ëª¨ë¸ ì •ë³´ ë¡œê¹…\n",
    "        wandb.log({\n",
    "            \"model_info/device\": str(self.device),\n",
    "            \"model_info/bert_model\": model_name,\n",
    "            \"model_info/max_length\": max_length\n",
    "        })\n",
    "\n",
    "    def extract_features(self, texts, batch_size=32):\n",
    "        self.model.eval()\n",
    "        all_features = []\n",
    "        total_tokens = 0\n",
    "        truncated_count = 0\n",
    "\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc='Extracting BERT features'):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "            \n",
    "            # í…ìŠ¤íŠ¸ í† í°í™”\n",
    "            inputs = self.tokenizer(\n",
    "                batch_texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                return_tensors='pt'\n",
    "            ).to(self.device)\n",
    "\n",
    "            # í† í°í™” í†µê³„ ìˆ˜ì§‘\n",
    "            total_tokens += inputs.input_ids.numel()\n",
    "            truncated_count += sum(len(self.tokenizer.encode(text)) > self.max_length for text in batch_texts)\n",
    "\n",
    "            # BERT íŠ¹ì§• ì¶”ì¶œ\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "                features = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "                all_features.append(features)\n",
    "\n",
    "        # í† í°í™” í†µê³„ ë¡œê¹…\n",
    "        wandb.log({\n",
    "            \"tokenization/avg_tokens_per_sample\": total_tokens / len(texts),\n",
    "            \"tokenization/truncated_samples\": truncated_count,\n",
    "            \"tokenization/truncation_ratio\": truncated_count / len(texts)\n",
    "        })\n",
    "\n",
    "        return np.vstack(all_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ì‹¤í—˜ ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„°ì…‹ ë¶„í• \n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    texts, labels, \n",
    "    test_size=config['test_size'], \n",
    "    random_state=config['random_state'], \n",
    "    stratify=labels\n",
    ")\n",
    "\n",
    "# ë¶„í•  ì •ë³´ ë¡œê¹…\n",
    "wandb.log({\n",
    "    \"split/train_size\": len(X_train),\n",
    "    \"split/test_size\": len(X_test),\n",
    "    \"split/train_spam_ratio\": sum(y_train) / len(y_train),\n",
    "    \"split/test_spam_ratio\": sum(y_test) / len(y_test)\n",
    "})\n",
    "\n",
    "# BERT íŠ¹ì§• ì¶”ì¶œ\n",
    "print(\"\\nBERT íŠ¹ì§• ì¶”ì¶œ ì¤‘...\")\n",
    "feature_extractor = BertFeatureExtractor(\n",
    "    model_name=config['bert_model'],\n",
    "    max_length=config['max_length']\n",
    ")\n",
    "\n",
    "X_train_features = feature_extractor.extract_features(X_train, batch_size=config['batch_size'])\n",
    "X_test_features = feature_extractor.extract_features(X_test, batch_size=config['batch_size'])\n",
    "\n",
    "# íŠ¹ì§• ì¶”ì¶œ ê²°ê³¼ ë¡œê¹…\n",
    "wandb.log({\n",
    "    \"features/train_shape\": list(X_train_features.shape),\n",
    "    \"features/test_shape\": list(X_test_features.shape),\n",
    "    \"features/mean_value\": float(X_train_features.mean()),\n",
    "    \"features/std_value\": float(X_train_features.std())\n",
    "})\n",
    "\n",
    "# SVM í•™ìŠµ\n",
    "print(\"\\nSVM í•™ìŠµ ì¤‘...\")\n",
    "svm = SVC(kernel=config['svm_kernel'], C=config['svm_C'], probability=True)\n",
    "svm.fit(X_train_features, y_train)\n",
    "\n",
    "# ì˜ˆì¸¡ ë° í‰ê°€\n",
    "y_pred = svm.predict(X_test_features)\n",
    "y_prob = svm.predict_proba(X_test_features)[:, 1]\n",
    "\n",
    "# ë©”íŠ¸ë¦­ ê³„ì‚°\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# ROC ê³¡ì„  ê³„ì‚°\n",
    "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(\"\\nì‹¤í—˜ ê²°ê³¼:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# í˜¼ë™ í–‰ë ¬ ì‹œê°í™”\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "wandb.log({\"evaluation/confusion_matrix\": wandb.Image(plt)})\n",
    "plt.close()\n",
    "\n",
    "# ROC ê³¡ì„  ì‹œê°í™”\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "wandb.log({\"evaluation/roc_curve\": wandb.Image(plt)})\n",
    "plt.close()\n",
    "\n",
    "# ì˜ˆì¸¡ í™•ë¥  ë¶„í¬ ì‹œê°í™”\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(y_prob[y_test == 0], bins=50, alpha=0.5, label='Normal', density=True)\n",
    "plt.hist(y_prob[y_test == 1], bins=50, alpha=0.5, label='Spam', density=True)\n",
    "plt.xlabel('Predicted Spam Probability')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Distribution of Predicted Probabilities')\n",
    "plt.legend()\n",
    "wandb.log({\"evaluation/probability_distribution\": wandb.Image(plt)})\n",
    "plt.close()\n",
    "\n",
    "# ë©”íŠ¸ë¦­ ë¡œê¹…\n",
    "wandb.log({\n",
    "    \"metrics/accuracy\": accuracy,\n",
    "    \"metrics/precision\": precision,\n",
    "    \"metrics/recall\": recall,\n",
    "    \"metrics/f1\": f1,\n",
    "    \"metrics/roc_auc\": roc_auc,\n",
    "    \"metrics/true_positives\": int(conf_matrix[1][1]),\n",
    "    \"metrics/false_positives\": int(conf_matrix[0][1]),\n",
    "    \"metrics/true_negatives\": int(conf_matrix[0][0]),\n",
    "    \"metrics/false_negatives\": int(conf_matrix[1][0])\n",
    "})\n",
    "\n",
    "# ì‹¤í—˜ ì¢…ë£Œ\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ìƒˆë¡œìš´ ì´ë©”ì¼ì— ëŒ€í•œ ì˜ˆì¸¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_email(text, feature_extractor, classifier):\n",
    "    # BERT íŠ¹ì§• ì¶”ì¶œ\n",
    "    features = feature_extractor.extract_features([text])\n",
    "    \n",
    "    # SVM ì˜ˆì¸¡\n",
    "    prediction = classifier.predict(features)[0]\n",
    "    probability = classifier.predict_proba(features)[0]\n",
    "    \n",
    "    return {\n",
    "        'prediction': 'ìŠ¤íŒ¸' if prediction == 1 else 'ì •ìƒ',\n",
    "        'spam_prob': probability[1],\n",
    "        'normal_prob': probability[0]\n",
    "    }\n",
    "\n",
    "# wandb ì‹¤í—˜ ì´ˆê¸°í™”\n",
    "run = wandb.init(\n",
    "    project=\"text-spam-detection\",\n",
    "    name=f\"BERT-SVM_{config['experiment_timestamp']}\",\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì´ë©”ì¼\n",
    "test_email = \"\"\"\n",
    "From: john.doe@company.com\n",
    "Return-Path: <john.doe@company.com>\n",
    "Delivered-To: recipient@localhost.com\n",
    "Received: from localhost (localhost [127.0.0.1])\n",
    "\tby server.company.com (Postfix) with ESMTP id ABC123XYZ\n",
    "\tfor <recipient@localhost.com>; Tue, 15 Feb 2024 10:30:15 +0900 (KST)\n",
    "Received: from mail.company.com (mail.company.com [192.168.1.100])\n",
    "\tby server.company.com (Postfix) with ESMTP id DEF456UVW\n",
    "\tfor <recipient@localhost.com>; Tue, 15 Feb 2024 10:30:14 +0900 (KST)\n",
    "Message-Id: <20240215103014.DEF456UVW@mail.company.com>\n",
    "Date: Tue, 15 Feb 2024 10:30:14 +0900 (KST)\n",
    "To: team@company.com\n",
    "From: \"John Doe\" <john.doe@company.com>\n",
    "MIME-Version: 1.0\n",
    "Content-Type: text/plain; charset=\"UTF-8\"\n",
    "Subject: Project Progress Report\n",
    "\n",
    "Dear Team Members,\n",
    "\n",
    "I would like to share this week's project progress update.\n",
    "\n",
    "1. AI Model Development\n",
    "- Completed implementation of BERT-based text classification model\n",
    "- Achieved 95% test accuracy\n",
    "- Hyperparameter optimization in progress\n",
    "\n",
    "2. Data Preprocessing\n",
    "- Data cleaning completed\n",
    "- Labeling work 80% complete\n",
    "- Additional data collection planned\n",
    "\n",
    "3. Next Week's Plan\n",
    "- Model performance improvement\n",
    "- Start API development\n",
    "- Documentation work\n",
    "\n",
    "Please reply if you have any questions or comments.\n",
    "\n",
    "Best regards,\n",
    "\n",
    "John Doe\n",
    "AI Development Team\n",
    "Company Inc.\n",
    "Tel: 02-123-4567\n",
    "\"\"\"\n",
    "\n",
    "result = predict_email(test_email, feature_extractor, svm)\n",
    "\n",
    "print(\"\\nì˜ˆì¸¡ ê²°ê³¼:\")\n",
    "print(f\"íŒì •: {result['prediction']}\")\n",
    "print(f\"ìŠ¤íŒ¸ í™•ë¥ : {result['spam_prob']:.4f}\")\n",
    "print(f\"ì •ìƒ í™•ë¥ : {result['normal_prob']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
