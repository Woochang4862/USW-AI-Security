"""
M4 Pro MacBook Optimization Setup for MMTD Development
Optimized configuration for Apple Silicon
"""

import torch
import os
import platform

def setup_m4_pro_environment():
    """Setup M4 Pro environment for MMTD development"""
    
    print("ğŸ Setting up M4 Pro MacBook environment for MMTD...")
    
    # 1. Check Apple Silicon and MPS availability
    if platform.processor() == 'arm':
        print("âœ… Apple Silicon detected")
        if torch.backends.mps.is_available():
            print("âœ… MPS (Metal Performance Shaders) available")
            device = torch.device("mps")
        else:
            print("âš ï¸ MPS not available, using CPU")
            device = torch.device("cpu")
    else:
        print("â„¹ï¸ Intel Mac detected")
        device = torch.device("cpu")
    
    # 2. Memory optimization for unified memory
    if hasattr(torch.backends, 'mps'):
        # Enable memory efficient attention
        os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'
        
    # 3. Set optimal thread count for M4 Pro (12 cores)
    torch.set_num_threads(8)  # Leave some cores for system
    
    print(f"ğŸ¯ Device: {device}")
    print(f"ğŸ§µ Threads: {torch.get_num_threads()}")
    
    return device

def get_optimal_batch_size_m4():
    """Get optimal batch size for M4 Pro"""
    # M4 Pro has 18-36GB unified memory
    # Conservative batch size for stability
    if torch.backends.mps.is_available():
        return 32  # Good balance for MPS
    else:
        return 16  # CPU fallback

def optimize_for_interpretability():
    """Settings optimized for interpretability research"""
    return {
        'batch_size': get_optimal_batch_size_m4(),
        'num_workers': 4,  # M4 Pro has good CPU performance
        'pin_memory': False,  # Not needed for MPS
        'persistent_workers': True,
        'prefetch_factor': 2
    }

def print_m4_tips():
    """Print M4 Pro specific optimization tips"""
    print("\nğŸš€ M4 Pro ìµœì í™” íŒ:")
    print("1. MPS ì‚¬ìš©ìœ¼ë¡œ GPU ê°€ì† (Neural Engine í™œìš©)")
    print("2. ë°°ì¹˜ í¬ê¸°: 32 (í†µí•© ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì‚¬ìš©)")
    print("3. ë©€í‹°ìŠ¤ë ˆë”©: 8 threads (12ì½”ì–´ ì¤‘ 8ê°œ ì‚¬ìš©)")
    print("4. ì˜ˆìƒ ì„±ëŠ¥: T4 GPUì˜ 70-80% ìˆ˜ì¤€")
    print("5. ì¥ì : ë©”ëª¨ë¦¬ ì œí•œ ì—†ìŒ, ì•ˆì •ì  ì„±ëŠ¥")

if __name__ == "__main__":
    device = setup_m4_pro_environment()
    settings = optimize_for_interpretability()
    
    print(f"\nğŸ¯ M4 Pro ê¶Œì¥ ì„¤ì •:")
    print(f"   Device: {device}")
    print(f"   ë°°ì¹˜ í¬ê¸°: {settings['batch_size']}")
    print(f"   Workers: {settings['num_workers']}")
    
    print_m4_tips()
    
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    print(f"\nğŸ“ M4 Proì—ì„œ ì‚¬ìš©í•  ì½”ë“œ:")
    print(f"""
# M4 Pro ìµœì í™” ì„¤ì •
device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")

# MMTD í‰ê°€ ì‹¤í–‰
evaluator = MMTDEvaluator(
    checkpoint_dir="MMTD/checkpoints",
    data_path="MMTD/DATA/email_data/pics", 
    csv_path="MMTD/DATA/email_data/EDP.csv",
    device=device,
    batch_size={settings['batch_size']}  # M4 Pro ìµœì í™”
)

# Logistic Regression êµ¬í˜„
logistic_classifier = LogisticRegressionClassifier(
    input_size=768,  # MMTD fusion output
    num_classes=2,
    device=device
)
    """) 