"""
Attention-based Interpretability Analysis for MMTD Models
ë‹¤ì¤‘ëª¨ë‹¬ ìŠ¤íŒ¸ íƒì§€ ëª¨ë¸ì˜ Attention ê¸°ë°˜ í•´ì„ê°€ëŠ¥ì„± ë¶„ì„
"""

import torch
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple, Optional, Any
import json
from pathlib import Path
import cv2
from PIL import Image
import logging
from transformers import AutoTokenizer
from sklearn.preprocessing import MinMaxScaler
import warnings
warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['figure.figsize'] = (12, 8)

logger = logging.getLogger(__name__)

class AttentionAnalyzer:
    """MMTD ëª¨ë¸ì˜ Attention ê¸°ë°˜ í•´ì„ê°€ëŠ¥ì„± ë¶„ì„ê¸°"""
    
    def __init__(self, model, tokenizer, device='cuda'):
        """
        Args:
            model: í›ˆë ¨ëœ MMTD ëª¨ë¸
            tokenizer: BERT í† í¬ë‚˜ì´ì €
            device: ê³„ì‚° ë””ë°”ì´ìŠ¤
        """
        self.model = model
        self.tokenizer = tokenizer
        self.device = device
        self.model.eval()
        
        # Attention hook ë“±ë¡
        self.attention_weights = {}
        self._register_hooks()
        
        logger.info("ğŸ” Attention Analyzer ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _register_hooks(self):
        """Attention weight ì¶”ì¶œì„ ìœ„í•œ hook ë“±ë¡"""
        
        def hook_fn(name):
            def hook(module, input, output):
                if hasattr(output, 'attentions') and output.attentions is not None:
                    self.attention_weights[name] = output.attentions
                elif isinstance(output, tuple) and len(output) > 1:
                    # attentionì´ tupleì˜ ì¼ë¶€ë¡œ ë°˜í™˜ë˜ëŠ” ê²½ìš°
                    for i, item in enumerate(output):
                        if hasattr(item, 'shape') and len(item.shape) == 4:  # attention í˜•íƒœ
                            self.attention_weights[f"{name}_{i}"] = item
            return hook
        
        # ê° ì¸ì½”ë”ì— hook ë“±ë¡
        if hasattr(self.model, 'text_encoder'):
            self.model.text_encoder.register_forward_hook(hook_fn('text_encoder'))
        if hasattr(self.model, 'image_encoder'):
            self.model.image_encoder.register_forward_hook(hook_fn('image_encoder'))
        if hasattr(self.model, 'fusion_layer'):
            self.model.fusion_layer.register_forward_hook(hook_fn('fusion_layer'))
    
    def extract_attention_weights(self, input_ids: torch.Tensor, 
                                pixel_values: torch.Tensor) -> Dict[str, torch.Tensor]:
        """ëª¨ë“  ë ˆë²¨ì˜ attention weights ì¶”ì¶œ"""
        
        self.attention_weights.clear()
        
        with torch.no_grad():
            # Forward pass with attention extraction
            outputs = self.model(
                input_ids=input_ids,
                pixel_values=pixel_values,
                output_attentions=True
            )
        
        # Attention weights ì •ë¦¬
        processed_attentions = {}
        
        for name, weights in self.attention_weights.items():
            if isinstance(weights, (list, tuple)):
                # ì—¬ëŸ¬ ë ˆì´ì–´ì˜ attention
                processed_attentions[name] = [w.cpu() for w in weights]
            else:
                processed_attentions[name] = weights.cpu()
        
        return processed_attentions, outputs
    
    def analyze_cross_modal_attention(self, fusion_attentions: torch.Tensor,
                                    text_length: int, image_length: int) -> Dict[str, torch.Tensor]:
        """Cross-modal attention ë¶„ì„"""
        
        if isinstance(fusion_attentions, (list, tuple)):
            # ë§ˆì§€ë§‰ ë ˆì´ì–´ ì‚¬ìš©
            attention_matrix = fusion_attentions[-1]
        else:
            attention_matrix = fusion_attentions
        
        # í—¤ë“œ í‰ê· 
        if len(attention_matrix.shape) == 4:  # [batch, heads, seq, seq]
            attention_matrix = attention_matrix.mean(dim=1)
        
        batch_size, seq_len, _ = attention_matrix.shape
        
        # ì‹œí€€ìŠ¤ êµ¬ì¡°: [CLS, í…ìŠ¤íŠ¸í† í°ë“¤, SEP, ì´ë¯¸ì§€íŒ¨ì¹˜ë“¤]
        text_end = text_length
        image_start = text_length
        image_end = image_start + image_length
        
        # ì˜ì—­ë³„ attention ì¶”ì¶œ
        regions = {
            'text_to_text': attention_matrix[:, 1:text_end, 1:text_end],
            'text_to_image': attention_matrix[:, 1:text_end, image_start:image_end],
            'image_to_text': attention_matrix[:, image_start:image_end, 1:text_end],
            'image_to_image': attention_matrix[:, image_start:image_end, image_start:image_end],
            'cls_to_text': attention_matrix[:, 0:1, 1:text_end],
            'cls_to_image': attention_matrix[:, 0:1, image_start:image_end],
            'full_matrix': attention_matrix
        }
        
        return regions
    
    def get_token_importance(self, cross_modal_attention: Dict[str, torch.Tensor],
                           tokens: List[str]) -> List[Dict[str, Any]]:
        """í…ìŠ¤íŠ¸ í† í°ë³„ ì¤‘ìš”ë„ ê³„ì‚°"""
        
        text_to_image = cross_modal_attention['text_to_image']
        cls_to_text = cross_modal_attention['cls_to_text']
        
        # ì—¬ëŸ¬ ì¤‘ìš”ë„ ë©”íŠ¸ë¦­ ê³„ì‚°
        token_importance = []
        
        for i, token in enumerate(tokens[1:]):  # CLS ì œì™¸
            if i >= text_to_image.shape[1]:
                break
                
            # 1. ì´ë¯¸ì§€ì— ëŒ€í•œ attention í•©ê³„
            image_attention = text_to_image[0, i, :].sum().item()
            
            # 2. CLS í† í°ìœ¼ë¡œë¶€í„°ì˜ attention
            cls_attention = cls_to_text[0, 0, i].item() if i < cls_to_text.shape[2] else 0
            
            # 3. ìê¸° ìì‹ ì— ëŒ€í•œ attention (text_to_text)
            if 'text_to_text' in cross_modal_attention:
                text_attention = cross_modal_attention['text_to_text'][0, i, i].item() if i < cross_modal_attention['text_to_text'].shape[1] else 0
            else:
                text_attention = 0
            
            # 4. ì¢…í•© ì¤‘ìš”ë„ (ê°€ì¤‘í‰ê· )
            combined_importance = (image_attention * 0.4 + cls_attention * 0.4 + text_attention * 0.2)
            
            token_importance.append({
                'token': token,
                'index': i,
                'image_attention': image_attention,
                'cls_attention': cls_attention,
                'text_attention': text_attention,
                'combined_importance': combined_importance,
                'is_special': token in ['[CLS]', '[SEP]', '[PAD]', '[UNK]']
            })
        
        # íŠ¹ìˆ˜ í† í° ì œì™¸í•˜ê³  ì •ë ¬
        filtered_importance = [t for t in token_importance if not t['is_special']]
        return sorted(filtered_importance, key=lambda x: x['combined_importance'], reverse=True)
    
    def get_patch_importance(self, cross_modal_attention: Dict[str, torch.Tensor],
                           patch_coordinates: Optional[List[Tuple[int, int]]] = None) -> List[Dict[str, Any]]:
        """ì´ë¯¸ì§€ íŒ¨ì¹˜ë³„ ì¤‘ìš”ë„ ê³„ì‚°"""
        
        image_to_text = cross_modal_attention['image_to_text']
        cls_to_image = cross_modal_attention['cls_to_image']
        
        num_patches = image_to_text.shape[1]
        
        # íŒ¨ì¹˜ ì¢Œí‘œê°€ ì—†ìœ¼ë©´ ìƒì„± (14x14 grid ê°€ì •)
        if patch_coordinates is None:
            patch_size = int(np.sqrt(num_patches))
            patch_coordinates = [(i // patch_size, i % patch_size) for i in range(num_patches)]
        
        patch_importance = []
        
        for i in range(min(num_patches, len(patch_coordinates))):
            # 1. í…ìŠ¤íŠ¸ë¡œë¶€í„° ë°›ëŠ” attention í•©ê³„
            text_attention = image_to_text[0, i, :].sum().item()
            
            # 2. CLS í† í°ìœ¼ë¡œë¶€í„°ì˜ attention
            cls_attention = cls_to_image[0, 0, i].item() if i < cls_to_image.shape[2] else 0
            
            # 3. ë‹¤ë¥¸ íŒ¨ì¹˜ë¡œë¶€í„°ì˜ attention
            if 'image_to_image' in cross_modal_attention:
                image_attention = cross_modal_attention['image_to_image'][0, i, :].sum().item() - cross_modal_attention['image_to_image'][0, i, i].item()
            else:
                image_attention = 0
            
            # 4. ì¢…í•© ì¤‘ìš”ë„
            combined_importance = (text_attention * 0.5 + cls_attention * 0.3 + image_attention * 0.2)
            
            patch_importance.append({
                'patch_index': i,
                'coordinates': patch_coordinates[i] if i < len(patch_coordinates) else (0, 0),
                'text_attention': text_attention,
                'cls_attention': cls_attention,
                'image_attention': image_attention,
                'combined_importance': combined_importance
            })
        
        return sorted(patch_importance, key=lambda x: x['combined_importance'], reverse=True)
    
    def attention_rollout(self, attentions: List[torch.Tensor]) -> torch.Tensor:
        """Attention Rollout: ë ˆì´ì–´ë³„ attention ëˆ„ì  ê³„ì‚°"""
        
        if not attentions:
            return None
        
        # ì²« ë²ˆì§¸ attentionìœ¼ë¡œ ì´ˆê¸°í™”
        result = attentions[0].mean(dim=1)  # í—¤ë“œ í‰ê· 
        
        # ê° ë ˆì´ì–´ì˜ attentionì„ ìˆœì°¨ì ìœ¼ë¡œ ê³±í•¨
        for attention in attentions[1:]:
            avg_attention = attention.mean(dim=1)
            # Residual connection ê³ ë ¤
            avg_attention = avg_attention + torch.eye(avg_attention.size(-1)).to(avg_attention.device)
            avg_attention = avg_attention / avg_attention.sum(dim=-1, keepdim=True)
            
            result = torch.matmul(avg_attention, result)
        
        return result
    
    def gradient_weighted_attention(self, input_ids: torch.Tensor,
                                  pixel_values: torch.Tensor,
                                  target_class: int = 1) -> Dict[str, torch.Tensor]:
        """Gradient-weighted Attention ê³„ì‚°"""
        
        # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°ì„ ìœ„í•´ requires_grad ì„¤ì •
        input_ids = input_ids.clone().detach().requires_grad_(True)
        pixel_values = pixel_values.clone().detach().requires_grad_(True)
        
        self.model.train()  # gradient ê³„ì‚°ì„ ìœ„í•´ train ëª¨ë“œ
        
        try:
            # Forward pass
            outputs = self.model(input_ids=input_ids, pixel_values=pixel_values, output_attentions=True)
            
            # Target classì˜ logitì— ëŒ€í•œ loss ê³„ì‚°
            target_logit = outputs.logits[0, target_class]
            
            # Backward pass
            target_logit.backward()
            
            # Attentionê³¼ gradientì˜ ê³±
            weighted_attentions = {}
            
            for name, attentions in self.attention_weights.items():
                if isinstance(attentions, (list, tuple)):
                    weighted_attentions[name] = []
                    for attention in attentions:
                        if attention.requires_grad:
                            grad = torch.autograd.grad(target_logit, attention, retain_graph=True)[0]
                            weighted = attention * grad
                            weighted_attentions[name].append(weighted.mean(dim=1))  # í—¤ë“œ í‰ê· 
                else:
                    if attentions.requires_grad:
                        grad = torch.autograd.grad(target_logit, attentions, retain_graph=True)[0]
                        weighted = attentions * grad
                        weighted_attentions[name] = weighted.mean(dim=1)
            
            return weighted_attentions
            
        finally:
            self.model.eval()  # ë‹¤ì‹œ eval ëª¨ë“œë¡œ
    
    def explain_prediction(self, text: str, image: torch.Tensor,
                         return_attention_maps: bool = True) -> Dict[str, Any]:
        """ì˜ˆì¸¡ì— ëŒ€í•œ ì¢…í•©ì ì¸ í•´ì„ ì œê³µ"""
        
        # í…ìŠ¤íŠ¸ í† í°í™”
        encoding = self.tokenizer(
            text,
            padding='max_length',
            truncation=True,
            max_length=512,
            return_tensors='pt'
        )
        
        input_ids = encoding['input_ids'].to(self.device)
        tokens = self.tokenizer.convert_ids_to_tokens(input_ids[0])
        
        # ì´ë¯¸ì§€ ì¤€ë¹„
        if len(image.shape) == 3:
            image = image.unsqueeze(0)
        image = image.to(self.device)
        
        # ì˜ˆì¸¡ ìˆ˜í–‰
        with torch.no_grad():
            outputs = self.model(input_ids=input_ids, pixel_values=image)
            prediction = torch.sigmoid(outputs.logits).cpu().numpy()[0, 0]
            predicted_class = int(prediction > 0.5)
        
        # Attention ë¶„ì„
        attentions, full_outputs = self.extract_attention_weights(input_ids, image)
        
        # ì‹œí€€ìŠ¤ ê¸¸ì´ ê³„ì‚°
        text_length = (input_ids != self.tokenizer.pad_token_id).sum().item()
        image_length = 196  # 14x14 patches for BEiT
        
        # Cross-modal attention ë¶„ì„
        if 'fusion_layer' in attentions:
            cross_modal = self.analyze_cross_modal_attention(
                attentions['fusion_layer'], text_length, image_length
            )
        else:
            cross_modal = {}
        
        # í† í° ì¤‘ìš”ë„
        token_importance = self.get_token_importance(cross_modal, tokens) if cross_modal else []
        
        # íŒ¨ì¹˜ ì¤‘ìš”ë„
        patch_importance = self.get_patch_importance(cross_modal) if cross_modal else []
        
        # ê²°ê³¼ ì •ë¦¬
        explanation = {
            'prediction': {
                'score': float(prediction),
                'class': predicted_class,
                'confidence': abs(prediction - 0.5) * 2,
                'label': 'SPAM' if predicted_class == 1 else 'HAM'
            },
            'text_analysis': {
                'original_text': text,
                'tokens': tokens,
                'important_tokens': token_importance[:10],
                'text_length': text_length
            },
            'image_analysis': {
                'important_patches': patch_importance[:20],
                'image_shape': image.shape,
                'num_patches': image_length
            },
            'cross_modal_analysis': {
                'text_to_image_strength': float(cross_modal.get('text_to_image', torch.tensor(0)).mean()) if cross_modal else 0,
                'image_to_text_strength': float(cross_modal.get('image_to_text', torch.tensor(0)).mean()) if cross_modal else 0,
                'modality_balance': self._calculate_modality_balance(cross_modal) if cross_modal else 0.5
            }
        }
        
        if return_attention_maps:
            explanation['attention_maps'] = {
                'cross_modal_attention': cross_modal,
                'all_attentions': attentions
            }
        
        return explanation
    
    def _calculate_modality_balance(self, cross_modal: Dict[str, torch.Tensor]) -> float:
        """ëª¨ë‹¬ë¦¬í‹°ê°„ ê· í˜• ê³„ì‚° (0: í…ìŠ¤íŠ¸ ì¤‘ì‹¬, 1: ì´ë¯¸ì§€ ì¤‘ì‹¬)"""
        
        if not cross_modal:
            return 0.5
        
        text_contribution = 0
        image_contribution = 0
        
        if 'cls_to_text' in cross_modal:
            text_contribution += cross_modal['cls_to_text'].sum().item()
        if 'cls_to_image' in cross_modal:
            image_contribution += cross_modal['cls_to_image'].sum().item()
        
        total = text_contribution + image_contribution
        if total > 0:
            return image_contribution / total
        return 0.5
    
    def save_explanation(self, explanation: Dict[str, Any], 
                        output_path: str, include_attention_maps: bool = False):
        """í•´ì„ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        
        # Attention mapsëŠ” ìš©ëŸ‰ì´ í¬ë¯€ë¡œ ì„ íƒì ìœ¼ë¡œ ì €ì¥
        if not include_attention_maps and 'attention_maps' in explanation:
            del explanation['attention_maps']
        
        # Tensorë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        def convert_tensors(obj):
            if isinstance(obj, torch.Tensor):
                return obj.tolist()
            elif isinstance(obj, dict):
                return {k: convert_tensors(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_tensors(item) for item in obj]
            return obj
        
        explanation_serializable = convert_tensors(explanation)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(explanation_serializable, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ğŸ’¾ í•´ì„ ê²°ê³¼ ì €ì¥: {output_path}") 