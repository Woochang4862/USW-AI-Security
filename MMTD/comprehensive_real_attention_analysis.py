import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple, Optional
import sys
sys.path.append('.')
from PIL import Image, ImageDraw, ImageFont
import random

from models import MMTD
from transformers import AutoTokenizer
import warnings
warnings.filterwarnings("ignore")

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'AppleGothic'
plt.rcParams['axes.unicode_minus'] = False

class ComprehensiveRealAttentionAnalysis:
    """
    ì‹¤ì œ MMTD ëª¨ë¸ì„ ì‚¬ìš©í•œ í¬ê´„ì ì¸ attention ë¶„ì„
    - ë©€í‹°ì–¸ì–´ ìƒ˜í”Œ ë¶„ì„
    - ì›ë³¸ ì´ë¯¸ì§€ì™€ attention ì˜¤ë²„ë ˆì´
    - ëª¨ë‹¬ë¦¬í‹°ë³„ ê¸°ì—¬ë„ ë¶„ì„
    - í¬ë¡œìŠ¤ ëª¨ë‹¬ íŒ¨í„´ ë¶„ì„
    """
    
    def __init__(self, checkpoint_path: str = "checkpoints/fold1/checkpoint-939/pytorch_model.bin"):
        self.device = torch.device("cuda" if torch.cuda.is_available() else 
                                  "mps" if torch.backends.mps.is_available() else "cpu")
        self.checkpoint_path = checkpoint_path
        
        # ëª¨ë¸ ë° ì „ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
        self.model = None
        self.tokenizer = None
        
        # ë¶„ì„ ê²°ê³¼ ì €ì¥
        self.analysis_results = []
        
        print(f"ğŸ”§ í¬ê´„ì  ì‹¤ì œ MMTD Attention ë¶„ì„ê¸° ì´ˆê¸°í™” (ë””ë°”ì´ìŠ¤: {self.device})")
    
    def load_model_and_checkpoint(self):
        """ì‹¤ì œ MMTD ëª¨ë¸ê³¼ ì²´í¬í¬ì¸íŠ¸ ë¡œë”©"""
        print("\nğŸ“‚ ì‹¤ì œ MMTD ëª¨ë¸ ë¡œë”©...")
        
        try:
            # ëª¨ë¸ ìƒì„±
            self.model = MMTD(
                bert_pretrain_weight='bert-base-multilingual-cased',
                beit_pretrain_weight='microsoft/dit-base'
            )
            
            # â˜… í•µì‹¬: output_attentions=True ì„¤ì •
            self.model.text_encoder.config.output_attentions = True
            self.model.image_encoder.config.output_attentions = True
            
            # ì²´í¬í¬ì¸íŠ¸ ë¡œë”©
            checkpoint = torch.load(self.checkpoint_path, map_location='cpu')
            missing_keys, unexpected_keys = self.model.load_state_dict(checkpoint, strict=False)
            
            # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •í•˜ê³  ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            self.model.eval()
            self.model.to(self.device)
            
            print(f"âœ… MMTD ëª¨ë¸ ë¡œë”© ì„±ê³µ ({sum(p.numel() for p in self.model.parameters()):,} íŒŒë¼ë¯¸í„°)")
            print(f"   Missing keys: {len(missing_keys)}, Unexpected keys: {len(unexpected_keys)}")
            
            # í† í¬ë‚˜ì´ì € ë¡œë”©
            self.tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')
            
            print("âœ… í† í¬ë‚˜ì´ì € ë¡œë”© ì™„ë£Œ")
            return True
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def create_realistic_email_image(self, text: str, is_spam: bool = True, language: str = "ko"):
        """í˜„ì‹¤ì ì¸ ì´ë©”ì¼ ì´ë¯¸ì§€ ìƒì„±"""
        # ì´ë¯¸ì§€ í¬ê¸°
        img_size = (224, 224)
        
        # ë°°ê²½ìƒ‰ ì„¤ì •
        if is_spam:
            # ìŠ¤íŒ¸: í™”ë ¤í•œ ë°°ê²½ìƒ‰ (ë¹¨ê°•, ë…¸ë‘ ê³„ì—´)
            bg_colors = [(255, 100, 100), (255, 200, 50), (255, 150, 150), (255, 180, 0)]
        else:
            # ì •ìƒ: ì°¨ë¶„í•œ ë°°ê²½ìƒ‰ (í°ìƒ‰, íšŒìƒ‰ ê³„ì—´)
            bg_colors = [(255, 255, 255), (240, 240, 240), (250, 250, 250), (245, 245, 245)]
        
        bg_color = random.choice(bg_colors)
        img = Image.new('RGB', img_size, bg_color)
        draw = ImageDraw.Draw(img)
        
        # í…ìŠ¤íŠ¸ ì¶”ê°€ (ì´ë¯¸ì§€ ìœ„ì—)
        try:
            font_size = random.randint(12, 20)
            # ê¸°ë³¸ í°íŠ¸ ì‚¬ìš©
            font = ImageFont.load_default()
        except:
            font = None
        
        # í…ìŠ¤íŠ¸ ìƒ‰ìƒ
        if is_spam:
            text_colors = [(255, 255, 255), (255, 255, 0), (0, 0, 0)]  # í°ìƒ‰, ë…¸ë‘, ê²€ì •
        else:
            text_colors = [(0, 0, 0), (50, 50, 50), (100, 100, 100)]  # ê²€ì •, íšŒìƒ‰ ê³„ì—´
        
        text_color = random.choice(text_colors)
        
        # ì§§ì€ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì²˜ìŒ 30ì)
        short_text = text[:30] + "..." if len(text) > 30 else text
        
        # í…ìŠ¤íŠ¸ ìœ„ì¹˜ (ì¤‘ì•™)
        try:
            bbox = draw.textbbox((0, 0), short_text, font=font)
            text_width = bbox[2] - bbox[0]
            text_height = bbox[3] - bbox[1]
            x = (img_size[0] - text_width) // 2
            y = (img_size[1] - text_height) // 2
            draw.text((x, y), short_text, fill=text_color, font=font)
        except:
            # í°íŠ¸ ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ ìœ„ì¹˜ì— í…ìŠ¤íŠ¸
            draw.text((20, 100), short_text, fill=text_color)
        
        # ìŠ¤íŒ¸ì¸ ê²½ìš° ì¶”ê°€ ì¥ì‹ ìš”ì†Œ
        if is_spam:
            # ëœë¤í•œ ì›ì´ë‚˜ ì‚¬ê°í˜• ì¶”ê°€
            for _ in range(random.randint(2, 5)):
                x1, y1 = random.randint(0, 180), random.randint(0, 180)
                x2, y2 = x1 + random.randint(20, 40), y1 + random.randint(20, 40)
                color = (random.randint(0, 255), random.randint(0, 255), random.randint(0, 255))
                if random.choice([True, False]):
                    draw.ellipse([x1, y1, x2, y2], outline=color, width=2)
                else:
                    draw.rectangle([x1, y1, x2, y2], outline=color, width=2)
        
        # PILì„ numpy ë°°ì—´ë¡œ ë³€í™˜í•˜ê³  torch tensorë¡œ
        img_array = np.array(img)
        img_tensor = torch.from_numpy(img_array).permute(2, 0, 1).float() / 255.0
        return img_tensor.unsqueeze(0), img_array  # [1, 3, 224, 224], numpy array for display
    
    def create_sample_input(self, text: str, language: str = "ko", is_spam: bool = True):
        """ìƒ˜í”Œ ì…ë ¥ ë°ì´í„° ìƒì„±"""
        # í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì§•
        text_inputs = self.tokenizer(
            text,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=128
        )
        
        # í˜„ì‹¤ì ì¸ ì´ë©”ì¼ ì´ë¯¸ì§€ ìƒì„±
        image_tensor, image_display = self.create_realistic_email_image(text, is_spam, language)
        
        # ì…ë ¥ ë°ì´í„° êµ¬ì„±
        inputs = {
            'input_ids': text_inputs['input_ids'].to(self.device),
            'attention_mask': text_inputs['attention_mask'].to(self.device),
            'token_type_ids': torch.zeros_like(text_inputs['input_ids']).to(self.device),
            'pixel_values': image_tensor.to(self.device)
        }
        
        return inputs, text, image_display
    
    def extract_attention_weights(self, inputs: Dict[str, torch.Tensor]):
        """ì‹¤ì œ attention ê°€ì¤‘ì¹˜ ì¶”ì¶œ"""
        attention_data = {}
        
        with torch.no_grad():
            # 1. BERT í…ìŠ¤íŠ¸ ì¸ì½”ë” ì‹¤í–‰
            text_inputs = {k: v for k, v in inputs.items() if k != 'pixel_values'}
            text_outputs = self.model.text_encoder(**text_inputs)
            
            # BERT attention ì¶”ì¶œ
            if hasattr(text_outputs, 'attentions') and text_outputs.attentions is not None:
                attention_data['bert_attentions'] = [att.cpu() for att in text_outputs.attentions]
            
            # 2. BEiT ì´ë¯¸ì§€ ì¸ì½”ë” ì‹¤í–‰
            image_outputs = self.model.image_encoder(pixel_values=inputs['pixel_values'])
            
            # BEiT attention ì¶”ì¶œ
            if hasattr(image_outputs, 'attentions') and image_outputs.attentions is not None:
                attention_data['beit_attentions'] = [att.cpu() for att in image_outputs.attentions]
            
            # 3. ì „ì²´ ëª¨ë¸ ì‹¤í–‰
            full_outputs = self.model(**inputs)
            
            # ì˜ˆì¸¡ ê²°ê³¼
            prediction = torch.softmax(full_outputs.logits, dim=-1)
            predicted_class = torch.argmax(prediction, dim=-1).item()
            confidence = prediction.max().item()
            
            attention_data.update({
                'prediction': prediction.cpu().numpy(),
                'predicted_class': predicted_class,
                'confidence': confidence,
                'text_hidden_states': text_outputs.hidden_states,
                'image_hidden_states': image_outputs.hidden_states
            })
        
        return attention_data
    
    def analyze_single_sample(self, text: str, language: str = "ko", is_spam: bool = True, sample_id: int = 1):
        """ë‹¨ì¼ ìƒ˜í”Œ ë¶„ì„"""
        print(f"\nğŸ“Š ìƒ˜í”Œ {sample_id} ë¶„ì„: {language.upper()} {'ìŠ¤íŒ¸' if is_spam else 'ì •ìƒ'}")
        print(f"   í…ìŠ¤íŠ¸: {text}")
        
        # ì…ë ¥ ìƒì„±
        inputs, processed_text, image_display = self.create_sample_input(text, language, is_spam)
        
        # Attention ì¶”ì¶œ
        attention_data = self.extract_attention_weights(inputs)
        
        # ê²°ê³¼ ì €ì¥
        result = {
            'sample_id': sample_id,
            'text': text,
            'language': language,
            'is_spam': is_spam,
            'predicted_class': attention_data['predicted_class'],
            'confidence': attention_data['confidence'],
            'bert_attentions': attention_data.get('bert_attentions'),
            'beit_attentions': attention_data.get('beit_attentions'),
            'image_display': image_display,
            'tokens': self.tokenizer.tokenize(text)
        }
        
        self.analysis_results.append(result)
        
        print(f"   ì˜ˆì¸¡: {'ìŠ¤íŒ¸' if attention_data['predicted_class'] == 1 else 'ì •ìƒ'} "
              f"(ì‹ ë¢°ë„: {attention_data['confidence']:.3f})")
        
        return result
    
    def visualize_image_attention_with_original(self, result: Dict, layer_idx: int = 11):
        """ì›ë³¸ ì´ë¯¸ì§€ì™€ í•¨ê»˜ ì´ë¯¸ì§€ attention ì‹œê°í™”"""
        if 'beit_attentions' not in result or result['beit_attentions'] is None:
            print("âŒ BEiT attention ë°ì´í„° ì—†ìŒ")
            return
        
        # ì§€ì •ëœ ë ˆì´ì–´ì˜ attention
        attention = result['beit_attentions'][layer_idx]  # [batch, heads, patches, patches]
        attention_avg = attention[0].mean(dim=0)  # [patches, patches]
        
        # CLS í† í° ì œì™¸
        if attention_avg.shape[0] > 196:  # 14x14=196 íŒ¨ì¹˜ + CLS
            image_attention = attention_avg[1:, 1:]  # CLS ì œê±°
        else:
            image_attention = attention_avg
        
        # íŒ¨ì¹˜ attentionì„ ì´ë¯¸ì§€ í˜•íƒœë¡œ ì¬êµ¬ì„±
        num_patches = int(np.sqrt(image_attention.shape[0]))
        
        if num_patches**2 == image_attention.shape[0]:
            self_attention = torch.diag(image_attention)
            attention_map = self_attention.reshape(num_patches, num_patches).numpy()
        else:
            attention_map = image_attention.mean(dim=1).reshape(num_patches, num_patches).numpy()
        
        # ì‹œê°í™”
        fig, axes = plt.subplots(1, 3, figsize=(18, 6))
        
        # 1. ì›ë³¸ ì´ë¯¸ì§€
        axes[0].imshow(result['image_display'])
        axes[0].set_title(f'ì›ë³¸ ì´ë©”ì¼ ì´ë¯¸ì§€\n{result["language"].upper()} {"ìŠ¤íŒ¸" if result["is_spam"] else "ì •ìƒ"}')
        axes[0].axis('off')
        
        # 2. Attention íˆíŠ¸ë§µ
        im1 = axes[1].imshow(attention_map, cmap='viridis', interpolation='nearest')
        axes[1].set_title(f'BEiT Attention Map (ë ˆì´ì–´ {layer_idx})')
        axes[1].set_xlabel('ì´ë¯¸ì§€ íŒ¨ì¹˜ (X)')
        axes[1].set_ylabel('ì´ë¯¸ì§€ íŒ¨ì¹˜ (Y)')
        plt.colorbar(im1, ax=axes[1])
        
        # 3. ì›ë³¸ ì´ë¯¸ì§€ + Attention ì˜¤ë²„ë ˆì´
        # Attention ë§µì„ ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°ë¡œ ì—…ìƒ˜í”Œë§
        upsampled = torch.nn.functional.interpolate(
            torch.tensor(attention_map).unsqueeze(0).unsqueeze(0).float(), 
            size=(224, 224), 
            mode='bilinear'
        )[0, 0].numpy()
        
        # ì›ë³¸ ì´ë¯¸ì§€ í‘œì‹œ
        axes[2].imshow(result['image_display'])
        # Attention ì˜¤ë²„ë ˆì´ (íˆ¬ëª…ë„ ì ìš©)
        im2 = axes[2].imshow(upsampled, cmap='hot', alpha=0.5, interpolation='bilinear')
        axes[2].set_title('ì›ë³¸ + Attention ì˜¤ë²„ë ˆì´')
        axes[2].axis('off')
        plt.colorbar(im2, ax=axes[2])
        
        plt.suptitle(f'ì‹¤ì œ ì´ë¯¸ì§€ Attention ë¶„ì„ - ìƒ˜í”Œ {result["sample_id"]}\n'
                    f'ì˜ˆì¸¡: {"ìŠ¤íŒ¸" if result["predicted_class"] == 1 else "ì •ìƒ"} '
                    f'(ì‹ ë¢°ë„: {result["confidence"]:.3f})')
        plt.tight_layout()
        
        filename = f'real_image_attention_sample_{result["sample_id"]}.png'
        plt.savefig(filename, dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"âœ… ì´ë¯¸ì§€ attention ì‹œê°í™” ì €ì¥: {filename}")
    
    def visualize_text_attention(self, result: Dict, layer_idx: int = 11):
        """í…ìŠ¤íŠ¸ attention ì‹œê°í™”"""
        if 'bert_attentions' not in result or result['bert_attentions'] is None:
            print("âŒ BERT attention ë°ì´í„° ì—†ìŒ")
            return
        
        # í† í° ì¤€ë¹„
        tokens = ['[CLS]'] + result['tokens'] + ['[SEP]']
        
        # ì§€ì •ëœ ë ˆì´ì–´ì˜ attention
        attention = result['bert_attentions'][layer_idx]  # [batch, heads, seq, seq]
        attention_avg = attention[0].mean(dim=0)  # [seq, seq]
        
        # ì‹œê°í™”
        plt.figure(figsize=(12, 10))
        
        # í…ìŠ¤íŠ¸ ê¸¸ì´ì— ë§ê²Œ ì¡°ì •
        seq_len = min(len(tokens), attention_avg.shape[0])
        attention_matrix = attention_avg[:seq_len, :seq_len].numpy()
        
        sns.heatmap(
            attention_matrix, 
            xticklabels=tokens[:seq_len], 
            yticklabels=tokens[:seq_len],
            cmap='Blues',
            annot=False,
            fmt='.3f',
            cbar_kws={'label': 'Attention ê°€ì¤‘ì¹˜'}
        )
        
        plt.title(f'ì‹¤ì œ BERT í…ìŠ¤íŠ¸ Attention - ìƒ˜í”Œ {result["sample_id"]} (ë ˆì´ì–´ {layer_idx})\n'
                 f'{result["language"].upper()} {"ìŠ¤íŒ¸" if result["is_spam"] else "ì •ìƒ"} | '
                 f'ì˜ˆì¸¡: {"ìŠ¤íŒ¸" if result["predicted_class"] == 1 else "ì •ìƒ"} '
                 f'(ì‹ ë¢°ë„: {result["confidence"]:.3f})')
        plt.xlabel('To Tokens')
        plt.ylabel('From Tokens')
        plt.xticks(rotation=45, ha='right')
        plt.yticks(rotation=0)
        plt.tight_layout()
        
        filename = f'real_text_attention_sample_{result["sample_id"]}.png'
        plt.savefig(filename, dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"âœ… í…ìŠ¤íŠ¸ attention ì‹œê°í™” ì €ì¥: {filename}")
    
    def run_comprehensive_analysis(self):
        """í¬ê´„ì ì¸ ì‹¤ì œ attention ë¶„ì„ ì‹¤í–‰"""
        print("ğŸš€ í¬ê´„ì ì¸ ì‹¤ì œ MMTD Attention ë¶„ì„ ì‹œì‘")
        print("="*80)
        
        # ëª¨ë¸ ë¡œë”©
        if not self.load_model_and_checkpoint():
            return False
        
        # ë©€í‹°ì–¸ì–´ ìƒ˜í”Œ ë°ì´í„°
        samples = [
            # í•œêµ­ì–´
            ("ë¬´ë£Œ ìƒí’ˆì„ ë°›ìœ¼ì„¸ìš”! ì§€ê¸ˆ í´ë¦­í•˜ì„¸ìš”!", "ko", True),
            ("ì•ˆë…•í•˜ì„¸ìš”. íšŒì˜ ì¼ì •ì„ ì•Œë ¤ë“œë¦½ë‹ˆë‹¤.", "ko", False),
            
            # ì˜ì–´
            ("FREE MONEY! Click here NOW!!!", "en", True),
            ("Hello, please find the meeting agenda attached.", "en", False),
            
            # ì¼ë³¸ì–´
            ("ãŠã‚ã§ã¨ã†ã”ã–ã„ã¾ã™ï¼è³å“ãŒå½“é¸ã—ã¾ã—ãŸï¼", "ja", True),
            ("ä¼šè­°ã®è³‡æ–™ã‚’ãŠé€ã‚Šã—ã¾ã™ã€‚", "ja", False),
            
            # ì¤‘êµ­ì–´
            ("æ­å–œæ‚¨ä¸­å¥–äº†ï¼ç«‹å³ç‚¹å‡»é¢†å–å¥–å“ï¼", "zh", True),
            ("è¯·æŸ¥æ”¶ä¼šè®®èµ„æ–™ï¼Œè°¢è°¢ã€‚", "zh", False),
        ]
        
        print(f"\nğŸ“Š {len(samples)}ê°œ ë©€í‹°ì–¸ì–´ ìƒ˜í”Œ ë¶„ì„ ì‹œì‘...")
        
        # ê° ìƒ˜í”Œ ë¶„ì„
        for i, (text, language, is_spam) in enumerate(samples, 1):
            result = self.analyze_single_sample(text, language, is_spam, i)
            
            # ì‹œê°í™”
            self.visualize_text_attention(result, layer_idx=11)
            self.visualize_image_attention_with_original(result, layer_idx=11)
        
        # ì¢…í•© ë¶„ì„
        self.generate_comprehensive_summary()
        
        print("\nğŸ‰ í¬ê´„ì ì¸ ì‹¤ì œ MMTD Attention ë¶„ì„ ì™„ë£Œ!")
        return True
    
    def generate_comprehensive_summary(self):
        """ì¢…í•© ë¶„ì„ ê²°ê³¼ ìš”ì•½"""
        print("\nğŸ“ˆ ì¢…í•© ë¶„ì„ ê²°ê³¼ ìš”ì•½")
        print("="*60)
        
        total_samples = len(self.analysis_results)
        correct_predictions = sum(1 for r in self.analysis_results 
                                if (r['predicted_class'] == 1) == r['is_spam'])
        accuracy = correct_predictions / total_samples
        
        print(f"   ì´ ìƒ˜í”Œ ìˆ˜: {total_samples}")
        print(f"   ì •í™•í•œ ì˜ˆì¸¡: {correct_predictions}")
        print(f"   ì •í™•ë„: {accuracy:.1%}")
        
        # ì–¸ì–´ë³„ ë¶„ì„
        languages = {}
        for result in self.analysis_results:
            lang = result['language']
            if lang not in languages:
                languages[lang] = {'total': 0, 'correct': 0, 'avg_confidence': 0}
            languages[lang]['total'] += 1
            if (result['predicted_class'] == 1) == result['is_spam']:
                languages[lang]['correct'] += 1
            languages[lang]['avg_confidence'] += result['confidence']
        
        print(f"\n   ì–¸ì–´ë³„ ì„±ëŠ¥:")
        for lang, stats in languages.items():
            avg_conf = stats['avg_confidence'] / stats['total']
            acc = stats['correct'] / stats['total']
            print(f"     {lang.upper()}: ì •í™•ë„ {acc:.1%}, í‰ê·  ì‹ ë¢°ë„ {avg_conf:.3f}")
        
        # ìŠ¤íŒ¸/ì •ìƒë³„ ë¶„ì„
        spam_results = [r for r in self.analysis_results if r['is_spam']]
        normal_results = [r for r in self.analysis_results if not r['is_spam']]
        
        spam_acc = sum(1 for r in spam_results if r['predicted_class'] == 1) / len(spam_results)
        normal_acc = sum(1 for r in normal_results if r['predicted_class'] == 0) / len(normal_results)
        
        print(f"\n   í´ë˜ìŠ¤ë³„ ì„±ëŠ¥:")
        print(f"     ìŠ¤íŒ¸ íƒì§€ìœ¨: {spam_acc:.1%}")
        print(f"     ì •ìƒ íƒì§€ìœ¨: {normal_acc:.1%}")


if __name__ == "__main__":
    # í¬ê´„ì ì¸ ì‹¤ì œ attention ë¶„ì„ ì‹¤í–‰
    analyzer = ComprehensiveRealAttentionAnalysis()
    analyzer.run_comprehensive_analysis() 