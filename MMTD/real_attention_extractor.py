import torch
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple, Optional
import sys
sys.path.append('.')

from models import MMTD
from transformers import AutoTokenizer, AutoFeatureExtractor
import warnings
warnings.filterwarnings("ignore")

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'AppleGothic'
plt.rcParams['axes.unicode_minus'] = False

class RealAttentionExtractor:
    """
    ì‹¤ì œ MMTD ëª¨ë¸ì—ì„œ attention ê°€ì¤‘ì¹˜ë¥¼ ì¶”ì¶œí•˜ëŠ” í´ë˜ìŠ¤
    - BERT text encoder attention
    - BEiT image encoder attention  
    - Multi-modality fusion layer attention
    """
    
    def __init__(self, checkpoint_path: str = "checkpoints/fold1/checkpoint-939/pytorch_model.bin"):
        self.device = torch.device("cuda" if torch.cuda.is_available() else 
                                  "mps" if torch.backends.mps.is_available() else "cpu")
        self.checkpoint_path = checkpoint_path
        
        # ëª¨ë¸ ë° ì „ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
        self.model = None
        self.tokenizer = None
        self.feature_extractor = None
        
        # Attention ì €ì¥ìš©
        self.attention_weights = {}
        self.hooks = []
        
        print(f"ğŸ”§ ì‹¤ì œ MMTD Attention ì¶”ì¶œê¸° ì´ˆê¸°í™” (ë””ë°”ì´ìŠ¤: {self.device})")
    
    def load_model_and_checkpoint(self):
        """ì‹¤ì œ MMTD ëª¨ë¸ê³¼ ì²´í¬í¬ì¸íŠ¸ ë¡œë”©"""
        print("\nğŸ“‚ ì‹¤ì œ MMTD ëª¨ë¸ ë¡œë”©...")
        
        try:
            # ëª¨ë¸ ìƒì„±
            self.model = MMTD(
                bert_pretrain_weight='bert-base-multilingual-cased',
                beit_pretrain_weight='microsoft/dit-base'
            )
            
            # ì²´í¬í¬ì¸íŠ¸ ë¡œë”©
            checkpoint = torch.load(self.checkpoint_path, map_location='cpu')
            missing_keys, unexpected_keys = self.model.load_state_dict(checkpoint, strict=False)
            
            # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •í•˜ê³  ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            self.model.eval()
            self.model.to(self.device)
            
            print(f"âœ… MMTD ëª¨ë¸ ë¡œë”© ì„±ê³µ ({sum(p.numel() for p in self.model.parameters()):,} íŒŒë¼ë¯¸í„°)")
            print(f"   Missing keys: {len(missing_keys)}, Unexpected keys: {len(unexpected_keys)}")
            
            # í† í¬ë‚˜ì´ì €ì™€ íŠ¹ì§• ì¶”ì¶œê¸° ë¡œë”©
            self.tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')
            self.feature_extractor = AutoFeatureExtractor.from_pretrained('microsoft/dit-base')
            
            print("âœ… í† í¬ë‚˜ì´ì € ë° íŠ¹ì§• ì¶”ì¶œê¸° ë¡œë”© ì™„ë£Œ")
            return True
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def register_attention_hooks(self):
        """Attention ê°€ì¤‘ì¹˜ë¥¼ ìº¡ì²˜í•˜ê¸° ìœ„í•œ Hook ë“±ë¡"""
        print("\nğŸ”— Attention Hook ë“±ë¡...")
        
        def bert_attention_hook(name):
            def hook(module, input, output):
                # BERT self-attentionì˜ attention_probs ìº¡ì²˜
                if hasattr(module, 'attention_probs') and module.attention_probs is not None:
                    self.attention_weights[f'bert_{name}'] = module.attention_probs.detach().cpu()
                # outputì´ íŠœí”Œì¸ ê²½ìš° attention weights í™•ì¸
                elif isinstance(output, tuple) and len(output) >= 2:
                    # output[1]ì´ attention weightsì¸ ê²½ìš°ê°€ ë§ìŒ
                    if hasattr(output[1], 'shape') and len(output[1].shape) >= 3:
                        self.attention_weights[f'bert_{name}'] = output[1].detach().cpu()
            return hook
        
        def beit_attention_hook(name):
            def hook(module, input, output):
                # BEiT self-attentionì˜ attention_probs ìº¡ì²˜
                if hasattr(module, 'attention_probs') and module.attention_probs is not None:
                    self.attention_weights[f'beit_{name}'] = module.attention_probs.detach().cpu()
                # outputì´ íŠœí”Œì¸ ê²½ìš°
                elif isinstance(output, tuple) and len(output) >= 2:
                    if hasattr(output[1], 'shape') and len(output[1].shape) >= 3:
                        self.attention_weights[f'beit_{name}'] = output[1].detach().cpu()
            return hook
        
        def attention_probs_hook(name, module_type):
            """ì‹¤ì œ attention probabilitiesì„ ì§ì ‘ ìº¡ì²˜"""
            def hook(module, input, output):
                # attention_probs ì†ì„±ì´ ìˆëŠ”ì§€ í™•ì¸
                if hasattr(module, 'attention_probs'):
                    attention_probs = module.attention_probs
                    if attention_probs is not None:
                        self.attention_weights[f'{module_type}_{name}'] = attention_probs.detach().cpu()
                        print(f"   ìº¡ì²˜ë¨: {module_type}_{name} - {attention_probs.shape}")
                
                # outputì—ì„œ attention weights ì°¾ê¸°
                if isinstance(output, tuple):
                    for i, out in enumerate(output):
                        if hasattr(out, 'shape') and len(out.shape) >= 3:
                            # attention weightsë¡œ ë³´ì´ëŠ” í…ì„œ
                            if 'attention' in str(type(out)).lower() or len(out.shape) == 4:
                                self.attention_weights[f'{module_type}_{name}_output_{i}'] = out.detach().cpu()
                                print(f"   ìº¡ì²˜ë¨: {module_type}_{name}_output_{i} - {out.shape}")
            return hook
        
        # BERT text encoder attention hooks - ë” êµ¬ì²´ì ìœ¼ë¡œ
        print("  BERT ë ˆì´ì–´ Hook ë“±ë¡...")
        for i, layer in enumerate(self.model.text_encoder.bert.encoder.layer):
            # self-attention hook
            if hasattr(layer.attention, 'self'):
                hook = layer.attention.self.register_forward_hook(
                    attention_probs_hook(f'layer_{i}', 'bert')
                )
                self.hooks.append(hook)
        
        # BEiT image encoder attention hooks - ë” êµ¬ì²´ì ìœ¼ë¡œ  
        print("  BEiT ë ˆì´ì–´ Hook ë“±ë¡...")
        for i, layer in enumerate(self.model.image_encoder.beit.encoder.layer):
            # self-attention hook
            if hasattr(layer, 'attention') and hasattr(layer.attention, 'attention'):
                hook = layer.attention.attention.register_forward_hook(
                    attention_probs_hook(f'layer_{i}', 'beit')
                )
                self.hooks.append(hook)
        
        # Multi-modality transformerì— ëŒ€í•´ì„œë„ ë” ì •êµí•œ hook
        print("  ìœµí•© ë ˆì´ì–´ Hook ë“±ë¡...")
        def fusion_attention_hook(name):
            def hook(module, input, output):
                # TransformerEncoderLayerì˜ self-attention ë¶€ë¶„ ìº¡ì²˜
                if len(input) > 0:
                    self.attention_weights[f'fusion_{name}_input'] = input[0].detach().cpu()
                if isinstance(output, torch.Tensor):
                    self.attention_weights[f'fusion_{name}_output'] = output.detach().cpu()
            return hook
        
        hook = self.model.multi_modality_transformer_layer.register_forward_hook(
            fusion_attention_hook('fusion')
        )
        self.hooks.append(hook)
        
        print(f"âœ… {len(self.hooks)}ê°œ Attention Hook ë“±ë¡ ì™„ë£Œ")
    
    def create_sample_input(self, text: str = "ìŠ¤íŒ¸ ì´ë©”ì¼ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ", image_size: int = 224):
        """ìƒ˜í”Œ ì…ë ¥ ë°ì´í„° ìƒì„±"""
        print(f"\nğŸ“ ìƒ˜í”Œ ì…ë ¥ ìƒì„±: '{text}'")
        
        # í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì§•
        text_inputs = self.tokenizer(
            text,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=128
        )
        
        # ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„± (ì‹¤ì œë¡œëŠ” ì´ë©”ì¼ ì´ë¯¸ì§€ ì‚¬ìš©)
        dummy_image = torch.randn(1, 3, image_size, image_size)
        
        # ì…ë ¥ ë°ì´í„° êµ¬ì„±
        inputs = {
            'input_ids': text_inputs['input_ids'].to(self.device),
            'attention_mask': text_inputs['attention_mask'].to(self.device),
            'token_type_ids': torch.zeros_like(text_inputs['input_ids']).to(self.device),
            'pixel_values': dummy_image.to(self.device)
        }
        
        print(f"âœ… ì…ë ¥ ë°ì´í„° ìƒì„± ì™„ë£Œ:")
        for key, tensor in inputs.items():
            print(f"   {key}: {tensor.shape}")
        
        return inputs, text
    
    def extract_attention_weights(self, inputs: Dict[str, torch.Tensor]):
        """ì‹¤ì œ attention ê°€ì¤‘ì¹˜ ì¶”ì¶œ"""
        print("\nğŸ” ì‹¤ì œ Attention ê°€ì¤‘ì¹˜ ì¶”ì¶œ ì¤‘...")
        
        # ì´ì „ attention ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        self.attention_weights.clear()
        
        # ëª¨ë¸ forward pass (attention hookì´ ìë™ìœ¼ë¡œ í˜¸ì¶œë¨)
        with torch.no_grad():
            outputs = self.model(**inputs)
        
        # ê²°ê³¼ í™•ì¸
        prediction = torch.softmax(outputs.logits, dim=-1)
        predicted_class = torch.argmax(prediction, dim=-1).item()
        confidence = prediction.max().item()
        
        print(f"âœ… Attention ì¶”ì¶œ ì™„ë£Œ!")
        print(f"   ì¶”ì¶œëœ attention ì¢…ë¥˜: {len(self.attention_weights)}ê°œ")
        print(f"   ì˜ˆì¸¡: {'ìŠ¤íŒ¸' if predicted_class == 1 else 'ì •ìƒ'} (ì‹ ë¢°ë„: {confidence:.4f})")
        
        for key, attention in self.attention_weights.items():
            if hasattr(attention, 'shape'):
                print(f"   {key}: {attention.shape}")
        
        return {
            'attention_weights': self.attention_weights.copy(),
            'prediction': prediction.cpu().numpy(),
            'predicted_class': predicted_class,
            'confidence': confidence
        }
    
    def visualize_text_attention(self, text: str, attention_data: Dict, layer_idx: int = 11):
        """í…ìŠ¤íŠ¸ attention ì‹œê°í™”"""
        print(f"\nğŸ“Š í…ìŠ¤íŠ¸ Attention ì‹œê°í™” (ë ˆì´ì–´ {layer_idx})")
        
        # í† í° ë¶„ë¦¬
        tokens = self.tokenizer.tokenize(text)
        tokens = ['[CLS]'] + tokens + ['[SEP]']
        
        # BERT attention ë°ì´í„° ì°¾ê¸°
        bert_key = f'bert_layer_{layer_idx}'
        if bert_key not in attention_data['attention_weights']:
            print(f"âš ï¸ {bert_key} attentionì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return
        
        attention = attention_data['attention_weights'][bert_key]
        
        # Attention í‰ê·  (ëª¨ë“  í—¤ë“œì˜ í‰ê· )
        if len(attention.shape) == 4:  # [batch, heads, seq, seq]
            attention_avg = attention[0].mean(dim=0)  # í—¤ë“œ í‰ê· 
        else:
            attention_avg = attention[0]
        
        # ì‹œê°í™”
        plt.figure(figsize=(12, 10))
        
        # í…ìŠ¤íŠ¸ ê¸¸ì´ì— ë§ê²Œ ì¡°ì •
        seq_len = min(len(tokens), attention_avg.shape[0])
        attention_matrix = attention_avg[:seq_len, :seq_len]
        
        sns.heatmap(
            attention_matrix, 
            xticklabels=tokens[:seq_len], 
            yticklabels=tokens[:seq_len],
            cmap='Blues',
            annot=False,
            fmt='.3f'
        )
        
        plt.title(f'BERT í…ìŠ¤íŠ¸ Attention (ë ˆì´ì–´ {layer_idx})\n'
                 f'ì˜ˆì¸¡: {"ìŠ¤íŒ¸" if attention_data["predicted_class"] == 1 else "ì •ìƒ"} '
                 f'(ì‹ ë¢°ë„: {attention_data["confidence"]:.3f})')
        plt.xlabel('Tokens (To)')
        plt.ylabel('Tokens (From)')
        plt.xticks(rotation=45, ha='right')
        plt.yticks(rotation=0)
        plt.tight_layout()
        
        filename = f'real_bert_attention_layer_{layer_idx}.png'
        plt.savefig(filename, dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"âœ… í…ìŠ¤íŠ¸ attention ì‹œê°í™” ì €ì¥: {filename}")
    
    def visualize_image_attention(self, attention_data: Dict, layer_idx: int = 11):
        """ì´ë¯¸ì§€ attention ì‹œê°í™”"""
        print(f"\nğŸ–¼ï¸ ì´ë¯¸ì§€ Attention ì‹œê°í™” (ë ˆì´ì–´ {layer_idx})")
        
        # BEiT attention ë°ì´í„° ì°¾ê¸°
        beit_key = f'beit_layer_{layer_idx}'
        if beit_key not in attention_data['attention_weights']:
            print(f"âš ï¸ {beit_key} attentionì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return
        
        attention = attention_data['attention_weights'][beit_key]
        
        # Attention í‰ê· 
        if len(attention.shape) == 4:  # [batch, heads, patches, patches]
            attention_avg = attention[0].mean(dim=0)  # í—¤ë“œ í‰ê· 
        else:
            attention_avg = attention[0]
        
        # íŒ¨ì¹˜ ìˆ˜ ê³„ì‚° (ë³´í†µ 14x14 = 196ê°œ íŒ¨ì¹˜)
        num_patches = attention_avg.shape[0]
        patch_size = int(np.sqrt(num_patches))
        
        # CLS í† í° ì œì™¸í•˜ê³  ì´ë¯¸ì§€ íŒ¨ì¹˜ë§Œ ì‚¬ìš©
        if attention_avg.shape[0] > patch_size**2:
            # CLS í† í°ì´ ìˆëŠ” ê²½ìš°
            image_attention = attention_avg[1:, 1:]  # CLS ì œê±°
        else:
            image_attention = attention_avg
        
        # íŒ¨ì¹˜ attentionì„ ì´ë¯¸ì§€ í˜•íƒœë¡œ ì¬êµ¬ì„±
        actual_patches = int(np.sqrt(image_attention.shape[0]))
        if actual_patches**2 == image_attention.shape[0]:
            # ê° íŒ¨ì¹˜ì—ì„œ ìê¸° ìì‹ ì— ëŒ€í•œ attention
            self_attention = torch.diag(image_attention)
            attention_map = self_attention.reshape(actual_patches, actual_patches)
        else:
            # ì „ì²´ attentionì˜ í‰ê· 
            attention_map = image_attention.mean(dim=1).reshape(actual_patches, actual_patches)
        
        # ì‹œê°í™”
        fig, axes = plt.subplots(1, 2, figsize=(15, 7))
        
        # ì›ë³¸ attention íˆíŠ¸ë§µ
        im1 = axes[0].imshow(attention_map, cmap='viridis', interpolation='nearest')
        axes[0].set_title(f'BEiT ì´ë¯¸ì§€ Attention Map (ë ˆì´ì–´ {layer_idx})')
        axes[0].set_xlabel('ì´ë¯¸ì§€ íŒ¨ì¹˜ (X)')
        axes[0].set_ylabel('ì´ë¯¸ì§€ íŒ¨ì¹˜ (Y)')
        plt.colorbar(im1, ax=axes[0])
        
        # ì—…ìƒ˜í”Œë§ëœ attention ë§µ
        upsampled = torch.nn.functional.interpolate(
            attention_map.unsqueeze(0).unsqueeze(0), 
            size=(224, 224), 
            mode='bilinear'
        )[0, 0]
        
        im2 = axes[1].imshow(upsampled, cmap='hot', interpolation='bilinear')
        axes[1].set_title('ê³ í•´ìƒë„ Attention ì˜¤ë²„ë ˆì´')
        axes[1].set_xlabel('í”½ì…€ X')
        axes[1].set_ylabel('í”½ì…€ Y')
        plt.colorbar(im2, ax=axes[1])
        
        plt.suptitle(f'ì‹¤ì œ BEiT ì´ë¯¸ì§€ Attention ë¶„ì„\n'
                    f'ì˜ˆì¸¡: {"ìŠ¤íŒ¸" if attention_data["predicted_class"] == 1 else "ì •ìƒ"} '
                    f'(ì‹ ë¢°ë„: {attention_data["confidence"]:.3f})')
        plt.tight_layout()
        
        filename = f'real_beit_attention_layer_{layer_idx}.png'
        plt.savefig(filename, dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"âœ… ì´ë¯¸ì§€ attention ì‹œê°í™” ì €ì¥: {filename}")
    
    def analyze_cross_modal_attention(self, attention_data: Dict):
        """í¬ë¡œìŠ¤ ëª¨ë‹¬ attention ë¶„ì„"""
        print(f"\nğŸ”— í¬ë¡œìŠ¤ ëª¨ë‹¬ Attention ë¶„ì„")
        
        # ìœµí•© ë ˆì´ì–´ ë°ì´í„° í™•ì¸
        fusion_key = 'fusion_fusion'
        if fusion_key not in attention_data['attention_weights']:
            print(f"âš ï¸ ìœµí•© ë ˆì´ì–´ attentionì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return
        
        fusion_data = attention_data['attention_weights'][fusion_key]
        
        print(f"âœ… ìœµí•© ë ˆì´ì–´ ë°ì´í„° í˜•íƒœ: {fusion_data.shape}")
        
        # ê°„ë‹¨í•œ í†µê³„ ë¶„ì„
        text_region = fusion_data[:, :64]  # í…ìŠ¤íŠ¸ ì˜ì—­ (ê°€ì •)
        image_region = fusion_data[:, 64:]  # ì´ë¯¸ì§€ ì˜ì—­ (ê°€ì •)
        
        text_activation = text_region.mean().item()
        image_activation = image_region.mean().item()
        
        print(f"   í…ìŠ¤íŠ¸ ì˜ì—­ í‰ê·  í™œì„±í™”: {text_activation:.4f}")
        print(f"   ì´ë¯¸ì§€ ì˜ì—­ í‰ê·  í™œì„±í™”: {image_activation:.4f}")
        print(f"   ëª¨ë‹¬ë¦¬í‹° ë¹„ìœ¨ (í…ìŠ¤íŠ¸/ì´ë¯¸ì§€): {text_activation/image_activation:.3f}")
    
    def cleanup_hooks(self):
        """ë“±ë¡ëœ Hook ì •ë¦¬"""
        for hook in self.hooks:
            hook.remove()
        self.hooks.clear()
        print("ğŸ§¹ Hook ì •ë¦¬ ì™„ë£Œ")
    
    def run_real_attention_analysis(self, text: str = "ë¬´ë£Œ ìƒí’ˆ ë°›ê¸°! ì§€ê¸ˆ í´ë¦­í•˜ì„¸ìš”!"):
        """ì‹¤ì œ attention ë¶„ì„ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        print("ğŸš€ ì‹¤ì œ MMTD Attention ë¶„ì„ ì‹œì‘")
        print("="*60)
        
        try:
            # 1. ëª¨ë¸ ë¡œë”©
            if not self.load_model_and_checkpoint():
                return False
            
            # 2. Hook ë“±ë¡
            self.register_attention_hooks()
            
            # 3. ìƒ˜í”Œ ì…ë ¥ ìƒì„±
            inputs, processed_text = self.create_sample_input(text)
            
            # 4. Attention ì¶”ì¶œ
            attention_data = self.extract_attention_weights(inputs)
            
            # 5. ì‹œê°í™”
            self.visualize_text_attention(processed_text, attention_data, layer_idx=11)
            self.visualize_image_attention(attention_data, layer_idx=11)
            self.analyze_cross_modal_attention(attention_data)
            
            print("\nğŸ‰ ì‹¤ì œ MMTD Attention ë¶„ì„ ì™„ë£Œ!")
            return True
            
        except Exception as e:
            print(f"âŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
            return False
            
        finally:
            # 7. ì •ë¦¬
            self.cleanup_hooks()


if __name__ == "__main__":
    # ì‹¤ì œ attention ë¶„ì„ ì‹¤í–‰
    extractor = RealAttentionExtractor()
    
    # ì—¬ëŸ¬ ìƒ˜í”Œ í…ŒìŠ¤íŠ¸
    test_samples = [
        "ë¬´ë£Œ ìƒí’ˆì„ ë°›ìœ¼ì„¸ìš”! ì§€ê¸ˆ í´ë¦­í•˜ì„¸ìš”!",  # ìŠ¤íŒ¸ ì˜ˆìƒ
        "ì•ˆë…•í•˜ì„¸ìš”. íšŒì˜ ì¼ì •ì„ ì•Œë ¤ë“œë¦½ë‹ˆë‹¤.",     # ì •ìƒ ì˜ˆìƒ
        "FREE MONEY! Click here NOW!!!",          # ì˜ì–´ ìŠ¤íŒ¸
        "ãŠã‚ã§ã¨ã†ã”ã–ã„ã¾ã™ï¼è³å“ãŒå½“é¸"          # ì¼ë³¸ì–´ ìŠ¤íŒ¸
    ]
    
    for i, sample_text in enumerate(test_samples):
        print(f"\n{'='*80}")
        print(f"ìƒ˜í”Œ {i+1}: {sample_text}")
        print(f"{'='*80}")
        
        success = extractor.run_real_attention_analysis(sample_text)
        if not success:
            print(f"âš ï¸ ìƒ˜í”Œ {i+1} ë¶„ì„ ì‹¤íŒ¨")
            break
    
    print("\nâœ… ëª¨ë“  ì‹¤ì œ attention ë¶„ì„ ì™„ë£Œ!") 