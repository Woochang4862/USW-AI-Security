import torch
import sys
sys.path.append('.')

from models import MMTD

def explore_model_structure():
    """ì‹¤ì œ MMTD ëª¨ë¸ êµ¬ì¡° íƒìƒ‰"""
    print("ğŸ” MMTD ëª¨ë¸ êµ¬ì¡° íƒìƒ‰ ì‹œì‘")
    print("="*60)
    
    # ëª¨ë¸ ìƒì„±
    model = MMTD(
        bert_pretrain_weight='bert-base-multilingual-cased',
        beit_pretrain_weight='microsoft/dit-base'
    )
    
    print(f"ğŸ“Š ì „ì²´ ëª¨ë¸ êµ¬ì¡°:")
    print(f"   - Text Encoder: {type(model.text_encoder).__name__}")
    print(f"   - Image Encoder: {type(model.image_encoder).__name__}")
    print(f"   - Fusion Layer: {type(model.multi_modality_transformer_layer).__name__}")
    
    print(f"\nğŸ”¤ BERT Text Encoder êµ¬ì¡° íƒìƒ‰:")
    print(f"   Type: {type(model.text_encoder.bert)}")
    
    # BERT encoder layers í™•ì¸
    if hasattr(model.text_encoder.bert, 'encoder'):
        encoder = model.text_encoder.bert.encoder
        print(f"   Encoder layers: {len(encoder.layer)}ê°œ")
        
        # ì²« ë²ˆì§¸ ë ˆì´ì–´ êµ¬ì¡° í™•ì¸
        first_layer = encoder.layer[0]
        print(f"   First layer type: {type(first_layer)}")
        print(f"   First layer attributes: {list(first_layer.__dict__.keys())}")
        
        if hasattr(first_layer, 'attention'):
            attention = first_layer.attention
            print(f"   Attention type: {type(attention)}")
            print(f"   Attention attributes: {list(attention.__dict__.keys())}")
            
            if hasattr(attention, 'self'):
                self_attn = attention.self
                print(f"   Self-attention type: {type(self_attn)}")
                print(f"   Self-attention attributes: {list(self_attn.__dict__.keys())}")
    
    print(f"\nğŸ–¼ï¸ BEiT Image Encoder êµ¬ì¡° íƒìƒ‰:")
    print(f"   Type: {type(model.image_encoder.beit)}")
    
    # BEiT encoder layers í™•ì¸
    if hasattr(model.image_encoder.beit, 'encoder'):
        encoder = model.image_encoder.beit.encoder
        print(f"   Encoder layers: {len(encoder.layer)}ê°œ")
        
        # ì²« ë²ˆì§¸ ë ˆì´ì–´ êµ¬ì¡° í™•ì¸
        first_layer = encoder.layer[0]
        print(f"   First layer type: {type(first_layer)}")
        print(f"   First layer attributes: {list(first_layer.__dict__.keys())}")
        
        if hasattr(first_layer, 'attention'):
            attention = first_layer.attention
            print(f"   Attention type: {type(attention)}")
            print(f"   Attention attributes: {list(attention.__dict__.keys())}")
            
            if hasattr(attention, 'attention'):
                self_attn = attention.attention
                print(f"   Self-attention type: {type(self_attn)}")
                print(f"   Self-attention attributes: {list(self_attn.__dict__.keys())}")
    
    print(f"\nğŸ”— Fusion Layer êµ¬ì¡°:")
    fusion = model.multi_modality_transformer_layer
    print(f"   Type: {type(fusion)}")
    print(f"   Attributes: {list(fusion.__dict__.keys())}")
    
    if hasattr(fusion, 'self_attn'):
        print(f"   Self-attention type: {type(fusion.self_attn)}")
        print(f"   Self-attention attributes: {list(fusion.self_attn.__dict__.keys())}")
    
    # ì‘ì€ í…ŒìŠ¤íŠ¸ ì‹¤í–‰í•˜ì—¬ ì‹¤ì œ output êµ¬ì¡° í™•ì¸
    print(f"\nğŸ§ª ì‹¤ì œ Forward Pass í…ŒìŠ¤íŠ¸:")
    model.eval()
    
    with torch.no_grad():
        dummy_input = {
            'input_ids': torch.randint(0, 1000, (1, 10)),
            'attention_mask': torch.ones(1, 10),
            'token_type_ids': torch.zeros(1, 10),
            'pixel_values': torch.randn(1, 3, 224, 224)
        }
        
        # í…ìŠ¤íŠ¸ ì¸ì½”ë”ë§Œ ì‹¤í–‰
        print("   í…ìŠ¤íŠ¸ ì¸ì½”ë” ì¶œë ¥ êµ¬ì¡°:")
        text_outputs = model.text_encoder(**{k: v for k, v in dummy_input.items() if k != 'pixel_values'})
        print(f"     Type: {type(text_outputs)}")
        print(f"     Attributes: {list(text_outputs.__dict__.keys()) if hasattr(text_outputs, '__dict__') else 'No dict'}")
        if hasattr(text_outputs, 'hidden_states'):
            print(f"     Hidden states: {len(text_outputs.hidden_states)} layers")
        if hasattr(text_outputs, 'attentions') and text_outputs.attentions is not None:
            print(f"     Attentions: {len(text_outputs.attentions)} layers")
        
        # ì´ë¯¸ì§€ ì¸ì½”ë”ë§Œ ì‹¤í–‰
        print("   ì´ë¯¸ì§€ ì¸ì½”ë” ì¶œë ¥ êµ¬ì¡°:")
        image_outputs = model.image_encoder(pixel_values=dummy_input['pixel_values'])
        print(f"     Type: {type(image_outputs)}")
        print(f"     Attributes: {list(image_outputs.__dict__.keys()) if hasattr(image_outputs, '__dict__') else 'No dict'}")
        if hasattr(image_outputs, 'hidden_states'):
            print(f"     Hidden states: {len(image_outputs.hidden_states)} layers")
        if hasattr(image_outputs, 'attentions') and image_outputs.attentions is not None:
            print(f"     Attentions: {len(image_outputs.attentions)} layers")
    
    print(f"\nâœ… ëª¨ë¸ êµ¬ì¡° íƒìƒ‰ ì™„ë£Œ!")

if __name__ == "__main__":
    explore_model_structure() 