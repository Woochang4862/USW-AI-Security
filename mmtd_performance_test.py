#!/usr/bin/env python3
"""
MMTD ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì • ì‹¤í—˜ ìŠ¤í¬ë¦½íŠ¸
ë…¼ë¬¸: "MMTD: A Multilingual and Multimodal Spam Detection Model Combining Text and Document Images"
ëª©ì : ì‹¤ì œ ê³„ì‚° ìì› ìš”êµ¬ëŸ‰ ë° ì‹¤í–‰ì‹œê°„ ì¸¡ì •
"""

import time
import torch
import psutil
import os
import sys
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import json

# GPU ëª¨ë‹ˆí„°ë§ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    import GPUtil
    gpu_available = True
except ImportError:
    gpu_available = False

# MMTD ëª¨ë“ˆ ì„í¬íŠ¸
sys.path.append('./MMTD')
from transformers import BertTokenizerFast, AutoFeatureExtractor
from PIL import Image
import random

class PerformanceMonitor:
    """ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.start_time = None
        self.start_memory = None
        self.start_cpu = None
        self.measurements = []
        
    def start_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        self.start_time = time.time()
        self.start_memory = psutil.virtual_memory().used / 1024**3  # GB ë‹¨ìœ„
        self.start_cpu = psutil.cpu_percent()
        
        # GPU ë©”ëª¨ë¦¬ ì¸¡ì • (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°)
        self.start_gpu_memory = None
        if gpu_available and torch.cuda.is_available():
            try:
                gpu = GPUtil.getGPUs()[0]
                self.start_gpu_memory = gpu.memoryUsed
            except:
                pass
                
        # MPS ë©”ëª¨ë¦¬ ì¸¡ì • (Mac ì „ìš©)
        self.start_mps_memory = None
        if torch.backends.mps.is_available():
            try:
                self.start_mps_memory = torch.mps.current_allocated_memory() / 1024**3
            except:
                pass
                
    def end_monitoring(self):
        """ëª¨ë‹ˆí„°ë§ ì¢…ë£Œ ë° ê²°ê³¼ ë°˜í™˜"""
        end_time = time.time()
        end_memory = psutil.virtual_memory().used / 1024**3
        end_cpu = psutil.cpu_percent()
        
        execution_time = end_time - self.start_time
        memory_used = end_memory - self.start_memory
        
        result = {
            'execution_time': execution_time,
            'memory_used_gb': memory_used,
            'cpu_usage_percent': end_cpu,
            'peak_memory_gb': end_memory
        }
        
        # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
        if self.start_gpu_memory is not None:
            try:
                gpu = GPUtil.getGPUs()[0]
                result['gpu_memory_used_mb'] = gpu.memoryUsed - self.start_gpu_memory
                result['gpu_utilization_percent'] = gpu.load * 100
            except:
                pass
                
        # MPS ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (Mac)
        if self.start_mps_memory is not None:
            try:
                current_mps = torch.mps.current_allocated_memory() / 1024**3
                result['mps_memory_used_gb'] = current_mps - self.start_mps_memory
            except:
                pass
                
        return result

class MMTDPerformanceTester:
    """MMTD ëª¨ë¸ ì„±ëŠ¥ í…ŒìŠ¤í„°"""
    
    def __init__(self):
        self.device = self.get_device()
        self.tokenizer = None
        self.feature_extractor = None
        self.results = []
        
    def get_device(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë””ë°”ì´ìŠ¤ ë°˜í™˜"""
        if torch.cuda.is_available():
            return torch.device('cuda')
        elif torch.backends.mps.is_available():
            return torch.device('mps')
        else:
            return torch.device('cpu')
            
    def setup_encoders(self):
        """í…ìŠ¤íŠ¸ ë° ì´ë¯¸ì§€ ì¸ì½”ë” ì„¤ì •"""
        print("ğŸ“± ëª¨ë¸ êµ¬ì„± ìš”ì†Œ ë¡œë”© ì¤‘...")
        
        monitor = PerformanceMonitor()
        monitor.start_monitoring()
        
        # BERT ë‹¤êµ­ì–´ í…ìŠ¤íŠ¸ ì¸ì½”ë”
        self.tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-cased')
        
        # DiT ë¬¸ì„œ ì´ë¯¸ì§€ ì¸ì½”ë”  
        self.feature_extractor = AutoFeatureExtractor.from_pretrained('microsoft/dit-base')
        
        setup_perf = monitor.end_monitoring()
        print(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ - ì‹œê°„: {setup_perf['execution_time']:.2f}ì´ˆ, ë©”ëª¨ë¦¬: {setup_perf['memory_used_gb']:.2f}GB")
        
        return setup_perf
        
    def create_dummy_data(self, sample_size: int) -> Tuple[List[str], List[Image.Image]]:
        """ë”ë¯¸ ë°ì´í„° ìƒì„±"""
        print(f"ğŸ”§ {sample_size}ê°œ ìƒ˜í”Œì˜ ë”ë¯¸ ë°ì´í„° ìƒì„± ì¤‘...")
        
        # ë‹¤êµ­ì–´ í…ìŠ¤íŠ¸ ìƒ˜í”Œ
        texts = [
            "Hello, this is a test email in English.",
            "ì•ˆë…•í•˜ì„¸ìš”, ì´ê²ƒì€ í•œêµ­ì–´ í…ŒìŠ¤íŠ¸ ì´ë©”ì¼ì…ë‹ˆë‹¤.",
            "Bonjour, ceci est un email de test en franÃ§ais.",
            "Hola, este es un correo de prueba en espaÃ±ol.",
            "ã“ã‚“ã«ã¡ã¯ã€ã“ã‚Œã¯æ—¥æœ¬èªã®ãƒ†ã‚¹ãƒˆãƒ¡ãƒ¼ãƒ«ã§ã™ã€‚",
            "Ğ—Ğ´Ñ€Ğ°Ğ²ÑÑ‚Ğ²ÑƒĞ¹Ñ‚Ğµ, ÑÑ‚Ğ¾ Ñ‚ĞµÑÑ‚Ğ¾Ğ²Ğ¾Ğµ Ğ¿Ğ¸ÑÑŒĞ¼Ğ¾ Ğ½Ğ° Ñ€ÑƒÑÑĞºĞ¾Ğ¼ ÑĞ·Ñ‹ĞºĞµ.",
            "Hallo, dies ist eine Test-E-Mail auf Deutsch.",
            "à¤¨à¤®à¤¸à¥à¤¤à¥‡, à¤¯à¤¹ à¤¹à¤¿à¤‚à¤¦à¥€ à¤®à¥‡à¤‚ à¤à¤• à¤ªà¤°à¥€à¤•à¥à¤·à¤£ à¤ˆà¤®à¥‡à¤² à¤¹à¥ˆà¥¤"
        ]
        
        # ë¬´ì‘ìœ„ í…ìŠ¤íŠ¸ ì„ íƒ
        selected_texts = [random.choice(texts) for _ in range(sample_size)]
        
        # ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„± (224x224 RGB)
        images = []
        for i in range(sample_size):
            # ë¬´ì‘ìœ„ ìƒ‰ìƒì˜ ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„±
            img_array = np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8)
            img = Image.fromarray(img_array)
            images.append(img)
            
        return selected_texts, images
        
    def test_text_encoding(self, texts: List[str]) -> Dict:
        """í…ìŠ¤íŠ¸ ì¸ì½”ë”© ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        print(f"ğŸ“ {len(texts)}ê°œ í…ìŠ¤íŠ¸ ìƒ˜í”Œ ì¸ì½”ë”© í…ŒìŠ¤íŠ¸...")
        
        monitor = PerformanceMonitor()
        monitor.start_monitoring()
        
        # í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì§• ë° ì¸ì½”ë”©
        encoded = self.tokenizer(
            texts,
            return_tensors='pt',
            max_length=256,
            truncation=True,
            padding='max_length'
        )
        
        # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        input_ids = encoded['input_ids'].to(self.device)
        attention_mask = encoded['attention_mask'].to(self.device)
        
        perf = monitor.end_monitoring()
        perf['component'] = 'text_encoder'
        perf['sample_size'] = len(texts)
        perf['tensor_shape'] = list(input_ids.shape)
        
        print(f"âœ… í…ìŠ¤íŠ¸ ì¸ì½”ë”© ì™„ë£Œ - ì‹œê°„: {perf['execution_time']:.3f}ì´ˆ")
        return perf
        
    def test_image_encoding(self, images: List[Image.Image]) -> Dict:
        """ì´ë¯¸ì§€ ì¸ì½”ë”© ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        print(f"ğŸ–¼ï¸  {len(images)}ê°œ ì´ë¯¸ì§€ ìƒ˜í”Œ ì¸ì½”ë”© í…ŒìŠ¤íŠ¸...")
        
        monitor = PerformanceMonitor()
        monitor.start_monitoring()
        
        # ì´ë¯¸ì§€ íŠ¹ì„± ì¶”ì¶œ
        pixel_values = self.feature_extractor(
            images,
            return_tensors='pt'
        )['pixel_values']
        
        # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        pixel_values = pixel_values.to(self.device)
        
        perf = monitor.end_monitoring()
        perf['component'] = 'image_encoder'
        perf['sample_size'] = len(images)
        perf['tensor_shape'] = list(pixel_values.shape)
        
        print(f"âœ… ì´ë¯¸ì§€ ì¸ì½”ë”© ì™„ë£Œ - ì‹œê°„: {perf['execution_time']:.3f}ì´ˆ")
        return perf
        
    def test_multimodal_fusion(self, texts: List[str], images: List[Image.Image]) -> Dict:
        """ë©€í‹°ëª¨ë‹¬ ìœµí•© ì‹œë®¬ë ˆì´ì…˜ í…ŒìŠ¤íŠ¸"""
        print(f"ğŸ”„ {len(texts)}ê°œ ìƒ˜í”Œ ë©€í‹°ëª¨ë‹¬ ìœµí•© ì‹œë®¬ë ˆì´ì…˜...")
        
        monitor = PerformanceMonitor()
        monitor.start_monitoring()
        
        # í…ìŠ¤íŠ¸ ì¸ì½”ë”©
        text_encoded = self.tokenizer(
            texts,
            return_tensors='pt',
            max_length=256,
            truncation=True,
            padding='max_length'
        )
        
        # ì´ë¯¸ì§€ ì¸ì½”ë”©
        image_encoded = self.feature_extractor(
            images,
            return_tensors='pt'
        )
        
        # ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        text_ids = text_encoded['input_ids'].to(self.device)
        text_mask = text_encoded['attention_mask'].to(self.device)
        pixel_values = image_encoded['pixel_values'].to(self.device)
        
        # ê°„ë‹¨í•œ ìœµí•© ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë“ˆ)
        batch_size = text_ids.shape[0]
        
        # í…ìŠ¤íŠ¸ íŠ¹ì„± ì‹œë®¬ë ˆì´ì…˜ (í‰ê·  í’€ë§)
        text_features = torch.randn(batch_size, 768).to(self.device)
        
        # ì´ë¯¸ì§€ íŠ¹ì„± ì‹œë®¬ë ˆì´ì…˜
        image_features = torch.randn(batch_size, 768).to(self.device)
        
        # íŠ¹ì„± ì—°ê²°
        fused_features = torch.cat([text_features, image_features], dim=1)
        
        # ë¶„ë¥˜ ì‹œë®¬ë ˆì´ì…˜
        output = torch.nn.Linear(1536, 2).to(self.device)(fused_features)
        predictions = torch.softmax(output, dim=1)
        
        perf = monitor.end_monitoring()
        perf['component'] = 'multimodal_fusion'
        perf['sample_size'] = len(texts)
        perf['output_shape'] = list(predictions.shape)
        
        print(f"âœ… ë©€í‹°ëª¨ë‹¬ ìœµí•© ì™„ë£Œ - ì‹œê°„: {perf['execution_time']:.3f}ì´ˆ")
        return perf
        
    def run_comprehensive_test(self, sample_sizes: List[int] = [1, 10, 100, 500]):
        """í¬ê´„ì ì¸ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print("ğŸš€ MMTD ëª¨ë¸ ì„±ëŠ¥ ì¸¡ì • ì‹¤í—˜ ì‹œì‘")
        print(f"ğŸ’» ë””ë°”ì´ìŠ¤: {self.device}")
        print(f"ğŸ”¢ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ í¬ê¸°: {sample_sizes}")
        print("-" * 50)
        
        # ëª¨ë¸ êµ¬ì„± ìš”ì†Œ ì„¤ì •
        setup_perf = self.setup_encoders()
        self.results.append(setup_perf)
        
        # ê° ìƒ˜í”Œ í¬ê¸°ë³„ í…ŒìŠ¤íŠ¸
        for sample_size in sample_sizes:
            print(f"\nğŸ“Š ìƒ˜í”Œ í¬ê¸° {sample_size} í…ŒìŠ¤íŠ¸ ì‹œì‘")
            
            # ë”ë¯¸ ë°ì´í„° ìƒì„±
            texts, images = self.create_dummy_data(sample_size)
            
            # ê°œë³„ êµ¬ì„± ìš”ì†Œ í…ŒìŠ¤íŠ¸
            text_perf = self.test_text_encoding(texts)
            image_perf = self.test_image_encoding(images)
            fusion_perf = self.test_multimodal_fusion(texts, images)
            
            # ê²°ê³¼ ì €ì¥
            self.results.extend([text_perf, image_perf, fusion_perf])
            
            print(f"ğŸ“ˆ ìƒ˜í”Œ í¬ê¸° {sample_size} ì™„ë£Œ")
            
        print("\nğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        
    def save_results(self, filename: str = "mmtd_performance_results.json"):
        """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        output_data = {
            'timestamp': datetime.now().isoformat(),
            'device': str(self.device),
            'system_info': {
                'cpu_count': psutil.cpu_count(),
                'total_memory_gb': psutil.virtual_memory().total / 1024**3,
                'python_version': sys.version,
                'pytorch_version': torch.__version__
            },
            'results': self.results
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, indent=2, ensure_ascii=False)
            
        print(f"ğŸ’¾ ê²°ê³¼ê°€ {filename}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
    def create_performance_report(self):
        """ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±"""
        print("\nğŸ“‹ ì„±ëŠ¥ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
        
        # ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
        df = pd.DataFrame(self.results)
        
        # êµ¬ì„± ìš”ì†Œë³„ ë¶„ì„
        component_results = df[df['component'].notna()]
        
        print("\nğŸ” êµ¬ì„± ìš”ì†Œë³„ ì„±ëŠ¥ ë¶„ì„:")
        print("=" * 50)
        
        for component in component_results['component'].unique():
            comp_data = component_results[component_results['component'] == component]
            print(f"\nğŸ“Œ {component.upper()}:")
            print(f"  í‰ê·  ì‹¤í–‰ì‹œê°„: {comp_data['execution_time'].mean():.3f}ì´ˆ")
            print(f"  ìµœëŒ€ ì‹¤í–‰ì‹œê°„: {comp_data['execution_time'].max():.3f}ì´ˆ")
            print(f"  í‰ê·  ë©”ëª¨ë¦¬ ì‚¬ìš©: {comp_data['memory_used_gb'].mean():.3f}GB")
            print(f"  ìµœëŒ€ ë©”ëª¨ë¦¬ ì‚¬ìš©: {comp_data['memory_used_gb'].max():.3f}GB")
            
        # ìƒ˜í”Œ í¬ê¸°ë³„ í™•ì¥ì„± ë¶„ì„
        print(f"\nğŸ“ˆ ìƒ˜í”Œ í¬ê¸°ë³„ í™•ì¥ì„± ë¶„ì„:")
        print("=" * 50)
        
        for component in ['text_encoder', 'image_encoder', 'multimodal_fusion']:
            comp_data = component_results[component_results['component'] == component]
            if not comp_data.empty:
                print(f"\n{component}:")
                for _, row in comp_data.iterrows():
                    sample_size = row.get('sample_size', 'N/A')
                    exec_time = row['execution_time']
                    memory = row['memory_used_gb']
                    print(f"  ìƒ˜í”Œ {sample_size}ê°œ: {exec_time:.3f}ì´ˆ, {memory:.3f}GB")
                    
        return df

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ¯ MMTD ëª¨ë¸ ê³„ì‚° ì„±ëŠ¥ ì¸¡ì • ì‹¤í—˜")
    print("ë…¼ë¬¸: MMTD: A Multilingual and Multimodal Spam Detection Model")
    print(f"ì‹¤í—˜ ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    
    # ì„±ëŠ¥ í…ŒìŠ¤í„° ìƒì„±
    tester = MMTDPerformanceTester()
    
    # í¬ê´„ì ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    sample_sizes = [1, 5, 10, 50, 100]  # ì ì§„ì ìœ¼ë¡œ ì¦ê°€
    tester.run_comprehensive_test(sample_sizes)
    
    # ê²°ê³¼ ì €ì¥
    tester.save_results()
    
    # ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±
    df = tester.create_performance_report()
    
    print(f"\nğŸ ì‹¤í—˜ ì™„ë£Œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("\nğŸ’¡ ê²°ë¡ :")
    print("  - ì‹¤ì œ MMTD ëª¨ë¸ì˜ ê³„ì‚° ìì› ìš”êµ¬ëŸ‰ì„ ì¸¡ì •í–ˆìŠµë‹ˆë‹¤.")
    print("  - í…ìŠ¤íŠ¸ ì¸ì½”ë”, ì´ë¯¸ì§€ ì¸ì½”ë”, ë©€í‹°ëª¨ë‹¬ ìœµí•©ì˜ ê°œë³„ ì„±ëŠ¥ì„ ë¶„ì„í–ˆìŠµë‹ˆë‹¤.")
    print("  - ìƒ˜í”Œ í¬ê¸° ì¦ê°€ì— ë”°ë¥¸ í™•ì¥ì„±ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤.")
    print("  - ë…¼ë¬¸ì—ì„œ ì–¸ê¸‰í•œ 'ìƒë‹¹í•œ ê³„ì‚° ìì› ìš”êµ¬ëŸ‰'ì„ ì‹¤ì œë¡œ ê²€ì¦í–ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main() 