"""
Colab Pro T4 GPU Setup for MMTD Evaluation
Optimized configuration for fast evaluation
"""

import torch
import os
import subprocess
import sys

def setup_colab_environment():
    """Setup Colab environment for MMTD evaluation"""
    
    print("ğŸš€ Setting up Colab Pro T4 environment for MMTD...")
    
    # 1. Check GPU availability
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
        print(f"âœ… GPU detected: {gpu_name}")
        print(f"ğŸ“Š GPU memory: {gpu_memory:.1f} GB")
    else:
        print("âŒ No GPU detected! Please enable GPU in Colab.")
        return False
    
    # 2. Install required packages
    packages = [
        "transformers==4.35.0",
        "torch==2.0.1",
        "torchvision==0.15.2",
        "scikit-learn",
        "pandas",
        "numpy",
        "matplotlib",
        "seaborn",
        "tqdm",
        "Pillow"
    ]
    
    print("ğŸ“¦ Installing packages...")
    for package in packages:
        subprocess.check_call([sys.executable, "-m", "pip", "install", package])
    
    # 3. Set optimal environment variables
    os.environ['CUDA_LAUNCH_BLOCKING'] = '0'  # Async execution
    os.environ['TORCH_CUDNN_V8_API_ENABLED'] = '1'  # cuDNN optimization
    
    # 4. Configure PyTorch for optimal performance
    torch.backends.cudnn.benchmark = True  # Optimize for fixed input sizes
    torch.backends.cudnn.deterministic = False  # Allow non-deterministic for speed
    
    print("âœ… Environment setup complete!")
    return True

def get_optimal_batch_size():
    """Calculate optimal batch size for T4 GPU"""
    if not torch.cuda.is_available():
        return 8
    
    gpu_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1e9
    
    if gpu_memory_gb >= 15:  # T4 has 16GB
        return 64  # Aggressive batch size for T4
    elif gpu_memory_gb >= 10:
        return 32
    else:
        return 16
    
def setup_mixed_precision():
    """Setup mixed precision training for faster inference"""
    return torch.cuda.amp.GradScaler()

def optimize_dataloader_settings():
    """Get optimal DataLoader settings for Colab"""
    return {
        'num_workers': 2,  # Colab has limited CPU cores
        'pin_memory': True,  # Faster GPU transfer
        'persistent_workers': True,  # Reuse workers
        'prefetch_factor': 2  # Prefetch batches
    }

def print_performance_tips():
    """Print performance optimization tips"""
    print("\nğŸ”¥ T4 GPU ìµœì í™” íŒ:")
    print("1. ë°°ì¹˜ í¬ê¸°: 64 ì‚¬ìš© (16GB ë©”ëª¨ë¦¬ í™œìš©)")
    print("2. Mixed Precision: ìë™ í™œì„±í™” (2ë°° ë¹ ë¦„)")
    print("3. DataLoader: num_workers=2, pin_memory=True")
    print("4. ì˜ˆìƒ í‰ê°€ ì‹œê°„: 3-5ë¶„ (CPU ëŒ€ë¹„ 5-8ë°° ë¹ ë¦„)")
    print("5. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: ~8GB (16GB ì¤‘)")

# Colabì—ì„œ ì‹¤í–‰í•  ì„¤ì •
if __name__ == "__main__":
    # Google Drive ë§ˆìš´íŠ¸ (ë°ì´í„° ì ‘ê·¼ìš©)
    try:
        from google.colab import drive
        drive.mount('/content/drive')
        print("âœ… Google Drive mounted")
    except:
        print("â„¹ï¸ Not in Colab environment")
    
    # í™˜ê²½ ì„¤ì •
    if setup_colab_environment():
        batch_size = get_optimal_batch_size()
        dataloader_settings = optimize_dataloader_settings()
        
        print(f"\nğŸ¯ ê¶Œì¥ ì„¤ì •:")
        print(f"   ë°°ì¹˜ í¬ê¸°: {batch_size}")
        print(f"   DataLoader ì„¤ì •: {dataloader_settings}")
        
        print_performance_tips()
        
        # ìƒ˜í”Œ ì½”ë“œ
        print(f"\nğŸ“ Colabì—ì„œ ì‚¬ìš©í•  ì½”ë“œ:")
        print(f"""
# MMTD í‰ê°€ ì‹¤í–‰
evaluator = MMTDEvaluator(
    checkpoint_dir="path/to/checkpoints",
    data_path="path/to/pics", 
    csv_path="path/to/EDP.csv",
    batch_size={batch_size}  # T4 ìµœì í™”
)

# Mixed precisionìœ¼ë¡œ ë¹ ë¥¸ í‰ê°€
with torch.cuda.amp.autocast():
    results = evaluator.evaluate_all_folds()
        """) 