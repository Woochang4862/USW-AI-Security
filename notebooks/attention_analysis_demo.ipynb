{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ” MMTD Attention ê¸°ë°˜ í•´ì„ê°€ëŠ¥ì„± ë¶„ì„ ë°ëª¨\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ MMTD ëª¨ë¸ì˜ Attention ê¸°ë°˜ í•´ì„ê°€ëŠ¥ì„± ë¶„ì„ ì‹œìŠ¤í…œì„ ì‹œì—°í•©ë‹ˆë‹¤.\n",
    "\n",
    "## ğŸ¯ ì£¼ìš” ê¸°ëŠ¥\n",
    "- **í…ìŠ¤íŠ¸ Attention**: ì–´ë–¤ ë‹¨ì–´ê°€ ì¤‘ìš”í•œì§€ ë¶„ì„\n",
    "- **ì´ë¯¸ì§€ Attention**: ì–´ë–¤ ì´ë¯¸ì§€ ì˜ì—­ì´ ì¤‘ìš”í•œì§€ ë¶„ì„\n",
    "- **Cross-Modal Attention**: í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ê°„ ìƒí˜¸ì‘ìš© ë¶„ì„\n",
    "- **ì¢…í•© í•´ì„**: ì˜ˆì¸¡ ê²°ê³¼ì— ëŒ€í•œ ì™„ì „í•œ ì„¤ëª… ì œê³µ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ì»¤ìŠ¤í…€ ëª¨ë“ˆ import\n",
    "from src.analysis.attention_analyzer import AttentionAnalyzer\n",
    "from src.analysis.attention_visualizer import AttentionVisualizer\n",
    "from src.models.interpretable_mmtd import InterpretableMMTD\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "print(\"ğŸš€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë”© ì™„ë£Œ!\")\n",
    "print(f\"ğŸ–¥ï¸ ì‚¬ìš© ê°€ëŠ¥í•œ ë””ë°”ì´ìŠ¤: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¥ ëª¨ë¸ ë° ë¶„ì„ ë„êµ¬ ë¡œë”©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì„¤ì •\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_path = '../checkpoints/best_interpretable_mmtd.ckpt'  # ì‹¤ì œ ëª¨ë¸ ê²½ë¡œë¡œ ìˆ˜ì •\n",
    "tokenizer_name = 'bert-base-uncased'\n",
    "\n",
    "print(f\"ğŸ”§ ì„¤ì •:\")\n",
    "print(f\"  - ë””ë°”ì´ìŠ¤: {device}\")\n",
    "print(f\"  - ëª¨ë¸ ê²½ë¡œ: {model_path}\")\n",
    "print(f\"  - í† í¬ë‚˜ì´ì €: {tokenizer_name}\")\n",
    "\n",
    "# ëª¨ë¸ ë¡œë”©\n",
    "try:\n",
    "    model = InterpretableMMTD.load_from_checkpoint(\n",
    "        model_path,\n",
    "        map_location=device\n",
    "    )\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    print(\"âœ… ëª¨ë¸ ë¡œë”© ì„±ê³µ!\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}\")\n",
    "    print(\"ğŸ’¡ ëª¨ë¸ ê²½ë¡œë¥¼ í™•ì¸í•˜ê±°ë‚˜ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì„ ì‚¬ìš©í•´ì£¼ì„¸ìš”.\")\n",
    "\n",
    "# í† í¬ë‚˜ì´ì € ë¡œë”©\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "print(\"âœ… í† í¬ë‚˜ì´ì € ë¡œë”© ì„±ê³µ!\")\n",
    "\n",
    "# ë¶„ì„ ë„êµ¬ ì´ˆê¸°í™”\n",
    "analyzer = AttentionAnalyzer(model, tokenizer, device)\n",
    "visualizer = AttentionVisualizer()\n",
    "print(\"âœ… ë¶„ì„ ë„êµ¬ ì´ˆê¸°í™” ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ ì˜ˆì‹œ ë°ì´í„° ì¤€ë¹„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì˜ˆì‹œ ìŠ¤íŒ¸ ì´ë©”ì¼ í…ìŠ¤íŠ¸\n",
    "spam_text = \"\"\"\n",
    "URGENT! You have WON $1,000,000 in our EXCLUSIVE lottery! \n",
    "Click here NOW to claim your FREE prize! \n",
    "Limited time offer - expires TODAY!\n",
    "Call 1-800-WIN-CASH immediately!\n",
    "\"\"\"\n",
    "\n",
    "# ì˜ˆì‹œ ì •ìƒ ì´ë©”ì¼ í…ìŠ¤íŠ¸\n",
    "ham_text = \"\"\"\n",
    "Hi John,\n",
    "\n",
    "I hope you're doing well. I wanted to follow up on our meeting yesterday \n",
    "about the project timeline. Could you please send me the updated schedule \n",
    "when you have a chance?\n",
    "\n",
    "Thanks,\n",
    "Sarah\n",
    "\"\"\"\n",
    "\n",
    "print(\"ğŸ“ ì˜ˆì‹œ í…ìŠ¤íŠ¸ ì¤€ë¹„ ì™„ë£Œ!\")\n",
    "print(f\"ìŠ¤íŒ¸ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(spam_text)} ê¸€ì\")\n",
    "print(f\"ì •ìƒ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(ham_text)} ê¸€ì\")\n",
    "\n",
    "# ì˜ˆì‹œ ì´ë¯¸ì§€ ìƒì„± (ì‹¤ì œë¡œëŠ” ë°ì´í„°ì…‹ì—ì„œ ë¡œë”©)\n",
    "# ë”ë¯¸ ì´ë¯¸ì§€ (224x224 RGB)\n",
    "dummy_image = torch.randn(3, 224, 224)\n",
    "print(f\"ğŸ–¼ï¸ ì˜ˆì‹œ ì´ë¯¸ì§€ shape: {dummy_image.shape}\")\n",
    "\n",
    "print(\"\\nâš ï¸ ì‹¤ì œ ì‚¬ìš© ì‹œì—ëŠ” ë°ì´í„°ì…‹ì—ì„œ ì‹¤ì œ ì´ë¯¸ì§€ë¥¼ ë¡œë”©í•´ì£¼ì„¸ìš”!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ” ë‹¨ì¼ ìƒ˜í”Œ Attention ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìŠ¤íŒ¸ í…ìŠ¤íŠ¸ ë¶„ì„\n",
    "print(\"ğŸ” ìŠ¤íŒ¸ í…ìŠ¤íŠ¸ Attention ë¶„ì„ ì¤‘...\")\n",
    "\n",
    "try:\n",
    "    spam_explanation = analyzer.explain_prediction(\n",
    "        text=spam_text,\n",
    "        image=dummy_image,\n",
    "        return_attention_maps=True\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… ë¶„ì„ ì™„ë£Œ!\")\n",
    "    \n",
    "    # ì˜ˆì¸¡ ê²°ê³¼ ì¶œë ¥\n",
    "    pred = spam_explanation['prediction']\n",
    "    print(f\"\\nğŸ“Š ì˜ˆì¸¡ ê²°ê³¼:\")\n",
    "    print(f\"  â€¢ ì˜ˆì¸¡ ë¼ë²¨: {pred['label']}\")\n",
    "    print(f\"  â€¢ ì˜ˆì¸¡ ì ìˆ˜: {pred['score']:.4f}\")\n",
    "    print(f\"  â€¢ ì‹ ë¢°ë„: {pred['confidence']:.4f}\")\n",
    "    \n",
    "    # ìƒìœ„ ì¤‘ìš” í† í° ì¶œë ¥\n",
    "    important_tokens = spam_explanation['text_analysis']['important_tokens'][:5]\n",
    "    print(f\"\\nğŸ“ ê°€ì¥ ì¤‘ìš”í•œ í…ìŠ¤íŠ¸ í† í° (Top 5):\")\n",
    "    for i, token in enumerate(important_tokens, 1):\n",
    "        print(f\"  {i}. '{token['token']}' - ì¤‘ìš”ë„: {token['combined_importance']:.4f}\")\n",
    "    \n",
    "    # ëª¨ë‹¬ë¦¬í‹° ê· í˜•\n",
    "    cross_modal = spam_explanation['cross_modal_analysis']\n",
    "    print(f\"\\nâš–ï¸ ëª¨ë‹¬ë¦¬í‹° ê· í˜•:\")\n",
    "    print(f\"  â€¢ í…ìŠ¤íŠ¸â†’ì´ë¯¸ì§€: {cross_modal['text_to_image_strength']:.4f}\")\n",
    "    print(f\"  â€¢ ì´ë¯¸ì§€â†’í…ìŠ¤íŠ¸: {cross_modal['image_to_text_strength']:.4f}\")\n",
    "    print(f\"  â€¢ ê· í˜•ë„: {cross_modal['modality_balance']:.4f} (0=í…ìŠ¤íŠ¸ ì¤‘ì‹¬, 1=ì´ë¯¸ì§€ ì¤‘ì‹¬)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ë¶„ì„ ì‹¤íŒ¨: {e}\")\n",
    "    spam_explanation = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š í…ìŠ¤íŠ¸ Attention ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if spam_explanation is not None:\n",
    "    # í…ìŠ¤íŠ¸ attention ì‹œê°í™”\n",
    "    text_fig = visualizer.visualize_text_attention(\n",
    "        tokens=spam_explanation['text_analysis']['tokens'],\n",
    "        token_importance=spam_explanation['text_analysis']['important_tokens'],\n",
    "        title=\"ìŠ¤íŒ¸ í…ìŠ¤íŠ¸ Attention ë¶„ì„\"\n",
    "    )\n",
    "    \n",
    "    plt.show()\n",
    "    print(\"ğŸ“Š í…ìŠ¤íŠ¸ attention ì‹œê°í™” ì™„ë£Œ!\")\nelse:\n",
    "    print(\"âŒ ë¶„ì„ ê²°ê³¼ê°€ ì—†ì–´ ì‹œê°í™”ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ–¼ï¸ ì´ë¯¸ì§€ Attention ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if spam_explanation is not None:\n",
    "    # ì´ë¯¸ì§€ attention ì‹œê°í™”\n",
    "    image_fig = visualizer.visualize_image_attention(\n",
    "        image=dummy_image,\n",
    "        patch_importance=spam_explanation['image_analysis']['important_patches'],\n",
    "        title=\"ìŠ¤íŒ¸ ì´ë¯¸ì§€ Attention ë¶„ì„\"\n",
    "    )\n",
    "    \n",
    "    plt.show()\n",
    "    print(\"ğŸ–¼ï¸ ì´ë¯¸ì§€ attention ì‹œê°í™” ì™„ë£Œ!\")\nelse:\n",
    "    print(\"âŒ ë¶„ì„ ê²°ê³¼ê°€ ì—†ì–´ ì‹œê°í™”ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”„ Cross-Modal Attention ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if spam_explanation is not None and 'attention_maps' in spam_explanation:\n",
    "    # Cross-modal attention ì‹œê°í™”\n",
    "    cross_modal_fig = visualizer.visualize_cross_modal_attention(\n",
    "        cross_modal_attention=spam_explanation['attention_maps']['cross_modal_attention'],\n",
    "        tokens=spam_explanation['text_analysis']['tokens'],\n",
    "        title=\"ìŠ¤íŒ¸ Cross-Modal Attention ë¶„ì„\"\n",
    "    )\n",
    "    \n",
    "    plt.show()\n",
    "    print(\"ğŸ”„ Cross-modal attention ì‹œê°í™” ì™„ë£Œ!\")\nelse:\n",
    "    print(\"âŒ Cross-modal attention ë°ì´í„°ê°€ ì—†ì–´ ì‹œê°í™”ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ ì¢…í•© ë¶„ì„ ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if spam_explanation is not None:\n",
    "    # ì¢…í•© ë¶„ì„ ì‹œê°í™”\n",
    "    comprehensive_fig = visualizer.visualize_comprehensive_explanation(\n",
    "        explanation=spam_explanation,\n",
    "        image=dummy_image,\n",
    "        title=\"ìŠ¤íŒ¸ ì´ë©”ì¼ ì¢…í•© Attention ë¶„ì„\"\n",
    "    )\n",
    "    \n",
    "    plt.show()\n",
    "    print(\"ğŸ¯ ì¢…í•© ë¶„ì„ ì‹œê°í™” ì™„ë£Œ!\")\nelse:\n",
    "    print(\"âŒ ë¶„ì„ ê²°ê³¼ê°€ ì—†ì–´ ì‹œê°í™”ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”„ ì •ìƒ ì´ë©”ì¼ ë¹„êµ ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì •ìƒ ì´ë©”ì¼ ë¶„ì„\n",
    "print(\"ğŸ” ì •ìƒ ì´ë©”ì¼ Attention ë¶„ì„ ì¤‘...\")\n",
    "\n",
    "try:\n",
    "    ham_explanation = analyzer.explain_prediction(\n",
    "        text=ham_text,\n",
    "        image=dummy_image,\n",
    "        return_attention_maps=True\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… ì •ìƒ ì´ë©”ì¼ ë¶„ì„ ì™„ë£Œ!\")\n",
    "    \n",
    "    # ìŠ¤íŒ¸ vs ì •ìƒ ë¹„êµ\n",
    "    if spam_explanation is not None:\n",
    "        print(\"\\nğŸ“Š ìŠ¤íŒ¸ vs ì •ìƒ ì´ë©”ì¼ ë¹„êµ:\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        print(f\"ìŠ¤íŒ¸ ì´ë©”ì¼:\")\n",
    "        print(f\"  â€¢ ì˜ˆì¸¡: {spam_explanation['prediction']['label']} ({spam_explanation['prediction']['score']:.4f})\")\n",
    "        print(f\"  â€¢ ëª¨ë‹¬ë¦¬í‹° ê· í˜•: {spam_explanation['cross_modal_analysis']['modality_balance']:.4f}\")\n",
    "        \n",
    "        print(f\"\\nì •ìƒ ì´ë©”ì¼:\")\n",
    "        print(f\"  â€¢ ì˜ˆì¸¡: {ham_explanation['prediction']['label']} ({ham_explanation['prediction']['score']:.4f})\")\n",
    "        print(f\"  â€¢ ëª¨ë‹¬ë¦¬í‹° ê· í˜•: {ham_explanation['cross_modal_analysis']['modality_balance']:.4f}\")\n",
    "        \n",
    "        # ì¤‘ìš” í† í° ë¹„êµ\n",
    "        spam_tokens = [t['token'] for t in spam_explanation['text_analysis']['important_tokens'][:3]]\n",
    "        ham_tokens = [t['token'] for t in ham_explanation['text_analysis']['important_tokens'][:3]]\n",
    "        \n",
    "        print(f\"\\nğŸ“ ì¤‘ìš” í† í° ë¹„êµ:\")\n",
    "        print(f\"ìŠ¤íŒ¸: {spam_tokens}\")\n",
    "        print(f\"ì •ìƒ: {ham_tokens}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ ì •ìƒ ì´ë©”ì¼ ë¶„ì„ ì‹¤íŒ¨: {e}\")\n",
    "    ham_explanation = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ ê²°ê³¼ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²°ê³¼ ì €ì¥\n",
    "output_dir = '../outputs/demo_results'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "if spam_explanation is not None:\n",
    "    # ìŠ¤íŒ¸ ë¶„ì„ ê²°ê³¼ ì €ì¥\n",
    "    analyzer.save_explanation(\n",
    "        spam_explanation,\n",
    "        f'{output_dir}/spam_explanation.json',\n",
    "        include_attention_maps=False\n",
    "    )\n",
    "    print(\"ğŸ’¾ ìŠ¤íŒ¸ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ!\")\n",
    "\n",
    "if ham_explanation is not None:\n",
    "    # ì •ìƒ ë¶„ì„ ê²°ê³¼ ì €ì¥\n",
    "    analyzer.save_explanation(\n",
    "        ham_explanation,\n",
    "        f'{output_dir}/ham_explanation.json',\n",
    "        include_attention_maps=False\n",
    "    )\n",
    "    print(\"ğŸ’¾ ì •ìƒ ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ!\")\n",
    "\n",
    "print(f\"\\nğŸ“ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ‰ ë°ëª¨ ì™„ë£Œ!\n",
    "\n",
    "### ğŸ” **ë¶„ì„ëœ ë‚´ìš©**\n",
    "1. **í…ìŠ¤íŠ¸ Attention**: ì–´ë–¤ ë‹¨ì–´ê°€ ìŠ¤íŒ¸/ì •ìƒ íŒë‹¨ì— ì¤‘ìš”í•œì§€\n",
    "2. **ì´ë¯¸ì§€ Attention**: ì–´ë–¤ ì´ë¯¸ì§€ ì˜ì—­ì´ ì¤‘ìš”í•œì§€\n",
    "3. **Cross-Modal Attention**: í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ê°€ ì–´ë–»ê²Œ ìƒí˜¸ì‘ìš©í•˜ëŠ”ì§€\n",
    "4. **ëª¨ë‹¬ë¦¬í‹° ê· í˜•**: í…ìŠ¤íŠ¸ vs ì´ë¯¸ì§€ ê¸°ì—¬ë„\n",
    "\n",
    "### ğŸš€ **ë‹¤ìŒ ë‹¨ê³„**\n",
    "- ì‹¤ì œ ë°ì´í„°ì…‹ìœ¼ë¡œ ë°°ì¹˜ ë¶„ì„ ì‹¤í–‰: `scripts/attention_analysis_experiment.py`\n",
    "- ë” ë§ì€ ìƒ˜í”Œë¡œ íŒ¨í„´ ë¶„ì„\n",
    "- ì˜¤ë¥˜ ì‚¬ë¡€ ì‹¬ì¸µ ë¶„ì„\n",
    "- ëª¨ë¸ ê°œì„ ì  ë„ì¶œ\n",
    "\n",
    "### ğŸ“Š **í•µì‹¬ ì¥ì **\n",
    "- âœ… **ì™„ì „í•œ íˆ¬ëª…ì„±**: ëª¨ë“  ì˜ˆì¸¡ì— ëŒ€í•œ ëª…í™•í•œ ê·¼ê±° ì œê³µ\n",
    "- âœ… **ë‹¤ì¤‘ëª¨ë‹¬ í•´ì„**: í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ëª¨ë‘ ë¶„ì„\n",
    "- âœ… **ì§ê´€ì  ì‹œê°í™”**: ë¹„ì „ë¬¸ê°€ë„ ì´í•´ ê°€ëŠ¥\n",
    "- âœ… **ì‹¤ìš©ì  í™œìš©**: ì‹¤ì œ ìŠ¤íŒ¸ í•„í„°ë§ ì‹œìŠ¤í…œì— ì ìš© ê°€ëŠ¥\n",
    "\n",
    "---\n",
    "*ğŸ”¬ ì´ê²ƒì´ ë°”ë¡œ \"ì§„ì§œ í•´ì„ê°€ëŠ¥í•œ\" AIì…ë‹ˆë‹¤!*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}