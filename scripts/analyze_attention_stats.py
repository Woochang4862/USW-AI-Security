#!/usr/bin/env python3
"""
ì–´í…ì…˜ í†µê³„ ìƒì„¸ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
"""

import json
import numpy as np

def main():
    with open('outputs/meaningful_interpretability/meaningful_interpretability_report.json', 'r') as f:
        report = json.load(f)

    # ì–´í…ì…˜ ì ìˆ˜ë“¤ ë¶„ì„
    scores = report['attention_analysis']['text_attention']['cls_attention_scores']
    tokens = report['attention_analysis']['sample_info']['tokens']

    print('ğŸ” ì–´í…ì…˜ ë¶„í¬ ìƒì„¸ ë¶„ì„')
    print('=' * 40)

    # ê¸°ë³¸ í†µê³„
    print(f'ì´ í† í° ìˆ˜: {len(scores)}ê°œ')
    print(f'í‰ê· : {np.mean(scores):.6f}')
    print(f'ìµœëŒ€: {np.max(scores):.6f}') 
    print(f'ìµœì†Œ: {np.min(scores):.6f}')
    print(f'í‘œì¤€í¸ì°¨: {np.std(scores):.6f}')

    # ìƒìœ„ í† í°ë“¤
    sorted_indices = np.argsort(scores)[::-1]
    print(f'\nğŸ”¥ ìƒìœ„ 5ê°œ í† í°:')
    for i in range(5):
        idx = sorted_indices[i]
        if idx < len(tokens):
            token = tokens[idx]
            score = scores[idx]
            print(f'{i+1}. "{token}": {score:.6f} ({score*100:.2f}%)')

    # ì–´í…ì…˜ ì§‘ì¤‘ë„ ë¶„ì„
    high_attention = sum(1 for s in scores if s > 0.01)
    medium_attention = sum(1 for s in scores if 0.001 < s <= 0.01) 
    low_attention = sum(1 for s in scores if s <= 0.001)

    print(f'\nğŸ“Š ì–´í…ì…˜ ì§‘ì¤‘ë„:')
    print(f'ê³ ì–´í…ì…˜ (>1%): {high_attention}ê°œ í† í°')
    print(f'ì¤‘ì–´í…ì…˜ (0.1-1%): {medium_attention}ê°œ í† í°')
    print(f'ì €ì–´í…ì…˜ (<0.1%): {low_attention}ê°œ í† í°')

    # SEP í† í° ì œì™¸í•œ ë¶„ì„
    non_sep_indices = [i for i, token in enumerate(tokens) if token != '[SEP]']
    non_sep_scores = [scores[i] for i in non_sep_indices if i < len(scores)]
    non_sep_tokens = [tokens[i] for i in non_sep_indices]

    print(f'\nğŸš« [SEP] í† í° ì œì™¸ ë¶„ì„:')
    if non_sep_scores:
        print(f'í‰ê· : {np.mean(non_sep_scores):.6f}')
        print(f'ìµœëŒ€: {np.max(non_sep_scores):.6f}')
        print(f'ì–´í…ì…˜ ì§‘ì¤‘ë„: {np.std(non_sep_scores):.6f}')
        
        # SEP ì œì™¸ ìƒìœ„ í† í°ë“¤
        non_sep_sorted = np.argsort(non_sep_scores)[::-1]
        print(f'\nğŸ¯ [SEP] ì œì™¸ ìƒìœ„ 3ê°œ ì‹¤ì œ ì¤‘ìš” í† í°:')
        for i in range(min(3, len(non_sep_sorted))):
            idx = non_sep_sorted[i]
            if idx < len(non_sep_tokens):
                token = non_sep_tokens[idx]
                score = non_sep_scores[idx]
                print(f'{i+1}. "{token}": {score:.6f} ({score*100:.2f}%)')

    # ì—”íŠ¸ë¡œí”¼ ì˜ë¯¸ ì„¤ëª…
    entropy = report['attention_analysis']['text_attention']['attention_statistics']['attention_entropy']
    max_entropy = np.log(len(scores))
    
    print(f'\nğŸ“ˆ ì—”íŠ¸ë¡œí”¼ ë¶„ì„:')
    print(f'í˜„ì¬ ì—”íŠ¸ë¡œí”¼: {entropy:.4f}')
    print(f'ìµœëŒ€ ì—”íŠ¸ë¡œí”¼: {max_entropy:.4f} (ì™„ì „ ê· ë“± ë¶„í¬)')
    print(f'ì •ê·œí™”ëœ ì—”íŠ¸ë¡œí”¼: {entropy/max_entropy:.4f} (0=ì™„ì „ì§‘ì¤‘, 1=ì™„ì „ê· ë“±)')
    
    if entropy/max_entropy < 0.3:
        print('â†’ ë§¤ìš° ì§‘ì¤‘ëœ ì–´í…ì…˜ íŒ¨í„´')
    elif entropy/max_entropy < 0.6:
        print('â†’ ì¤‘ê°„ ì •ë„ ì§‘ì¤‘ëœ ì–´í…ì…˜ íŒ¨í„´')
    else:
        print('â†’ ê³ ë¥´ê²Œ ë¶„ì‚°ëœ ì–´í…ì…˜ íŒ¨í„´')

if __name__ == "__main__":
    main() 