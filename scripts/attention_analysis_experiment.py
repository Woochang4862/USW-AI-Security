"""
Attention-based Interpretability Analysis Experiment
MMTD ëª¨ë¸ì˜ Attention ê¸°ë°˜ í•´ì„ê°€ëŠ¥ì„± ë¶„ì„ ì‹¤í—˜
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

import torch
import torch.nn.functional as F
import numpy as np
import pandas as pd
from pathlib import Path
import json
import logging
from datetime import datetime
from tqdm import tqdm
import argparse
from typing import Dict, List, Any, Tuple
import warnings
warnings.filterwarnings('ignore')

# Custom imports
from src.analysis.attention_analyzer import AttentionAnalyzer
from src.analysis.attention_visualizer import AttentionVisualizer
from src.models.interpretable_mmtd import InterpretableMMTD
from src.data_loader import EDPDataModule
from transformers import AutoTokenizer
from torchvision import transforms

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(f'logs/attention_analysis_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class AttentionAnalysisExperiment:
    """Attention ê¸°ë°˜ í•´ì„ê°€ëŠ¥ì„± ë¶„ì„ ì‹¤í—˜ í´ë˜ìŠ¤"""
    
    def __init__(self, config: Dict[str, Any]):
        """
        Args:
            config: ì‹¤í—˜ ì„¤ì • ë”•ì…”ë„ˆë¦¬
        """
        self.config = config
        self.device = torch.device(config.get('device', 'cuda' if torch.cuda.is_available() else 'cpu'))
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        self.output_dir = Path(config['output_dir'])
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # ëª¨ë¸ ë° ë¶„ì„ê¸° ì´ˆê¸°í™”
        self.model = None
        self.tokenizer = None
        self.analyzer = None
        self.visualizer = None
        self.data_module = None
        
        logger.info(f"ğŸš€ Attention ë¶„ì„ ì‹¤í—˜ ì´ˆê¸°í™” (ë””ë°”ì´ìŠ¤: {self.device})")
    
    def load_model_and_tokenizer(self):
        """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë”©"""
        
        logger.info("ğŸ“¥ ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë”© ì¤‘...")
        
        try:
            # ëª¨ë¸ ë¡œë”©
            model_path = self.config['model_path']
            self.model = InterpretableMMTD.load_from_checkpoint(
                model_path,
                map_location=self.device
            )
            self.model.to(self.device)
            self.model.eval()
            
            # í† í¬ë‚˜ì´ì € ë¡œë”©
            tokenizer_name = self.config.get('tokenizer_name', 'bert-base-uncased')
            self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)
            
            # ë¶„ì„ê¸° ë° ì‹œê°í™” ë„êµ¬ ì´ˆê¸°í™”
            self.analyzer = AttentionAnalyzer(self.model, self.tokenizer, self.device)
            self.visualizer = AttentionVisualizer()
            
            logger.info("âœ… ëª¨ë¸ ë° ë„êµ¬ ë¡œë”© ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            raise
    
    def load_data(self):
        """ë°ì´í„° ë¡œë”©"""
        
        logger.info("ğŸ“Š ë°ì´í„° ë¡œë”© ì¤‘...")
        
        try:
            self.data_module = EDPDataModule(
                data_dir=self.config['data_dir'],
                batch_size=1,  # ë¶„ì„ì„ ìœ„í•´ ë°°ì¹˜ í¬ê¸° 1
                num_workers=0
            )
            self.data_module.setup()
            
            logger.info("âœ… ë°ì´í„° ë¡œë”© ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {e}")
            raise
    
    def analyze_single_sample(self, text: str, image: torch.Tensor, 
                            label: int, sample_id: str) -> Dict[str, Any]:
        """ë‹¨ì¼ ìƒ˜í”Œì— ëŒ€í•œ attention ë¶„ì„"""
        
        logger.info(f"ğŸ” ìƒ˜í”Œ {sample_id} ë¶„ì„ ì¤‘...")
        
        try:
            # Attention ë¶„ì„ ìˆ˜í–‰
            explanation = self.analyzer.explain_prediction(
                text=text,
                image=image,
                return_attention_maps=True
            )
            
            # ì‹¤ì œ ë¼ë²¨ ì •ë³´ ì¶”ê°€
            explanation['ground_truth'] = {
                'label': 'SPAM' if label == 1 else 'HAM',
                'class': int(label)
            }
            
            # ì˜ˆì¸¡ ì •í™•ì„± ê³„ì‚°
            predicted_class = explanation['prediction']['class']
            is_correct = (predicted_class == label)
            explanation['prediction']['is_correct'] = is_correct
            explanation['prediction']['accuracy'] = 1.0 if is_correct else 0.0
            
            return explanation
            
        except Exception as e:
            logger.error(f"âŒ ìƒ˜í”Œ {sample_id} ë¶„ì„ ì‹¤íŒ¨: {e}")
            return None
    
    def create_visualizations(self, explanation: Dict[str, Any], 
                            image: torch.Tensor, sample_id: str):
        """ë‹¨ì¼ ìƒ˜í”Œì— ëŒ€í•œ ì‹œê°í™” ìƒì„±"""
        
        try:
            vis_dir = self.output_dir / 'visualizations' / sample_id
            vis_dir.mkdir(parents=True, exist_ok=True)
            
            # 1. í…ìŠ¤íŠ¸ attention ì‹œê°í™”
            if explanation['text_analysis']['important_tokens']:
                text_fig = self.visualizer.visualize_text_attention(
                    tokens=explanation['text_analysis']['tokens'],
                    token_importance=explanation['text_analysis']['important_tokens'],
                    title=f"í…ìŠ¤íŠ¸ Attention ë¶„ì„ - {sample_id}",
                    save_path=str(vis_dir / 'text_attention.png')
                )
                
            # 2. ì´ë¯¸ì§€ attention ì‹œê°í™”
            if explanation['image_analysis']['important_patches']:
                image_fig = self.visualizer.visualize_image_attention(
                    image=image,
                    patch_importance=explanation['image_analysis']['important_patches'],
                    title=f"ì´ë¯¸ì§€ Attention ë¶„ì„ - {sample_id}",
                    save_path=str(vis_dir / 'image_attention.png')
                )
            
            # 3. Cross-modal attention ì‹œê°í™”
            if 'attention_maps' in explanation and explanation['attention_maps']['cross_modal_attention']:
                cross_modal_fig = self.visualizer.visualize_cross_modal_attention(
                    cross_modal_attention=explanation['attention_maps']['cross_modal_attention'],
                    tokens=explanation['text_analysis']['tokens'],
                    title=f"Cross-Modal Attention ë¶„ì„ - {sample_id}",
                    save_path=str(vis_dir / 'cross_modal_attention.png')
                )
            
            # 4. ì¢…í•© ë¶„ì„ ì‹œê°í™”
            comprehensive_fig = self.visualizer.visualize_comprehensive_explanation(
                explanation=explanation,
                image=image,
                title=f"ì¢…í•© Attention ë¶„ì„ - {sample_id}",
                save_path=str(vis_dir / 'comprehensive_analysis.png')
            )
            
            logger.info(f"ğŸ“Š ìƒ˜í”Œ {sample_id} ì‹œê°í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ ìƒ˜í”Œ {sample_id} ì‹œê°í™” ì‹¤íŒ¨: {e}")
    
    def run_batch_analysis(self, num_samples: int = 50) -> List[Dict[str, Any]]:
        """ë°°ì¹˜ ë¶„ì„ ì‹¤í–‰"""
        
        logger.info(f"ğŸ” {num_samples}ê°œ ìƒ˜í”Œ ë°°ì¹˜ ë¶„ì„ ì‹œì‘")
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œë” ê°€ì ¸ì˜¤ê¸°
        test_loader = self.data_module.test_dataloader()
        
        explanations = []
        sample_count = 0
        
        for batch_idx, batch in enumerate(tqdm(test_loader, desc="Attention ë¶„ì„ ì§„í–‰ì¤‘")):
            if sample_count >= num_samples:
                break
            
            # ë°°ì¹˜ì—ì„œ ë°ì´í„° ì¶”ì¶œ
            text = batch['text'][0] if isinstance(batch['text'], list) else batch['text'][0].item()
            image = batch['image'][0]
            label = batch['label'][0].item()
            
            sample_id = f"sample_{batch_idx:04d}"
            
            # ë‹¨ì¼ ìƒ˜í”Œ ë¶„ì„
            explanation = self.analyze_single_sample(text, image, label, sample_id)
            
            if explanation is not None:
                explanations.append(explanation)
                
                # ì‹œê°í™” ìƒì„± (ì²˜ìŒ 10ê°œ ìƒ˜í”Œë§Œ)
                if sample_count < 10:
                    self.create_visualizations(explanation, image, sample_id)
                
                # ê°œë³„ ê²°ê³¼ ì €ì¥
                self.analyzer.save_explanation(
                    explanation,
                    str(self.output_dir / f'{sample_id}_explanation.json'),
                    include_attention_maps=False
                )
                
                sample_count += 1
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            torch.cuda.empty_cache() if torch.cuda.is_available() else None
        
        logger.info(f"âœ… ë°°ì¹˜ ë¶„ì„ ì™„ë£Œ: {len(explanations)}ê°œ ìƒ˜í”Œ ì²˜ë¦¬")
        return explanations
    
    def analyze_attention_patterns(self, explanations: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Attention íŒ¨í„´ í†µê³„ ë¶„ì„"""
        
        logger.info("ğŸ“ˆ Attention íŒ¨í„´ ë¶„ì„ ì¤‘...")
        
        analysis = {
            'total_samples': len(explanations),
            'accuracy': sum(exp['prediction']['accuracy'] for exp in explanations) / len(explanations),
            'spam_samples': sum(1 for exp in explanations if exp['ground_truth']['class'] == 1),
            'ham_samples': sum(1 for exp in explanations if exp['ground_truth']['class'] == 0),
            'text_importance_stats': {},
            'image_importance_stats': {},
            'modality_balance_stats': {},
            'error_analysis': {}
        }
        
        # í…ìŠ¤íŠ¸ ì¤‘ìš”ë„ ë¶„ì„
        text_importances = []
        important_tokens_all = []
        
        for exp in explanations:
            if exp['text_analysis']['important_tokens']:
                token_scores = [t['combined_importance'] for t in exp['text_analysis']['important_tokens']]
                text_importances.extend(token_scores)
                important_tokens_all.extend([t['token'] for t in exp['text_analysis']['important_tokens'][:5]])
        
        if text_importances:
            analysis['text_importance_stats'] = {
                'mean': np.mean(text_importances),
                'std': np.std(text_importances),
                'min': np.min(text_importances),
                'max': np.max(text_importances),
                'most_common_tokens': pd.Series(important_tokens_all).value_counts().head(10).to_dict()
            }
        
        # ì´ë¯¸ì§€ ì¤‘ìš”ë„ ë¶„ì„
        image_importances = []
        important_patches_all = []
        
        for exp in explanations:
            if exp['image_analysis']['important_patches']:
                patch_scores = [p['combined_importance'] for p in exp['image_analysis']['important_patches']]
                image_importances.extend(patch_scores)
                important_patches_all.extend([str(p['coordinates']) for p in exp['image_analysis']['important_patches'][:5]])
        
        if image_importances:
            analysis['image_importance_stats'] = {
                'mean': np.mean(image_importances),
                'std': np.std(image_importances),
                'min': np.min(image_importances),
                'max': np.max(image_importances),
                'most_common_patches': pd.Series(important_patches_all).value_counts().head(10).to_dict()
            }
        
        # ëª¨ë‹¬ë¦¬í‹° ê· í˜• ë¶„ì„
        modality_balances = [exp['cross_modal_analysis']['modality_balance'] for exp in explanations]
        
        analysis['modality_balance_stats'] = {
            'mean_balance': np.mean(modality_balances),
            'std_balance': np.std(modality_balances),
            'text_dominant_samples': sum(1 for b in modality_balances if b < 0.4),
            'image_dominant_samples': sum(1 for b in modality_balances if b > 0.6),
            'balanced_samples': sum(1 for b in modality_balances if 0.4 <= b <= 0.6)
        }
        
        # ì˜¤ë¥˜ ë¶„ì„
        correct_predictions = [exp for exp in explanations if exp['prediction']['is_correct']]
        incorrect_predictions = [exp for exp in explanations if not exp['prediction']['is_correct']]
        
        analysis['error_analysis'] = {
            'correct_count': len(correct_predictions),
            'incorrect_count': len(incorrect_predictions),
            'false_positives': sum(1 for exp in incorrect_predictions 
                                 if exp['prediction']['class'] == 1 and exp['ground_truth']['class'] == 0),
            'false_negatives': sum(1 for exp in incorrect_predictions 
                                 if exp['prediction']['class'] == 0 and exp['ground_truth']['class'] == 1)
        }
        
        # ì •í™•/ì˜¤ë‹µ ì˜ˆì¸¡ì˜ attention íŒ¨í„´ ì°¨ì´
        if correct_predictions and incorrect_predictions:
            correct_modality_balance = np.mean([exp['cross_modal_analysis']['modality_balance'] 
                                              for exp in correct_predictions])
            incorrect_modality_balance = np.mean([exp['cross_modal_analysis']['modality_balance'] 
                                                for exp in incorrect_predictions])
            
            analysis['error_analysis']['modality_balance_difference'] = {
                'correct_mean': correct_modality_balance,
                'incorrect_mean': incorrect_modality_balance,
                'difference': abs(correct_modality_balance - incorrect_modality_balance)
            }
        
        logger.info("âœ… Attention íŒ¨í„´ ë¶„ì„ ì™„ë£Œ")
        return analysis
    
    def create_summary_report(self, explanations: List[Dict[str, Any]], 
                            analysis: Dict[str, Any]):
        """ìš”ì•½ ë³´ê³ ì„œ ìƒì„±"""
        
        logger.info("ğŸ“‹ ìš”ì•½ ë³´ê³ ì„œ ìƒì„± ì¤‘...")
        
        # ëŒ€í‘œ ìƒ˜í”Œë“¤ ì„ íƒ (ì •í™•í•œ ì˜ˆì¸¡, í‹€ë¦° ì˜ˆì¸¡, ë‹¤ì–‘í•œ ëª¨ë‹¬ë¦¬í‹° ê· í˜•)
        representative_samples = []
        
        # ì •í™•í•œ ì˜ˆì¸¡ ì¤‘ ì‹ ë¢°ë„ ë†’ì€ ìƒ˜í”Œ
        correct_samples = [exp for exp in explanations if exp['prediction']['is_correct']]
        if correct_samples:
            best_correct = max(correct_samples, key=lambda x: x['prediction']['confidence'])
            representative_samples.append(best_correct)
        
        # í‹€ë¦° ì˜ˆì¸¡ ì¤‘ ì‹ ë¢°ë„ ë†’ì€ ìƒ˜í”Œ (í¥ë¯¸ë¡œìš´ ì¼€ì´ìŠ¤)
        incorrect_samples = [exp for exp in explanations if not exp['prediction']['is_correct']]
        if incorrect_samples:
            worst_incorrect = max(incorrect_samples, key=lambda x: x['prediction']['confidence'])
            representative_samples.append(worst_incorrect)
        
        # í…ìŠ¤íŠ¸ ì¤‘ì‹¬ ìƒ˜í”Œ
        text_dominant = [exp for exp in explanations if exp['cross_modal_analysis']['modality_balance'] < 0.3]
        if text_dominant:
            representative_samples.append(text_dominant[0])
        
        # ì´ë¯¸ì§€ ì¤‘ì‹¬ ìƒ˜í”Œ
        image_dominant = [exp for exp in explanations if exp['cross_modal_analysis']['modality_balance'] > 0.7]
        if image_dominant:
            representative_samples.append(image_dominant[0])
        
        # ìš”ì•½ ì‹œê°í™” ìƒì„±
        if representative_samples:
            self.visualizer.create_attention_summary_report(
                representative_samples,
                save_path=str(self.output_dir / 'attention_summary_report.png')
            )
        
        # ë¶„ì„ ê²°ê³¼ ì €ì¥
        with open(self.output_dir / 'attention_analysis_report.json', 'w', encoding='utf-8') as f:
            json.dump(analysis, f, indent=2, ensure_ascii=False)
        
        # ìƒì„¸ ê²°ê³¼ ì €ì¥
        detailed_results = {
            'config': self.config,
            'analysis': analysis,
            'representative_samples': representative_samples
        }
        
        with open(self.output_dir / 'detailed_attention_results.json', 'w', encoding='utf-8') as f:
            json.dump(detailed_results, f, indent=2, ensure_ascii=False)
        
        logger.info("âœ… ìš”ì•½ ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ")
    
    def run_experiment(self):
        """ì „ì²´ ì‹¤í—˜ ì‹¤í–‰"""
        
        logger.info("ğŸš€ Attention ë¶„ì„ ì‹¤í—˜ ì‹œì‘")
        
        try:
            # 1. ëª¨ë¸ ë° ë°ì´í„° ë¡œë”©
            self.load_model_and_tokenizer()
            self.load_data()
            
            # 2. ë°°ì¹˜ ë¶„ì„ ì‹¤í–‰
            explanations = self.run_batch_analysis(self.config.get('num_samples', 50))
            
            # 3. íŒ¨í„´ ë¶„ì„
            analysis = self.analyze_attention_patterns(explanations)
            
            # 4. ìš”ì•½ ë³´ê³ ì„œ ìƒì„±
            self.create_summary_report(explanations, analysis)
            
            # 5. ê²°ê³¼ ì¶œë ¥
            self.print_summary(analysis)
            
            logger.info("âœ… Attention ë¶„ì„ ì‹¤í—˜ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ ì‹¤í—˜ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            raise
    
    def print_summary(self, analysis: Dict[str, Any]):
        """ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        
        print("\n" + "="*80)
        print("ğŸ” ATTENTION ê¸°ë°˜ í•´ì„ê°€ëŠ¥ì„± ë¶„ì„ ê²°ê³¼ ìš”ì•½")
        print("="*80)
        
        print(f"\nğŸ“Š ì „ì²´ í†µê³„:")
        print(f"  â€¢ ë¶„ì„ ìƒ˜í”Œ ìˆ˜: {analysis['total_samples']}ê°œ")
        print(f"  â€¢ ì „ì²´ ì •í™•ë„: {analysis['accuracy']*100:.2f}%")
        print(f"  â€¢ ìŠ¤íŒ¸ ìƒ˜í”Œ: {analysis['spam_samples']}ê°œ")
        print(f"  â€¢ í–„ ìƒ˜í”Œ: {analysis['ham_samples']}ê°œ")
        
        if analysis['text_importance_stats']:
            print(f"\nğŸ“ í…ìŠ¤íŠ¸ Attention í†µê³„:")
            stats = analysis['text_importance_stats']
            print(f"  â€¢ í‰ê·  ì¤‘ìš”ë„: {stats['mean']:.4f}")
            print(f"  â€¢ í‘œì¤€í¸ì°¨: {stats['std']:.4f}")
            print(f"  â€¢ ìµœëŒ€ê°’: {stats['max']:.4f}")
            
            print(f"  â€¢ ìì£¼ ë“±ì¥í•˜ëŠ” ì¤‘ìš” í† í°:")
            for token, count in list(stats['most_common_tokens'].items())[:5]:
                print(f"    - {token}: {count}íšŒ")
        
        if analysis['image_importance_stats']:
            print(f"\nğŸ–¼ï¸ ì´ë¯¸ì§€ Attention í†µê³„:")
            stats = analysis['image_importance_stats']
            print(f"  â€¢ í‰ê·  ì¤‘ìš”ë„: {stats['mean']:.4f}")
            print(f"  â€¢ í‘œì¤€í¸ì°¨: {stats['std']:.4f}")
            print(f"  â€¢ ìµœëŒ€ê°’: {stats['max']:.4f}")
        
        print(f"\nâš–ï¸ ëª¨ë‹¬ë¦¬í‹° ê· í˜• ë¶„ì„:")
        balance_stats = analysis['modality_balance_stats']
        print(f"  â€¢ í‰ê·  ê· í˜•ë„: {balance_stats['mean_balance']:.3f} (0=í…ìŠ¤íŠ¸ ì¤‘ì‹¬, 1=ì´ë¯¸ì§€ ì¤‘ì‹¬)")
        print(f"  â€¢ í…ìŠ¤íŠ¸ ì¤‘ì‹¬ ìƒ˜í”Œ: {balance_stats['text_dominant_samples']}ê°œ")
        print(f"  â€¢ ì´ë¯¸ì§€ ì¤‘ì‹¬ ìƒ˜í”Œ: {balance_stats['image_dominant_samples']}ê°œ")
        print(f"  â€¢ ê· í˜• ì¡íŒ ìƒ˜í”Œ: {balance_stats['balanced_samples']}ê°œ")
        
        print(f"\nâŒ ì˜¤ë¥˜ ë¶„ì„:")
        error_stats = analysis['error_analysis']
        print(f"  â€¢ ì •í™•í•œ ì˜ˆì¸¡: {error_stats['correct_count']}ê°œ")
        print(f"  â€¢ í‹€ë¦° ì˜ˆì¸¡: {error_stats['incorrect_count']}ê°œ")
        print(f"  â€¢ False Positive (í–„â†’ìŠ¤íŒ¸): {error_stats['false_positives']}ê°œ")
        print(f"  â€¢ False Negative (ìŠ¤íŒ¸â†’í–„): {error_stats['false_negatives']}ê°œ")
        
        if 'modality_balance_difference' in error_stats:
            diff_stats = error_stats['modality_balance_difference']
            print(f"  â€¢ ì •í™•í•œ ì˜ˆì¸¡ì˜ ëª¨ë‹¬ë¦¬í‹° ê· í˜•: {diff_stats['correct_mean']:.3f}")
            print(f"  â€¢ í‹€ë¦° ì˜ˆì¸¡ì˜ ëª¨ë‹¬ë¦¬í‹° ê· í˜•: {diff_stats['incorrect_mean']:.3f}")
            print(f"  â€¢ ì°¨ì´: {diff_stats['difference']:.3f}")
        
        print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {self.output_dir}")
        print("="*80)


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    parser = argparse.ArgumentParser(description='Attention ê¸°ë°˜ í•´ì„ê°€ëŠ¥ì„± ë¶„ì„ ì‹¤í—˜')
    parser.add_argument('--model_path', type=str, required=True,
                       help='í•™ìŠµëœ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ')
    parser.add_argument('--data_dir', type=str, required=True,
                       help='ë°ì´í„°ì…‹ ë””ë ‰í† ë¦¬ ê²½ë¡œ')
    parser.add_argument('--output_dir', type=str, 
                       default=f'outputs/attention_analysis_{datetime.now().strftime("%Y%m%d_%H%M%S")}',
                       help='ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬')
    parser.add_argument('--num_samples', type=int, default=50,
                       help='ë¶„ì„í•  ìƒ˜í”Œ ìˆ˜')
    parser.add_argument('--device', type=str, default='auto',
                       help='ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ (cuda/cpu/auto)')
    parser.add_argument('--tokenizer_name', type=str, default='bert-base-uncased',
                       help='ì‚¬ìš©í•  í† í¬ë‚˜ì´ì €')
    
    args = parser.parse_args()
    
    # ë””ë°”ì´ìŠ¤ ìë™ ì„¤ì •
    if args.device == 'auto':
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
    else:
        device = args.device
    
    # ì‹¤í—˜ ì„¤ì •
    config = {
        'model_path': args.model_path,
        'data_dir': args.data_dir,
        'output_dir': args.output_dir,
        'num_samples': args.num_samples,
        'device': device,
        'tokenizer_name': args.tokenizer_name
    }
    
    # ì‹¤í—˜ ì‹¤í–‰
    experiment = AttentionAnalysisExperiment(config)
    experiment.run_experiment()


if __name__ == "__main__":
    main() 