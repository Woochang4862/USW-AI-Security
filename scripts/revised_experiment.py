#!/usr/bin/env python3
"""
ìˆ˜ì •ëœ MMTD ëª¨ë¸ë¡œ ì¬ì‹¤í—˜
pooler ìˆ˜ì •ì‚¬í•­ì„ ë°˜ì˜í•œ Logistic Regression ë¶„ë¥˜ê¸° ì‹¤í—˜
"""

import os
import sys
import torch
import time
import json
import logging
from datetime import datetime
from pathlib import Path
import sklearn

# Add src directory to path
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))

from models.interpretable_mmtd import create_interpretable_mmtd
from models.mmtd_data_loader import create_mmtd_data_module
from models.mmtd_trainer import MMTDTrainer

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class RevisedExperimentRunner:
    """ìˆ˜ì •ëœ ëª¨ë¸ë¡œ ì¬ì‹¤í—˜ ì‹¤í–‰"""
    
    def __init__(self, output_dir: str = None):
        self.device = torch.device("mps" if torch.backends.mps.is_available() 
                                  else "cuda" if torch.cuda.is_available() 
                                  else "cpu")
        
        # Create timestamped output directory
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.output_dir = output_dir or f"outputs/revised_experiment_{timestamp}"
        Path(self.output_dir).mkdir(parents=True, exist_ok=True)
        
        logger.info(f"ğŸ¯ ìˆ˜ì •ëœ ëª¨ë¸ ì¬ì‹¤í—˜ ì‹œì‘")
        logger.info(f"ğŸ’» Device: {self.device}")
        logger.info(f"ğŸ“ Output: {self.output_dir}")
    
    def create_revised_model(self):
        """ìˆ˜ì •ëœ poolerë¥¼ í¬í•¨í•œ InterpretableMMTD ëª¨ë¸ ìƒì„±"""
        logger.info("ğŸ—ï¸ ìˆ˜ì •ëœ ëª¨ë¸ ìƒì„± ì¤‘...")
        
        # Create model with logistic regression classifier
        model = create_interpretable_mmtd(
            classifier_type="logistic_regression",
            classifier_config={
                'l1_lambda': 0.0001,
                'l2_lambda': 0.001,
                'dropout_rate': 0.1
            },
            device=self.device,
            checkpoint_path="MMTD/checkpoints/fold1/checkpoint-939/pytorch_model.bin"
        )
        
        logger.info(f"âœ… ëª¨ë¸ ìƒì„± ì™„ë£Œ")
        logger.info(f"ğŸ“Š ì´ íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in model.parameters()):,}")
        
        # Get model summary
        model_summary = model.get_model_summary()
        logger.info(f"ğŸ§  ë¶„ë¥˜ê¸° íŒŒë¼ë¯¸í„°: {model_summary['classifier_parameters']:,}")
        
        return model, model_summary
    
    def prepare_data(self):
        """ë°ì´í„° ì¤€ë¹„"""
        logger.info("ğŸ“š ë°ì´í„° ë¡œë”© ì¤‘...")
        
        data_module = create_mmtd_data_module(
            csv_path="MMTD/DATA/email_data/EDP.csv",
            data_path="MMTD/DATA/email_data/pics",
            batch_size=16,  # ì•ˆì •ì ì¸ ë°°ì¹˜ í¬ê¸°
            max_samples=1000,  # ë¹ ë¥¸ ì‹¤í—˜ì„ ìœ„í•´ ìƒ˜í”Œ ì œí•œ
            num_workers=2,
            test_split=0.2,
            val_split=0.1
        )
        
        logger.info("âœ… ë°ì´í„° ë¡œë”© ì™„ë£Œ")
        logger.info(f"ğŸ“Š í›ˆë ¨ ìƒ˜í”Œ: {len(data_module.train_dataset)}")
        logger.info(f"ğŸ“Š ê²€ì¦ ìƒ˜í”Œ: {len(data_module.val_dataset)}")
        logger.info(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ: {len(data_module.test_dataset)}")
        
        return data_module
    
    def run_training_experiment(self):
        """í›ˆë ¨ ì‹¤í—˜ ì‹¤í–‰"""
        logger.info("ğŸš€ í›ˆë ¨ ì‹¤í—˜ ì‹œì‘")
        
        # Create model and data
        model, model_summary = self.create_revised_model()
        data_module = self.prepare_data()
        
        # Setup simple training
        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)
        criterion = torch.nn.CrossEntropyLoss()
        
        # Training configuration
        num_epochs = 3
        best_val_accuracy = 0.0
        training_history = []
        
        # Train model
        start_time = time.time()
        
        for epoch in range(num_epochs):
            logger.info(f"ğŸ“š ì—í¬í¬ {epoch + 1}/{num_epochs}")
            
            # Training phase
            model.train()
            train_loss = 0.0
            train_correct = 0
            train_total = 0
            
            train_loader = data_module.train_dataloader()
            for batch_idx, batch in enumerate(train_loader):
                # Move to device
                batch = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v 
                        for k, v in batch.items()}
                
                # Forward pass
                optimizer.zero_grad()
                outputs = model(**batch)
                loss = outputs.loss
                
                # Backward pass
                loss.backward()
                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                optimizer.step()
                
                # Statistics
                train_loss += loss.item()
                predictions = torch.argmax(outputs.logits, dim=1)
                train_correct += (predictions == batch['labels']).sum().item()
                train_total += batch['labels'].size(0)
                
                if batch_idx % 10 == 0:
                    logger.info(f"  ë°°ì¹˜ {batch_idx}: ì†ì‹¤ {loss.item():.4f}")
            
            # Validation phase
            model.eval()
            val_loss = 0.0
            val_correct = 0
            val_total = 0
            
            val_loader = data_module.val_dataloader()
            with torch.no_grad():
                for batch in val_loader:
                    batch = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v 
                            for k, v in batch.items()}
                    
                    outputs = model(**batch)
                    loss = outputs.loss
                    
                    val_loss += loss.item()
                    predictions = torch.argmax(outputs.logits, dim=1)
                    val_correct += (predictions == batch['labels']).sum().item()
                    val_total += batch['labels'].size(0)
            
            # Calculate metrics
            train_accuracy = train_correct / train_total
            val_accuracy = val_correct / val_total
            avg_train_loss = train_loss / len(train_loader)
            avg_val_loss = val_loss / len(val_loader)
            
            # Update best accuracy
            if val_accuracy > best_val_accuracy:
                best_val_accuracy = val_accuracy
            
            # Log results
            epoch_results = {
                'epoch': epoch + 1,
                'train_loss': avg_train_loss,
                'train_accuracy': train_accuracy,
                'val_loss': avg_val_loss,
                'val_accuracy': val_accuracy
            }
            training_history.append(epoch_results)
            
            logger.info(f"  í›ˆë ¨ ì •í™•ë„: {train_accuracy:.4f}, ê²€ì¦ ì •í™•ë„: {val_accuracy:.4f}")
        
        training_time = time.time() - start_time
        
        training_results = {
            'training_history': training_history,
            'best_val_accuracy': best_val_accuracy,
            'total_training_time': training_time
        }
        
        logger.info(f"â±ï¸ í›ˆë ¨ ì™„ë£Œ: {training_time:.2f}ì´ˆ")
        logger.info(f"ğŸ“ˆ ìµœê³  ê²€ì¦ ì •í™•ë„: {best_val_accuracy:.4f}")
        
        return model, training_results, model_summary
    
    def run_evaluation_experiment(self, model, data_module):
        """í‰ê°€ ì‹¤í—˜ ì‹¤í–‰"""
        logger.info("ğŸ“Š í‰ê°€ ì‹¤í—˜ ì‹œì‘")
        
        # Evaluate on test set
        model.eval()
        test_loss = 0.0
        test_correct = 0
        test_total = 0
        all_predictions = []
        all_labels = []
        
        test_loader = data_module.test_dataloader()
        with torch.no_grad():
            for batch in test_loader:
                batch = {k: v.to(self.device) if isinstance(v, torch.Tensor) else v 
                        for k, v in batch.items()}
                
                outputs = model(**batch)
                loss = outputs.loss
                
                test_loss += loss.item()
                predictions = torch.argmax(outputs.logits, dim=1)
                test_correct += (predictions == batch['labels']).sum().item()
                test_total += batch['labels'].size(0)
                
                all_predictions.extend(predictions.cpu().numpy())
                all_labels.extend(batch['labels'].cpu().numpy())
        
        # Calculate detailed metrics
        from sklearn.metrics import precision_recall_fscore_support, confusion_matrix
        
        accuracy = test_correct / test_total
        avg_loss = test_loss / len(test_loader)
        precision, recall, f1_score, _ = precision_recall_fscore_support(
            all_labels, all_predictions, average='weighted'
        )
        
        test_results = {
            'accuracy': accuracy,
            'loss': avg_loss,
            'precision': precision,
            'recall': recall,
            'f1_score': f1_score,
            'confusion_matrix': confusion_matrix(all_labels, all_predictions).tolist()
        }
        
        logger.info(f"ğŸ¯ í…ŒìŠ¤íŠ¸ ì •í™•ë„: {accuracy:.4f}")
        logger.info(f"ğŸ“‰ í…ŒìŠ¤íŠ¸ ì†ì‹¤: {avg_loss:.4f}")
        logger.info(f"ğŸ” F1 ìŠ¤ì½”ì–´: {f1_score:.4f}")
        
        return test_results
    
    def run_interpretability_analysis(self, model, data_module):
        """í•´ì„ì„± ë¶„ì„ ì‹¤í–‰"""
        logger.info("ğŸ” í•´ì„ì„± ë¶„ì„ ì‹œì‘")
        
        # Get feature importance
        try:
            feature_importance = model.get_feature_importance()
            
            # Simple statistics
            importance_stats = {
                'mean_importance': float(torch.mean(torch.abs(feature_importance))),
                'max_importance': float(torch.max(torch.abs(feature_importance))),
                'min_importance': float(torch.min(torch.abs(feature_importance))),
                'std_importance': float(torch.std(feature_importance)),
                'sparsity': float(torch.sum(torch.abs(feature_importance) < 0.001) / len(feature_importance))
            }
            
            logger.info(f"ğŸ“Š í‰ê·  ì¤‘ìš”ë„: {importance_stats['mean_importance']:.6f}")
            logger.info(f"ğŸ“Š ìµœëŒ€ ì¤‘ìš”ë„: {importance_stats['max_importance']:.6f}")
            logger.info(f"ğŸ“Š í¬ì†Œì„±: {importance_stats['sparsity']:.3f}")
            
            return importance_stats
            
        except Exception as e:
            logger.warning(f"í•´ì„ì„± ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {}
    
    def save_results(self, training_results, test_results, model_summary, importance_stats):
        """ê²°ê³¼ ì €ì¥"""
        logger.info("ğŸ’¾ ê²°ê³¼ ì €ì¥ ì¤‘...")
        
        # Combine all results
        final_results = {
            'experiment_info': {
                'timestamp': datetime.now().isoformat(),
                'device': str(self.device),
                'model_type': 'InterpretableMMTD_Revised',
                'classifier_type': 'logistic_regression',
                'note': 'Experiment with corrected pooler implementation'
            },
            'model_summary': model_summary,
            'training_results': training_results,
            'test_results': test_results,
            'interpretability_analysis': importance_stats
        }
        
        # Save to JSON
        results_path = os.path.join(self.output_dir, "revised_experiment_results.json")
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(final_results, f, indent=2, ensure_ascii=False)
        
        logger.info(f"âœ… ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {results_path}")
        
        # Print summary
        self.print_summary(final_results)
        
        return final_results
    
    def print_summary(self, results):
        """ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "="*60)
        print("ğŸ¯ ìˆ˜ì •ëœ MMTD ëª¨ë¸ ì¬ì‹¤í—˜ ê²°ê³¼")
        print("="*60)
        
        # Model info
        model_info = results['model_summary']
        print(f"ğŸ“Š ëª¨ë¸ íƒ€ì…: {model_info['model_type']}")
        print(f"ğŸ§  ë¶„ë¥˜ê¸°: {model_info['classifier_type']}")
        print(f"ğŸ“ˆ ì´ íŒŒë¼ë¯¸í„°: {model_info['total_parameters']:,}")
        print(f"ğŸ›ï¸ í›ˆë ¨ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: {model_info['trainable_parameters']:,}")
        
        # Performance
        test_results = results['test_results']
        print(f"\nğŸ“Š ì„±ëŠ¥ ê²°ê³¼:")
        print(f"  ğŸ¯ í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_results['accuracy']:.4f}")
        print(f"  ğŸ“‰ í…ŒìŠ¤íŠ¸ ì†ì‹¤: {test_results['loss']:.4f}")
        print(f"  ğŸ” F1 ìŠ¤ì½”ì–´: {test_results['f1_score']:.4f}")
        
        # Training info
        training_results = results['training_results']
        if 'best_val_accuracy' in training_results:
            print(f"  ğŸ“ˆ ìµœê³  ê²€ì¦ ì •í™•ë„: {training_results['best_val_accuracy']:.4f}")
        
        # Interpretability
        if results['interpretability_analysis']:
            interp = results['interpretability_analysis']
            print(f"\nğŸ” í•´ì„ì„± ë¶„ì„:")
            print(f"  ğŸ“Š í‰ê·  íŠ¹ì§• ì¤‘ìš”ë„: {interp['mean_importance']:.6f}")
            print(f"  ğŸ“Š í¬ì†Œì„±: {interp['sparsity']:.3f}")
        
        print(f"\nğŸ“ ìƒì„¸ ê²°ê³¼: {self.output_dir}")
        print("="*60)
    
    def run_full_experiment(self):
        """ì „ì²´ ì‹¤í—˜ ì‹¤í–‰"""
        try:
            logger.info("ğŸš€ ìˆ˜ì •ëœ ëª¨ë¸ ì „ì²´ ì¬ì‹¤í—˜ ì‹œì‘")
            
            # 1. Training experiment
            trained_model, training_results, model_summary = self.run_training_experiment()
            
            # 2. Prepare data for evaluation
            data_module = self.prepare_data()
            
            # 3. Evaluation experiment
            test_results = self.run_evaluation_experiment(trained_model, data_module)
            
            # 4. Interpretability analysis
            importance_stats = self.run_interpretability_analysis(trained_model, data_module)
            
            # 5. Save results
            final_results = self.save_results(
                training_results, test_results, model_summary, importance_stats
            )
            
            logger.info("âœ… ìˆ˜ì •ëœ ëª¨ë¸ ì¬ì‹¤í—˜ ì™„ë£Œ!")
            return final_results
            
        except Exception as e:
            logger.error(f"âŒ ì‹¤í—˜ ì‹¤íŒ¨: {e}")
            raise


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    runner = RevisedExperimentRunner()
    results = runner.run_full_experiment()
    return results


if __name__ == "__main__":
    results = main() 