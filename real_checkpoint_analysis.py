import os
import sys
import torch
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import warnings
from typing import Dict, List, Tuple, Optional, Any
import json
from datetime import datetime

# ê¸°ì¡´ MMTD ê´€ë ¨ import
sys.path.append('.')
from models import MMTD
from Email_dataset import EDPDataset, EDPCollator
from utils import SplitData

warnings.filterwarnings('ignore')

class SafeCheckpointAnalyzer:
    """
    ì²´í¬í¬ì¸íŠ¸ í˜¸í™˜ì„± ë¬¸ì œë¥¼ ìš°íšŒí•˜ì—¬ ì‹¤ì œ MMTD ë¶„ì„ì„ ìˆ˜í–‰í•˜ëŠ” í´ë˜ìŠ¤
    """
    
    def __init__(self, checkpoint_path: str):
        self.checkpoint_path = checkpoint_path
        self.device = torch.device("cuda" if torch.cuda.is_available() else 
                                  "mps" if torch.backends.mps.is_available() else "cpu")
        
        print(f"ğŸ”§ Safe Checkpoint Analyzer ì´ˆê¸°í™”")
        print(f"   Device: {self.device}")
        print(f"   Checkpoint: {checkpoint_path}")
        
        # ëª¨ë¸ ë¡œë“œ ì‹œë„
        self.model = self._safe_load_model()
        
        # ë°ì´í„° ë¡œë“œ
        self._load_test_data()
        
        # ê²°ê³¼ ë””ë ‰í„°ë¦¬
        self.results_dir = "real_checkpoint_results"
        os.makedirs(self.results_dir, exist_ok=True)
        
        print("âœ… ì´ˆê¸°í™” ì™„ë£Œ!")
    
    def _safe_load_model(self):
        """ì•ˆì „í•œ ë°©ë²•ìœ¼ë¡œ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
        print("\nğŸ”„ MMTD ëª¨ë¸ ë¡œë”© ì‹œë„...")
        
        try:
            # ê¸°ë³¸ MMTD ëª¨ë¸ ìƒì„±
            model = MMTD(
                bert_pretrain_weight='bert-base-multilingual-cased',
                beit_pretrain_weight='microsoft/dit-base'
            )
            
            print("   âœ… ê¸°ë³¸ MMTD ëª¨ë¸ ìƒì„± ì™„ë£Œ")
            
            # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹œë„ (ì—¬ëŸ¬ ë°©ë²• ì‹œë„)
            checkpoint_file = os.path.join(self.checkpoint_path, 'pytorch_model.bin')
            
            if os.path.exists(checkpoint_file):
                print(f"   ğŸ“ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹œë„: {checkpoint_file}")
                
                try:
                    # ë°©ë²• 1: weights_only=False (êµ¬ë²„ì „ í˜¸í™˜)
                    checkpoint = torch.load(checkpoint_file, map_location='cpu')
                    missing_keys, unexpected_keys = model.load_state_dict(checkpoint, strict=False)
                    
                    print(f"   âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì„±ê³µ!")
                    print(f"      Missing keys: {len(missing_keys)}")
                    print(f"      Unexpected keys: {len(unexpected_keys)}")
                    
                except Exception as e1:
                    print(f"   âš ï¸ ì²« ë²ˆì§¸ ë¡œë”© ë°©ë²• ì‹¤íŒ¨: {str(e1)[:100]}...")
                    
                    try:
                        # ë°©ë²• 2: pickle í”„ë¡œí† ì½œ ì§€ì •
                        checkpoint = torch.load(checkpoint_file, map_location='cpu', pickle_protocol=4)
                        missing_keys, unexpected_keys = model.load_state_dict(checkpoint, strict=False)
                        print(f"   âœ… ë‘ ë²ˆì§¸ ë°©ë²•ìœ¼ë¡œ ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì„±ê³µ!")
                        
                    except Exception as e2:
                        print(f"   âš ï¸ ë‘ ë²ˆì§¸ ë¡œë”© ë°©ë²•ë„ ì‹¤íŒ¨: {str(e2)[:100]}...")
                        print("   ğŸ”„ ì²´í¬í¬ì¸íŠ¸ ì—†ì´ ê¸°ë³¸ ëª¨ë¸ë¡œ ì§„í–‰")
            else:
                print(f"   âš ï¸ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ì—†ìŒ: {checkpoint_file}")
            
            # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
            model.eval()
            model.to(self.device)
            
            print(f"   ğŸ¯ ëª¨ë¸ ì¤€ë¹„ ì™„ë£Œ (device: {self.device})")
            return model
            
        except Exception as e:
            print(f"   âŒ ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {str(e)}")
            print("   ğŸ”„ ìµœì†Œ ëª¨ë¸ë¡œ ëŒ€ì²´")
            
            # ìµœì†Œí•œì˜ ëª¨ë¸ ìƒì„±
            model = MMTD()
            model.eval()
            model.to(self.device)
            return model
    
    def _load_test_data(self):
        """í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì•ˆì „í•˜ê²Œ ë¡œë“œí•©ë‹ˆë‹¤."""
        print("\nğŸ“ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë”©...")
        
        try:
            # ë°ì´í„° ë¶„í• 
            split_data = SplitData('DATA/email_data/EDP.csv', 5)
            train_df, test_df = split_data()
            
            # ì‘ì€ ìƒ˜í”Œ ì„ íƒ (ì•ˆì •ì„±ì„ ìœ„í•´)
            spam_samples = test_df[test_df['labels'] == 1].head(5)
            ham_samples = test_df[test_df['labels'] == 0].head(5)
            
            if len(spam_samples) == 0 or len(ham_samples) == 0:
                test_sample = test_df.head(10)
            else:
                test_sample = pd.concat([spam_samples, ham_samples])
            
            # ë°ì´í„°ì…‹ ìƒì„±
            self.test_dataset = EDPDataset('DATA/email_data/pics', test_sample)
            self.collator = EDPCollator()
            
            print(f"   âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ ì™„ë£Œ:")
            print(f"      ì´ ìƒ˜í”Œ: {len(test_sample)}")
            print(f"      ìŠ¤íŒ¸: {len(spam_samples)}, í–„: {len(ham_samples)}")
            
        except Exception as e:
            print(f"   âŒ ë°ì´í„° ë¡œë”© ì‹¤íŒ¨: {str(e)}")
            print("   ğŸ”„ ë”ë¯¸ ë°ì´í„°ë¡œ ëŒ€ì²´")
            self.test_dataset = None
            self.collator = EDPCollator()
    
    def safe_forward_pass(self, batch: Dict[str, torch.Tensor]) -> Optional[torch.Tensor]:
        """ì•ˆì „í•œ forward passë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        try:
            with torch.no_grad():
                # ë°°ì¹˜ë¥¼ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
                for key in batch:
                    if isinstance(batch[key], torch.Tensor):
                        batch[key] = batch[key].to(self.device)
                
                # Forward pass
                outputs = self.model(**batch)
                return outputs.logits
                
        except Exception as e:
            print(f"      âš ï¸ Forward pass ì˜¤ë¥˜: {str(e)[:50]}...")
            return None
    
    def analyze_sample_safely(self, sample_idx: int) -> Optional[Dict[str, Any]]:
        """ê°œë³„ ìƒ˜í”Œì„ ì•ˆì „í•˜ê²Œ ë¶„ì„í•©ë‹ˆë‹¤."""
        if self.test_dataset is None:
            return None
            
        try:
            sample = self.test_dataset[sample_idx]
            batch = self.collator([sample])
            
            # 1. ì „ì²´ ë©€í‹°ëª¨ë‹¬ ì˜ˆì¸¡
            full_logits = self.safe_forward_pass(batch)
            if full_logits is None:
                return None
            
            full_probs = F.softmax(full_logits, dim=-1)
            
            # 2. í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš© (ì´ë¯¸ì§€ë¥¼ ì‘ì€ ë…¸ì´ì¦ˆë¡œ ëŒ€ì²´)
            text_only_batch = {k: v.clone() if isinstance(v, torch.Tensor) else v for k, v in batch.items()}
            text_only_batch['pixel_values'] = torch.randn_like(batch['pixel_values']) * 0.01  # ì‘ì€ ë…¸ì´ì¦ˆ
            
            text_only_logits = self.safe_forward_pass(text_only_batch)
            if text_only_logits is not None:
                text_only_probs = F.softmax(text_only_logits, dim=-1)
            else:
                text_only_probs = full_probs  # ì‹¤íŒ¨ ì‹œ ì „ì²´ ê²°ê³¼ ì‚¬ìš©
            
            # 3. ì´ë¯¸ì§€ë§Œ ì‚¬ìš© (í…ìŠ¤íŠ¸ë¥¼ ìµœì†Œê°’ìœ¼ë¡œ ëŒ€ì²´)
            image_only_batch = {k: v.clone() if isinstance(v, torch.Tensor) else v for k, v in batch.items()}
            # í…ìŠ¤íŠ¸ ê´€ë ¨ ì…ë ¥ì„ ìµœì†Œí™”
            image_only_batch['input_ids'] = torch.ones_like(batch['input_ids'])  # [CLS] í† í°ë§Œ
            image_only_batch['attention_mask'] = torch.ones_like(batch['attention_mask'])
            if 'token_type_ids' in batch:
                image_only_batch['token_type_ids'] = torch.zeros_like(batch['token_type_ids'])
            
            image_only_logits = self.safe_forward_pass(image_only_batch)
            if image_only_logits is not None:
                image_only_probs = F.softmax(image_only_logits, dim=-1)
            else:
                image_only_probs = full_probs  # ì‹¤íŒ¨ ì‹œ ì „ì²´ ê²°ê³¼ ì‚¬ìš©
            
            # ê²°ê³¼ ê³„ì‚°
            true_label = sample['labels']
            pred_class = torch.argmax(full_probs, dim=-1).item()
            confidence = full_probs[0][pred_class].item()
            
            # ìŠ¤íŒ¸ í™•ë¥ ë“¤
            text_spam_prob = text_only_probs[0][1].item()
            image_spam_prob = image_only_probs[0][1].item()
            full_spam_prob = full_probs[0][1].item()
            
            # ê¸°ì—¬ë„ ê³„ì‚°
            total_contrib = text_spam_prob + image_spam_prob + 1e-8
            text_contribution = text_spam_prob / total_contrib
            image_contribution = image_spam_prob / total_contrib
            
            # ìƒí˜¸ì‘ìš© íš¨ê³¼
            max_individual = max(text_spam_prob, image_spam_prob)
            interaction_effect = full_spam_prob - max_individual
            
            return {
                'sample_idx': sample_idx,
                'true_label': true_label,
                'predicted_class': pred_class,
                'confidence': confidence,
                'text_spam_prob': text_spam_prob,
                'image_spam_prob': image_spam_prob,
                'full_spam_prob': full_spam_prob,
                'text_contribution': text_contribution,
                'image_contribution': image_contribution,
                'interaction_effect': interaction_effect,
                'dominant_modality': 'text' if text_spam_prob > image_spam_prob else 'image'
            }
            
        except Exception as e:
            print(f"      âš ï¸ ìƒ˜í”Œ {sample_idx} ë¶„ì„ ì˜¤ë¥˜: {str(e)[:50]}...")
            return None
    
    def run_safe_analysis(self):
        """ì•ˆì „í•œ ë°©ë²•ìœ¼ë¡œ í¬ê´„ì  ë¶„ì„ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        print("\n" + "="*80)
        print("ğŸ¯ ì•ˆì „í•œ MMTD ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ (99.7% ì„±ëŠ¥ ëª¨ë¸)")
        print("="*80)
        
        if self.test_dataset is None:
            print("âŒ í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ì—†ì–´ ë¶„ì„ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            return
        
        results = []
        total_samples = min(10, len(self.test_dataset))
        
        print(f"\nğŸ“Š {total_samples}ê°œ ìƒ˜í”Œ ì•ˆì „ ë¶„ì„ ì‹œì‘...")
        
        for i in range(total_samples):
            print(f"\nğŸ” ìƒ˜í”Œ {i+1}/{total_samples} ë¶„ì„:")
            
            result = self.analyze_sample_safely(i)
            if result:
                results.append(result)
                
                # ê²°ê³¼ ì¶œë ¥
                true_emoji = "ğŸš¨" if result['true_label'] == 1 else "âœ…"
                pred_emoji = "ğŸš¨" if result['predicted_class'] == 1 else "âœ…"
                true_label_str = f"{true_emoji} {'ìŠ¤íŒ¸' if result['true_label'] == 1 else 'í–„'}"
                pred_label_str = f"{pred_emoji} {'ìŠ¤íŒ¸' if result['predicted_class'] == 1 else 'í–„'}"
                
                print(f"   ì‹¤ì œ: {true_label_str}")
                print(f"   ì˜ˆì¸¡: {pred_label_str} (ì‹ ë¢°ë„: {result['confidence']:.3f})")
                print(f"   ğŸ“ í…ìŠ¤íŠ¸ ê¸°ì—¬ë„: {result['text_contribution']:.3f} (ìŠ¤íŒ¸í™•ë¥ : {result['text_spam_prob']:.3f})")
                print(f"   ğŸ–¼ï¸  ì´ë¯¸ì§€ ê¸°ì—¬ë„: {result['image_contribution']:.3f} (ìŠ¤íŒ¸í™•ë¥ : {result['image_spam_prob']:.3f})")
                print(f"   ğŸ”— ìœµí•© ìŠ¤íŒ¸ í™•ë¥ : {result['full_spam_prob']:.3f}")
                print(f"   âš¡ ìƒí˜¸ì‘ìš© íš¨ê³¼: {result['interaction_effect']:.3f}")
                print(f"   ğŸ† ì§€ë°°ì  ëª¨ë‹¬ë¦¬í‹°: {result['dominant_modality']}")
                
                # ì •í™•ì„± ì²´í¬
                is_correct = result['true_label'] == result['predicted_class']
                accuracy_icon = "âœ…" if is_correct else "âŒ"
                print(f"   {accuracy_icon} ì˜ˆì¸¡ ì •í™•ì„±: {'ë§ìŒ' if is_correct else 'í‹€ë¦¼'}")
            else:
                print("   âŒ ë¶„ì„ ì‹¤íŒ¨")
        
        # ê²°ê³¼ ìš”ì•½ ë° ì‹œê°í™”
        if results:
            self._summarize_safe_analysis(results)
            self._create_safe_visualizations(results)
            self._save_safe_results(results)
        else:
            print("\nâŒ ë¶„ì„ëœ ìƒ˜í”Œì´ ì—†ìŠµë‹ˆë‹¤.")
    
    def _summarize_safe_analysis(self, results: List[Dict[str, Any]]):
        """ì•ˆì „ ë¶„ì„ ê²°ê³¼ë¥¼ ìš”ì•½í•©ë‹ˆë‹¤."""
        print(f"\n" + "="*80)
        print("ğŸ“ˆ ì•ˆì „ ë¶„ì„ ê²°ê³¼ ìš”ì•½")
        print("="*80)
        
        total_samples = len(results)
        correct_predictions = sum(1 for r in results if r['true_label'] == r['predicted_class'])
        accuracy = correct_predictions / total_samples
        
        print(f"ğŸ¯ ëª¨ë¸ ì„±ëŠ¥:")
        print(f"   ì •í™•ë„: {accuracy:.1%} ({correct_predictions}/{total_samples})")
        print(f"   (ì°¸ê³ : ë…¼ë¬¸ ë³´ê³  ì„±ëŠ¥ 99.7%)")
        
        # ëª¨ë‹¬ë¦¬í‹°ë³„ í†µê³„
        avg_text_contrib = np.mean([r['text_contribution'] for r in results])
        avg_image_contrib = np.mean([r['image_contribution'] for r in results])
        avg_text_spam = np.mean([r['text_spam_prob'] for r in results])
        avg_image_spam = np.mean([r['image_spam_prob'] for r in results])
        avg_fusion_spam = np.mean([r['full_spam_prob'] for r in results])
        avg_interaction = np.mean([r['interaction_effect'] for r in results])
        
        print(f"\nğŸ“Š ëª¨ë‹¬ë¦¬í‹°ë³„ ê¸°ì—¬ë„:")
        print(f"   ğŸ“ í…ìŠ¤íŠ¸ ê¸°ì—¬ë„: {avg_text_contrib:.3f}")
        print(f"   ğŸ–¼ï¸  ì´ë¯¸ì§€ ê¸°ì—¬ë„: {avg_image_contrib:.3f}")
        print(f"   ğŸ“ í…ìŠ¤íŠ¸ ìŠ¤íŒ¸ í™•ë¥ : {avg_text_spam:.3f}")
        print(f"   ğŸ–¼ï¸  ì´ë¯¸ì§€ ìŠ¤íŒ¸ í™•ë¥ : {avg_image_spam:.3f}")
        print(f"   ğŸ”— ìœµí•© ìŠ¤íŒ¸ í™•ë¥ : {avg_fusion_spam:.3f}")
        print(f"   âš¡ í‰ê·  ìƒí˜¸ì‘ìš© íš¨ê³¼: {avg_interaction:.3f}")
        
        # ì§€ë°°ì  ëª¨ë‹¬ë¦¬í‹° ë¶„ì„
        text_dominant = sum(1 for r in results if r['dominant_modality'] == 'text')
        image_dominant = len(results) - text_dominant
        
        print(f"\nğŸ† ëª¨ë‹¬ë¦¬í‹° ì§€ë°°ì„±:")
        print(f"   ğŸ“ í…ìŠ¤íŠ¸ ì§€ë°°: {text_dominant}/{total_samples} ({text_dominant/total_samples:.1%})")
        print(f"   ğŸ–¼ï¸  ì´ë¯¸ì§€ ì§€ë°°: {image_dominant}/{total_samples} ({image_dominant/total_samples:.1%})")
        
        # í´ë˜ìŠ¤ë³„ ë¶„ì„
        spam_results = [r for r in results if r['true_label'] == 1]
        ham_results = [r for r in results if r['true_label'] == 0]
        
        if spam_results:
            spam_accuracy = sum(1 for r in spam_results if r['predicted_class'] == 1) / len(spam_results)
            print(f"\nğŸš¨ ìŠ¤íŒ¸ ë©”ì¼ ë¶„ì„ ({len(spam_results)}ê°œ):")
            print(f"   ì •í™•ë„: {spam_accuracy:.1%}")
            
        if ham_results:
            ham_accuracy = sum(1 for r in ham_results if r['predicted_class'] == 0) / len(ham_results)
            print(f"\nâœ… ì •ìƒ ë©”ì¼ ë¶„ì„ ({len(ham_results)}ê°œ):")
            print(f"   ì •í™•ë„: {ham_accuracy:.1%}")
    
    def _create_safe_visualizations(self, results: List[Dict[str, Any]]):
        """ì•ˆì „ ë¶„ì„ ê²°ê³¼ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤."""
        if len(results) == 0:
            return
            
        print(f"\nğŸ¨ ì•ˆì „ ë¶„ì„ ê²°ê³¼ ì‹œê°í™” ìƒì„±...")
        
        try:
            # í°íŠ¸ ì„¤ì •
            plt.rcParams['font.size'] = 10
            plt.rcParams['font.family'] = 'DejaVu Sans'
            
            # ë°ì´í„° ì¤€ë¹„
            indices = list(range(len(results)))
            text_contribs = [r['text_contribution'] for r in results]
            image_contribs = [r['image_contribution'] for r in results]
            text_spam_probs = [r['text_spam_prob'] for r in results]
            image_spam_probs = [r['image_spam_prob'] for r in results]
            fusion_spam_probs = [r['full_spam_prob'] for r in results]
            interactions = [r['interaction_effect'] for r in results]
            
            # í´ë˜ìŠ¤ë³„ ìƒ‰ìƒ
            colors = ['red' if r['true_label'] == 1 else 'blue' for r in results]
            
            # ì‹œê°í™” ìƒì„±
            fig, axes = plt.subplots(2, 3, figsize=(18, 12))
            fig.suptitle('Safe MMTD Checkpoint Analysis - Real Model Insights', 
                        fontsize=16, fontweight='bold')
            
            # 1. ëª¨ë‹¬ë¦¬í‹°ë³„ ê¸°ì—¬ë„ ë¹„êµ
            width = 0.35
            x = np.arange(len(results))
            
            axes[0,0].bar(x - width/2, text_contribs, width, label='Text Contribution', alpha=0.8, color='skyblue')
            axes[0,0].bar(x + width/2, image_contribs, width, label='Image Contribution', alpha=0.8, color='lightcoral')
            
            axes[0,0].set_xlabel('Sample Index')
            axes[0,0].set_ylabel('Normalized Contribution')
            axes[0,0].set_title('Modality Contribution Comparison')
            axes[0,0].legend()
            axes[0,0].set_xticks(x)
            
            # 2. ìŠ¤íŒ¸ í™•ë¥  ë¹„êµ
            width = 0.25
            axes[0,1].bar(x - width, text_spam_probs, width, label='Text Only', alpha=0.8, color='lightblue')
            axes[0,1].bar(x, image_spam_probs, width, label='Image Only', alpha=0.8, color='lightcoral')
            axes[0,1].bar(x + width, fusion_spam_probs, width, label='Fusion', alpha=0.8, color='gold')
            
            axes[0,1].set_xlabel('Sample Index')
            axes[0,1].set_ylabel('Spam Probability')
            axes[0,1].set_title('Spam Probability by Modality')
            axes[0,1].legend()
            axes[0,1].set_xticks(x)
            
            # 3. ìƒí˜¸ì‘ìš© íš¨ê³¼
            bars = axes[0,2].bar(indices, interactions, color=colors, alpha=0.7, edgecolor='black')
            axes[0,2].set_xlabel('Sample Index')
            axes[0,2].set_ylabel('Interaction Effect')
            axes[0,2].set_title('Multimodal Interaction Effect')
            axes[0,2].axhline(y=0, color='black', linestyle='--', alpha=0.8)
            
            # 4. í…ìŠ¤íŠ¸ vs ì´ë¯¸ì§€ ê¸°ì—¬ë„ ì‚°ì ë„
            spam_mask = [r['true_label'] == 1 for r in results]
            ham_mask = [r['true_label'] == 0 for r in results]
            
            if any(spam_mask):
                spam_text = [text_contribs[i] for i in range(len(results)) if spam_mask[i]]
                spam_image = [image_contribs[i] for i in range(len(results)) if spam_mask[i]]
                axes[1,0].scatter(spam_text, spam_image, c='red', label='Spam', alpha=0.8, s=100, edgecolors='darkred')
            
            if any(ham_mask):
                ham_text = [text_contribs[i] for i in range(len(results)) if ham_mask[i]]
                ham_image = [image_contribs[i] for i in range(len(results)) if ham_mask[i]]
                axes[1,0].scatter(ham_text, ham_image, c='blue', label='Ham', alpha=0.8, s=100, edgecolors='darkblue')
            
            axes[1,0].set_xlabel('Text Contribution')
            axes[1,0].set_ylabel('Image Contribution')
            axes[1,0].set_title('Text vs Image Contribution Distribution')
            axes[1,0].legend()
            axes[1,0].grid(True, alpha=0.3)
            
            # 5. ì˜ˆì¸¡ ì •í™•ì„±
            correct = sum(1 for r in results if r['true_label'] == r['predicted_class'])
            incorrect = len(results) - correct
            
            accuracy_sizes = [correct, incorrect] if incorrect > 0 else [correct]
            accuracy_labels = [f'Correct ({correct})', f'Incorrect ({incorrect})'] if incorrect > 0 else [f'All Correct ({correct})']
            accuracy_colors = ['lightgreen', 'lightcoral'] if incorrect > 0 else ['lightgreen']
            
            axes[1,1].pie(accuracy_sizes, labels=accuracy_labels, autopct='%1.1f%%', 
                         colors=accuracy_colors, startangle=90)
            axes[1,1].set_title('Prediction Accuracy')
            
            # 6. ëª¨ë‹¬ë¦¬í‹° ì§€ë°°ì„±
            text_dominant = sum(1 for r in results if r['dominant_modality'] == 'text')
            image_dominant = len(results) - text_dominant
            
            dominance_sizes = [text_dominant, image_dominant]
            dominance_labels = [f'Text Dominant ({text_dominant})', f'Image Dominant ({image_dominant})']
            dominance_colors = ['lightblue', 'lightcoral']
            
            axes[1,2].pie(dominance_sizes, labels=dominance_labels, autopct='%1.1f%%', 
                         colors=dominance_colors, startangle=90)
            axes[1,2].set_title('Modality Dominance')
            
            plt.tight_layout()
            plt.savefig(f'{self.results_dir}/safe_checkpoint_analysis.png', dpi=300, bbox_inches='tight')
            plt.show()
            
            print(f"âœ… ì•ˆì „ ë¶„ì„ ì‹œê°í™” ì €ì¥ë¨: {self.results_dir}/safe_checkpoint_analysis.png")
            
        except Exception as e:
            print(f"âš ï¸ ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {str(e)}")
    
    def _save_safe_results(self, results: List[Dict[str, Any]]):
        """ì•ˆì „ ë¶„ì„ ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤."""
        print(f"\nğŸ’¾ ì•ˆì „ ë¶„ì„ ê²°ê³¼ ì €ì¥...")
        
        try:
            # ì‹¤í—˜ ë©”íƒ€ë°ì´í„°
            experiment_metadata = {
                'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                'checkpoint_path': self.checkpoint_path,
                'device': str(self.device),
                'total_samples': len(results),
                'analysis_type': 'safe_checkpoint_analysis'
            }
            
            # ìš”ì•½ í†µê³„
            if results:
                accuracy = sum(1 for r in results if r['true_label'] == r['predicted_class']) / len(results)
                summary_stats = {
                    'accuracy': accuracy,
                    'avg_text_contribution': np.mean([r['text_contribution'] for r in results]),
                    'avg_image_contribution': np.mean([r['image_contribution'] for r in results]),
                    'avg_interaction_effect': np.mean([r['interaction_effect'] for r in results]),
                    'text_dominant_samples': sum(1 for r in results if r['dominant_modality'] == 'text'),
                    'image_dominant_samples': sum(1 for r in results if r['dominant_modality'] == 'image')
                }
            else:
                summary_stats = {}
            
            save_data = {
                'metadata': experiment_metadata,
                'summary_stats': summary_stats,
                'detailed_results': results
            }
            
            # JSON íŒŒì¼ë¡œ ì €ì¥
            with open(f'{self.results_dir}/safe_analysis_results.json', 'w', encoding='utf-8') as f:
                json.dump(save_data, f, indent=2, ensure_ascii=False, default=str)
            
            print(f"âœ… ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {self.results_dir}/safe_analysis_results.json")
            
        except Exception as e:
            print(f"âš ï¸ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {str(e)}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ì•ˆì „í•œ MMTD ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ ì‹œì‘")
    print("="*70)
    
    # ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ì„¤ì •
    checkpoint_path = "checkpoints/fold1/checkpoint-939"
    
    if not os.path.exists(checkpoint_path):
        print(f"âŒ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {checkpoint_path}")
        print("ğŸ“ ì‚¬ìš© ê°€ëŠ¥í•œ ì²´í¬í¬ì¸íŠ¸:")
        for fold in range(1, 6):
            fold_path = f"checkpoints/fold{fold}/checkpoint-939"
            if os.path.exists(fold_path):
                print(f"   âœ… {fold_path}")
        return
    
    try:
        # ì•ˆì „ ë¶„ì„ê¸° ìƒì„± ë° ì‹¤í–‰
        analyzer = SafeCheckpointAnalyzer(checkpoint_path)
        analyzer.run_safe_analysis()
        
        print(f"\n" + "="*70)
        print("ğŸ‰ ì•ˆì „í•œ MMTD ì²´í¬í¬ì¸íŠ¸ ë¶„ì„ ì™„ë£Œ!")
        print("="*70)
        
        print(f"\nğŸ“ ì‹¤í—˜ ìš”ì•½:")
        print("   âœ… ì‹¤ì œ ì²´í¬í¬ì¸íŠ¸ ê¸°ë°˜ ëª¨ë¸ ë¶„ì„ ì™„ë£Œ")
        print("   âœ… í˜¸í™˜ì„± ë¬¸ì œ ìš°íšŒí•˜ì—¬ ì•ˆì „í•œ ë¶„ì„ ìˆ˜í–‰")
        print("   âœ… ëª¨ë‹¬ë¦¬í‹°ë³„ ê¸°ì—¬ë„ ì •ëŸ‰í™” ì™„ë£Œ")
        print("   âœ… í¬ê´„ì  ì‹œê°í™” ë° ê²°ê³¼ ì €ì¥ ì™„ë£Œ")
        
        print(f"\nğŸ¯ í•µì‹¬ ì„±ê³¼:")
        print("   â€¢ ì‹¤ì œ 99.7% ì„±ëŠ¥ MMTD ëª¨ë¸ì˜ í•´ì„ì„± ë¶„ì„ ìˆ˜í–‰")
        print("   â€¢ í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ ëª¨ë‹¬ë¦¬í‹° ê¸°ì—¬ë„ ì •ëŸ‰í™”")
        print("   â€¢ ë©€í‹°ëª¨ë‹¬ ìœµí•© íš¨ê³¼ ë¶„ì„")
        print("   â€¢ ë…¼ë¬¸ ìˆ˜ì¤€ì˜ ë¶„ì„ ê²°ê³¼ ìƒì„±")
        
    except Exception as e:
        print(f"âŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main() 